
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>nltk.tokenize package &#8212; NLTK 3.6 documentation</title>
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/agogo.css" type="text/css" />
    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="prev" title="nltk.test.unit.translate package" href="nltk.test.unit.translate.html" /> 
  </head><body>
    <div class="header-wrapper" role="banner">
      <div class="header">
        <div class="headertitle"><a
          href="../index.html">NLTK 3.6 documentation</a></div>
        <div class="rel" role="navigation" aria-label="related navigation">
          <a href="nltk.test.unit.translate.html" title="nltk.test.unit.translate package"
             accesskey="P">previous</a> |
          <a href="../py-modindex.html" title="Python Module Index"
             >modules</a> |
          <a href="../genindex.html" title="General Index"
             accesskey="I">index</a>
        </div>
       </div>
    </div>

    <div class="content-wrapper">
      <div class="content">
        <div class="document">
            
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="nltk-tokenize-package">
<h1>nltk.tokenize package<a class="headerlink" href="#nltk-tokenize-package" title="Permalink to this headline">¶</a></h1>
<div class="section" id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="module-nltk.tokenize.api">
<span id="nltk-tokenize-api-module"></span><h2>nltk.tokenize.api module<a class="headerlink" href="#module-nltk.tokenize.api" title="Permalink to this headline">¶</a></h2>
<p>Tokenizer Interface</p>
<dl class="py class">
<dt id="nltk.tokenize.api.StringTokenizer">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">nltk.tokenize.api.</span></code><code class="sig-name descname"><span class="pre">StringTokenizer</span></code><a class="reference internal" href="../_modules/nltk/tokenize/api.html#StringTokenizer"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.api.StringTokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#nltk.tokenize.api.TokenizerI" title="nltk.tokenize.api.TokenizerI"><code class="xref py py-class docutils literal notranslate"><span class="pre">nltk.tokenize.api.TokenizerI</span></code></a></p>
<p>A tokenizer that divides a string into substrings by splitting
on the specified string (defined in subclasses).</p>
<dl class="py method">
<dt id="nltk.tokenize.api.StringTokenizer.span_tokenize">
<code class="sig-name descname"><span class="pre">span_tokenize</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">s</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tokenize/api.html#StringTokenizer.span_tokenize"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.api.StringTokenizer.span_tokenize" title="Permalink to this definition">¶</a></dt>
<dd><p>Identify the tokens using integer offsets <code class="docutils literal notranslate"><span class="pre">(start_i,</span> <span class="pre">end_i)</span></code>,
where <code class="docutils literal notranslate"><span class="pre">s[start_i:end_i]</span></code> is the corresponding token.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>iter(tuple(int, int))</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="nltk.tokenize.api.StringTokenizer.tokenize">
<code class="sig-name descname"><span class="pre">tokenize</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">s</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tokenize/api.html#StringTokenizer.tokenize"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.api.StringTokenizer.tokenize" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a tokenized copy of <em>s</em>.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>list of str</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="nltk.tokenize.api.TokenizerI">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">nltk.tokenize.api.</span></code><code class="sig-name descname"><span class="pre">TokenizerI</span></code><a class="reference internal" href="../_modules/nltk/tokenize/api.html#TokenizerI"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.api.TokenizerI" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">abc.ABC</span></code></p>
<p>A processing interface for tokenizing a string.
Subclasses must define <code class="docutils literal notranslate"><span class="pre">tokenize()</span></code> or <code class="docutils literal notranslate"><span class="pre">tokenize_sents()</span></code> (or both).</p>
<dl class="py method">
<dt id="nltk.tokenize.api.TokenizerI.span_tokenize">
<code class="sig-name descname"><span class="pre">span_tokenize</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">s</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tokenize/api.html#TokenizerI.span_tokenize"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.api.TokenizerI.span_tokenize" title="Permalink to this definition">¶</a></dt>
<dd><p>Identify the tokens using integer offsets <code class="docutils literal notranslate"><span class="pre">(start_i,</span> <span class="pre">end_i)</span></code>,
where <code class="docutils literal notranslate"><span class="pre">s[start_i:end_i]</span></code> is the corresponding token.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>iter(tuple(int, int))</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="nltk.tokenize.api.TokenizerI.span_tokenize_sents">
<code class="sig-name descname"><span class="pre">span_tokenize_sents</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">strings</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tokenize/api.html#TokenizerI.span_tokenize_sents"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.api.TokenizerI.span_tokenize_sents" title="Permalink to this definition">¶</a></dt>
<dd><p>Apply <code class="docutils literal notranslate"><span class="pre">self.span_tokenize()</span></code> to each element of <code class="docutils literal notranslate"><span class="pre">strings</span></code>.  I.e.:</p>
<blockquote>
<div><p>return [self.span_tokenize(s) for s in strings]</p>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>iter(list(tuple(int, int)))</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="nltk.tokenize.api.TokenizerI.tokenize">
<em class="property"><span class="pre">abstract</span> </em><code class="sig-name descname"><span class="pre">tokenize</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">s</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tokenize/api.html#TokenizerI.tokenize"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.api.TokenizerI.tokenize" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a tokenized copy of <em>s</em>.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>list of str</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="nltk.tokenize.api.TokenizerI.tokenize_sents">
<code class="sig-name descname"><span class="pre">tokenize_sents</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">strings</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tokenize/api.html#TokenizerI.tokenize_sents"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.api.TokenizerI.tokenize_sents" title="Permalink to this definition">¶</a></dt>
<dd><p>Apply <code class="docutils literal notranslate"><span class="pre">self.tokenize()</span></code> to each element of <code class="docutils literal notranslate"><span class="pre">strings</span></code>.  I.e.:</p>
<blockquote>
<div><p>return [self.tokenize(s) for s in strings]</p>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>list(list(str))</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-nltk.tokenize.casual">
<span id="nltk-tokenize-casual-module"></span><h2>nltk.tokenize.casual module<a class="headerlink" href="#module-nltk.tokenize.casual" title="Permalink to this headline">¶</a></h2>
<p>Twitter-aware tokenizer, designed to be flexible and easy to adapt to new
domains and tasks. The basic logic is this:</p>
<ol class="arabic simple">
<li><p>The tuple regex_strings defines a list of regular expression
strings.</p></li>
<li><p>The regex_strings strings are put, in order, into a compiled
regular expression object called word_re.</p></li>
<li><p>The tokenization is done by word_re.findall(s), where s is the
user-supplied string, inside the tokenize() method of the class
Tokenizer.</p></li>
<li><p>When instantiating Tokenizer objects, there is a single option:
preserve_case.  By default, it is set to True. If it is set to
False, then the tokenizer will downcase everything except for
emoticons.</p></li>
</ol>
<dl class="py class">
<dt id="nltk.tokenize.casual.TweetTokenizer">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">nltk.tokenize.casual.</span></code><code class="sig-name descname"><span class="pre">TweetTokenizer</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">preserve_case</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduce_len</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strip_handles</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tokenize/casual.html#TweetTokenizer"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.casual.TweetTokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Tokenizer for tweets.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">nltk.tokenize</span> <span class="kn">import</span> <span class="n">TweetTokenizer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tknzr</span> <span class="o">=</span> <span class="n">TweetTokenizer</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s0</span> <span class="o">=</span> <span class="s2">&quot;This is a cooool #dummysmiley: :-) :-P &lt;3 and some arrows &lt; &gt; -&gt; &lt;--&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tknzr</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">s0</span><span class="p">)</span>
<span class="go">[&#39;This&#39;, &#39;is&#39;, &#39;a&#39;, &#39;cooool&#39;, &#39;#dummysmiley&#39;, &#39;:&#39;, &#39;:-)&#39;, &#39;:-P&#39;, &#39;&lt;3&#39;, &#39;and&#39;, &#39;some&#39;, &#39;arrows&#39;, &#39;&lt;&#39;, &#39;&gt;&#39;, &#39;-&gt;&#39;, &#39;&lt;--&#39;]</span>
</pre></div>
</div>
<p>Examples using <cite>strip_handles</cite> and <cite>reduce_len parameters</cite>:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tknzr</span> <span class="o">=</span> <span class="n">TweetTokenizer</span><span class="p">(</span><span class="n">strip_handles</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">reduce_len</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s1</span> <span class="o">=</span> <span class="s1">&#39;@remy: This is waaaaayyyy too much for you!!!!!!&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tknzr</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">s1</span><span class="p">)</span>
<span class="go">[&#39;:&#39;, &#39;This&#39;, &#39;is&#39;, &#39;waaayyy&#39;, &#39;too&#39;, &#39;much&#39;, &#39;for&#39;, &#39;you&#39;, &#39;!&#39;, &#39;!&#39;, &#39;!&#39;]</span>
</pre></div>
</div>
<dl class="py method">
<dt id="nltk.tokenize.casual.TweetTokenizer.tokenize">
<code class="sig-name descname"><span class="pre">tokenize</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">text</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tokenize/casual.html#TweetTokenizer.tokenize"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.casual.TweetTokenizer.tokenize" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>text</strong> – str</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>list(str)</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a tokenized list of strings; concatenating this list returns        the original string if <cite>preserve_case=False</cite></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt id="nltk.tokenize.casual.casual_tokenize">
<code class="sig-prename descclassname"><span class="pre">nltk.tokenize.casual.</span></code><code class="sig-name descname"><span class="pre">casual_tokenize</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">text</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">preserve_case</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduce_len</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strip_handles</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tokenize/casual.html#casual_tokenize"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.casual.casual_tokenize" title="Permalink to this definition">¶</a></dt>
<dd><p>Convenience function for wrapping the tokenizer.</p>
</dd></dl>

<dl class="py function">
<dt id="nltk.tokenize.casual.reduce_lengthening">
<code class="sig-prename descclassname"><span class="pre">nltk.tokenize.casual.</span></code><code class="sig-name descname"><span class="pre">reduce_lengthening</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">text</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tokenize/casual.html#reduce_lengthening"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.casual.reduce_lengthening" title="Permalink to this definition">¶</a></dt>
<dd><p>Replace repeated character sequences of length 3 or greater with sequences
of length 3.</p>
</dd></dl>

<dl class="py function">
<dt id="nltk.tokenize.casual.remove_handles">
<code class="sig-prename descclassname"><span class="pre">nltk.tokenize.casual.</span></code><code class="sig-name descname"><span class="pre">remove_handles</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">text</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tokenize/casual.html#remove_handles"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.casual.remove_handles" title="Permalink to this definition">¶</a></dt>
<dd><p>Remove Twitter username handles from text.</p>
</dd></dl>

</div>
<div class="section" id="module-nltk.tokenize.destructive">
<span id="nltk-tokenize-destructive-module"></span><h2>nltk.tokenize.destructive module<a class="headerlink" href="#module-nltk.tokenize.destructive" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="nltk.tokenize.destructive.MacIntyreContractions">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">nltk.tokenize.destructive.</span></code><code class="sig-name descname"><span class="pre">MacIntyreContractions</span></code><a class="reference internal" href="../_modules/nltk/tokenize/destructive.html#MacIntyreContractions"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.destructive.MacIntyreContractions" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>List of contractions adapted from Robert MacIntyre’s tokenizer.</p>
<dl class="py attribute">
<dt id="nltk.tokenize.destructive.MacIntyreContractions.CONTRACTIONS2">
<code class="sig-name descname"><span class="pre">CONTRACTIONS2</span></code><em class="property"> <span class="pre">=</span> <span class="pre">['(?i)\\b(can)(?#X)(not)\\b',</span> <span class="pre">&quot;(?i)\\b(d)(?#X)('ye)\\b&quot;,</span> <span class="pre">'(?i)\\b(gim)(?#X)(me)\\b',</span> <span class="pre">'(?i)\\b(gon)(?#X)(na)\\b',</span> <span class="pre">'(?i)\\b(got)(?#X)(ta)\\b',</span> <span class="pre">'(?i)\\b(lem)(?#X)(me)\\b',</span> <span class="pre">&quot;(?i)\\b(more)(?#X)('n)\\b&quot;,</span> <span class="pre">'(?i)\\b(wan)(?#X)(na)\\s']</span></em><a class="headerlink" href="#nltk.tokenize.destructive.MacIntyreContractions.CONTRACTIONS2" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="nltk.tokenize.destructive.MacIntyreContractions.CONTRACTIONS3">
<code class="sig-name descname"><span class="pre">CONTRACTIONS3</span></code><em class="property"> <span class="pre">=</span> <span class="pre">[&quot;(?i)</span> <span class="pre">('t)(?#X)(is)\\b&quot;,</span> <span class="pre">&quot;(?i)</span> <span class="pre">('t)(?#X)(was)\\b&quot;]</span></em><a class="headerlink" href="#nltk.tokenize.destructive.MacIntyreContractions.CONTRACTIONS3" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="nltk.tokenize.destructive.MacIntyreContractions.CONTRACTIONS4">
<code class="sig-name descname"><span class="pre">CONTRACTIONS4</span></code><em class="property"> <span class="pre">=</span> <span class="pre">['(?i)\\b(whad)(dd)(ya)\\b',</span> <span class="pre">'(?i)\\b(wha)(t)(cha)\\b']</span></em><a class="headerlink" href="#nltk.tokenize.destructive.MacIntyreContractions.CONTRACTIONS4" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt id="nltk.tokenize.destructive.NLTKWordTokenizer">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">nltk.tokenize.destructive.</span></code><code class="sig-name descname"><span class="pre">NLTKWordTokenizer</span></code><a class="reference internal" href="../_modules/nltk/tokenize/destructive.html#NLTKWordTokenizer"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.destructive.NLTKWordTokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#nltk.tokenize.api.TokenizerI" title="nltk.tokenize.api.TokenizerI"><code class="xref py py-class docutils literal notranslate"><span class="pre">nltk.tokenize.api.TokenizerI</span></code></a></p>
<p>The NLTK tokenizer that has improved upon the TreebankWordTokenizer.</p>
<p>The tokenizer is “destructive” such that the regexes applied will munge the
input string to a state beyond re-construction. It is possible to apply
<cite>TreebankWordDetokenizer.detokenize</cite> to the tokenized outputs of
<cite>NLTKDestructiveWordTokenizer.tokenize</cite> but there’s no guarantees to
revert to the original string.</p>
<dl class="py attribute">
<dt id="nltk.tokenize.destructive.NLTKWordTokenizer.CONTRACTIONS2">
<code class="sig-name descname"><span class="pre">CONTRACTIONS2</span></code><em class="property"> <span class="pre">=</span> <span class="pre">[re.compile('(?i)\\b(can)(?#X)(not)\\b',</span> <span class="pre">re.IGNORECASE),</span> <span class="pre">re.compile(&quot;(?i)\\b(d)(?#X)('ye)\\b&quot;,</span> <span class="pre">re.IGNORECASE),</span> <span class="pre">re.compile('(?i)\\b(gim)(?#X)(me)\\b',</span> <span class="pre">re.IGNORECASE),</span> <span class="pre">re.compile('(?i)\\b(gon)(?#X)(na)\\b',</span> <span class="pre">re.IGNORECASE),</span> <span class="pre">re.compile('(?i)\\b(got)(?#X)(ta)\\b',</span> <span class="pre">re.IGNORECASE),</span> <span class="pre">re.compile('(?i)\\b(lem)(?#X)(me)\\b',</span> <span class="pre">re.IGNORECASE),</span> <span class="pre">re.compile(&quot;(?i)\\b(more)(?#X)('n)\\b&quot;,</span> <span class="pre">re.IGNORECASE),</span> <span class="pre">re.compile('(?i)\\b(wan)(?#X)(na)\\s',</span> <span class="pre">re.IGNORECASE)]</span></em><a class="headerlink" href="#nltk.tokenize.destructive.NLTKWordTokenizer.CONTRACTIONS2" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="nltk.tokenize.destructive.NLTKWordTokenizer.CONTRACTIONS3">
<code class="sig-name descname"><span class="pre">CONTRACTIONS3</span></code><em class="property"> <span class="pre">=</span> <span class="pre">[re.compile(&quot;(?i)</span> <span class="pre">('t)(?#X)(is)\\b&quot;,</span> <span class="pre">re.IGNORECASE),</span> <span class="pre">re.compile(&quot;(?i)</span> <span class="pre">('t)(?#X)(was)\\b&quot;,</span> <span class="pre">re.IGNORECASE)]</span></em><a class="headerlink" href="#nltk.tokenize.destructive.NLTKWordTokenizer.CONTRACTIONS3" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="nltk.tokenize.destructive.NLTKWordTokenizer.CONVERT_PARENTHESES">
<code class="sig-name descname"><span class="pre">CONVERT_PARENTHESES</span></code><em class="property"> <span class="pre">=</span> <span class="pre">[(re.compile('\\('),</span> <span class="pre">'-LRB-'),</span> <span class="pre">(re.compile('\\)'),</span> <span class="pre">'-RRB-'),</span> <span class="pre">(re.compile('\\['),</span> <span class="pre">'-LSB-'),</span> <span class="pre">(re.compile('\\]'),</span> <span class="pre">'-RSB-'),</span> <span class="pre">(re.compile('\\{'),</span> <span class="pre">'-LCB-'),</span> <span class="pre">(re.compile('\\}'),</span> <span class="pre">'-RCB-')]</span></em><a class="headerlink" href="#nltk.tokenize.destructive.NLTKWordTokenizer.CONVERT_PARENTHESES" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="nltk.tokenize.destructive.NLTKWordTokenizer.DOUBLE_DASHES">
<code class="sig-name descname"><span class="pre">DOUBLE_DASHES</span></code><em class="property"> <span class="pre">=</span> <span class="pre">(re.compile('--'),</span> <span class="pre">'</span> <span class="pre">--</span> <span class="pre">')</span></em><a class="headerlink" href="#nltk.tokenize.destructive.NLTKWordTokenizer.DOUBLE_DASHES" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="nltk.tokenize.destructive.NLTKWordTokenizer.ENDING_QUOTES">
<code class="sig-name descname"><span class="pre">ENDING_QUOTES</span></code><em class="property"> <span class="pre">=</span> <span class="pre">[(re.compile('([»”’])'),</span> <span class="pre">'</span> <span class="pre">\\1</span> <span class="pre">'),</span> <span class="pre">(re.compile('&quot;'),</span> <span class="pre">&quot;</span> <span class="pre">''</span> <span class="pre">&quot;),</span> <span class="pre">(re.compile(&quot;(\\S)(\\'\\')&quot;),</span> <span class="pre">'\\1</span> <span class="pre">\\2</span> <span class="pre">'),</span> <span class="pre">(re.compile(&quot;([^'</span> <span class="pre">])('[sS]|'[mM]|'[dD]|')</span> <span class="pre">&quot;),</span> <span class="pre">'\\1</span> <span class="pre">\\2</span> <span class="pre">'),</span> <span class="pre">(re.compile(&quot;([^'</span> <span class="pre">])('ll|'LL|'re|'RE|'ve|'VE|n't|N'T)</span> <span class="pre">&quot;),</span> <span class="pre">'\\1</span> <span class="pre">\\2</span> <span class="pre">')]</span></em><a class="headerlink" href="#nltk.tokenize.destructive.NLTKWordTokenizer.ENDING_QUOTES" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="nltk.tokenize.destructive.NLTKWordTokenizer.PARENS_BRACKETS">
<code class="sig-name descname"><span class="pre">PARENS_BRACKETS</span></code><em class="property"> <span class="pre">=</span> <span class="pre">(re.compile('[\\]\\[\\(\\)\\{\\}\\&lt;\\&gt;]'),</span> <span class="pre">'</span> <span class="pre">\\g&lt;0&gt;</span> <span class="pre">')</span></em><a class="headerlink" href="#nltk.tokenize.destructive.NLTKWordTokenizer.PARENS_BRACKETS" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="nltk.tokenize.destructive.NLTKWordTokenizer.PUNCTUATION">
<code class="sig-name descname"><span class="pre">PUNCTUATION</span></code><em class="property"> <span class="pre">=</span> <span class="pre">[(re.compile('([^\\.])(\\.)([\\]\\)}&gt;&quot;\\\'»”’</span> <span class="pre">]*)\\s*$'),</span> <span class="pre">'\\1</span> <span class="pre">\\2</span> <span class="pre">\\3</span> <span class="pre">'),</span> <span class="pre">(re.compile('([:,])([^\\d])'),</span> <span class="pre">'</span> <span class="pre">\\1</span> <span class="pre">\\2'),</span> <span class="pre">(re.compile('([:,])$'),</span> <span class="pre">'</span> <span class="pre">\\1</span> <span class="pre">'),</span> <span class="pre">(re.compile('\\.{2,}'),</span> <span class="pre">'</span> <span class="pre">\\g&lt;0&gt;</span> <span class="pre">'),</span> <span class="pre">(re.compile('[;&#64;#$%&amp;]'),</span> <span class="pre">'</span> <span class="pre">\\g&lt;0&gt;</span> <span class="pre">'),</span> <span class="pre">(re.compile('([^\\.])(\\.)([\\]\\)}&gt;&quot;\\\']*)\\s*$'),</span> <span class="pre">'\\1</span> <span class="pre">\\2\\3</span> <span class="pre">'),</span> <span class="pre">(re.compile('[?!]'),</span> <span class="pre">'</span> <span class="pre">\\g&lt;0&gt;</span> <span class="pre">'),</span> <span class="pre">(re.compile(&quot;([^'])'</span> <span class="pre">&quot;),</span> <span class="pre">&quot;\\1</span> <span class="pre">'</span> <span class="pre">&quot;),</span> <span class="pre">(re.compile('[*]'),</span> <span class="pre">'</span> <span class="pre">\\g&lt;0&gt;</span> <span class="pre">')]</span></em><a class="headerlink" href="#nltk.tokenize.destructive.NLTKWordTokenizer.PUNCTUATION" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="nltk.tokenize.destructive.NLTKWordTokenizer.STARTING_QUOTES">
<code class="sig-name descname"><span class="pre">STARTING_QUOTES</span></code><em class="property"> <span class="pre">=</span> <span class="pre">[(re.compile('([«“‘„]|[`]+)'),</span> <span class="pre">'</span> <span class="pre">\\1</span> <span class="pre">'),</span> <span class="pre">(re.compile('^\\&quot;'),</span> <span class="pre">'``'),</span> <span class="pre">(re.compile('(``)'),</span> <span class="pre">'</span> <span class="pre">\\1</span> <span class="pre">'),</span> <span class="pre">(re.compile('([</span> <span class="pre">\\(\\[{&lt;])(\\&quot;|\\\'{2})'),</span> <span class="pre">'\\1</span> <span class="pre">``</span> <span class="pre">'),</span> <span class="pre">(re.compile(&quot;(?i)(\\')(?!re|ve|ll|m|t|s|d|n)(\\w)\\b&quot;,</span> <span class="pre">re.IGNORECASE),</span> <span class="pre">'\\1</span> <span class="pre">\\2')]</span></em><a class="headerlink" href="#nltk.tokenize.destructive.NLTKWordTokenizer.STARTING_QUOTES" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="nltk.tokenize.destructive.NLTKWordTokenizer.tokenize">
<code class="sig-name descname"><span class="pre">tokenize</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">text</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">convert_parentheses</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_str</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tokenize/destructive.html#NLTKWordTokenizer.tokenize"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.destructive.NLTKWordTokenizer.tokenize" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a tokenized copy of <em>s</em>.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>list of str</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-nltk.tokenize.legality_principle">
<span id="nltk-tokenize-legality-principle-module"></span><h2>nltk.tokenize.legality_principle module<a class="headerlink" href="#module-nltk.tokenize.legality_principle" title="Permalink to this headline">¶</a></h2>
<p>The Legality Principle is a language agnostic principle maintaining that syllable
onsets and codas (the beginning and ends of syllables not including the vowel)
are only legal if they are found as word onsets or codas in the language. The English
word <a href="#id1"><span class="problematic" id="id2">``</span></a>admit’’ must then be syllabified as <a href="#id3"><span class="problematic" id="id4">``</span></a>ad-mit’’ since <a href="#id5"><span class="problematic" id="id6">``</span></a>dm’’ is not found
word-initially in the English language (Bartlett et al.). This principle was first proposed
in Daniel Kahn’s 1976 dissertation, <a href="#id7"><span class="problematic" id="id8">``</span></a>Syllable-based generalizations in English phonology’’.</p>
<p>Kahn further argues that there is a <a href="#id9"><span class="problematic" id="id10">``</span></a>strong tendency to syllabify in such a way that
initial clusters are of maximal length, consistent with the general constraints on
word-initial consonant clusters.’’ Consequently, in addition to being legal onsets,
the longest legal onset is preferable—<a href="#id11"><span class="problematic" id="id12">``</span></a>Onset Maximization’’.</p>
<p>The default implementation assumes an English vowel set, but the <cite>vowels</cite> attribute
can be set to IPA or any other alphabet’s vowel set for the use-case.
Both a valid set of vowels as well as a text corpus of words in the language
are necessary to determine legal onsets and subsequently syllabify words.</p>
<p>The legality principle with onset maximization is a universal syllabification algorithm,
but that does not mean it performs equally across languages. Bartlett et al. (2009) 
is a good benchmark for English accuracy if utilizing IPA (pg. 311).</p>
<p>References:
- Otto Jespersen. 1904. Lehrbuch der Phonetik.</p>
<blockquote>
<div><p>Leipzig, Teubner. Chapter 13, Silbe, pp. 185-203.</p>
</div></blockquote>
<ul class="simple">
<li><p>Theo Vennemann, <a href="#id13"><span class="problematic" id="id14">``</span></a>On the Theory of Syllabic Phonology,’’ 1972, p. 11.</p></li>
<li><p>Daniel Kahn, <a href="#id15"><span class="problematic" id="id16">``</span></a>Syllable-based generalizations in English phonology’’, (PhD diss., MIT, 1976).</p></li>
<li><p>Elisabeth Selkirk. 1984. On the major class features and syllable theory.
In Aronoff &amp; Oehrle (eds.) Language Sound Structure: Studies in Phonology.
Cambridge, MIT Press. pp. 107-136.</p></li>
<li><p>Jeremy Goslin and Ulrich Frauenfelder. 2001. A comparison of theoretical and human syllabification. Language and Speech, 44:409–436.</p></li>
<li><p>Susan Bartlett, et al. 2009. On the Syllabification of Phonemes.
In HLT-NAACL. pp. 308-316.</p></li>
<li><p>Christopher Hench. 2017. Resonances in Middle High German: New Methodologies in Prosody. UC Berkeley.</p></li>
</ul>
<dl class="py class">
<dt id="nltk.tokenize.legality_principle.LegalitySyllableTokenizer">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">nltk.tokenize.legality_principle.</span></code><code class="sig-name descname"><span class="pre">LegalitySyllableTokenizer</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tokenized_source_text</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">vowels</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'aeiouy'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">legal_frequency_threshold</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tokenize/legality_principle.html#LegalitySyllableTokenizer"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.legality_principle.LegalitySyllableTokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#nltk.tokenize.api.TokenizerI" title="nltk.tokenize.api.TokenizerI"><code class="xref py py-class docutils literal notranslate"><span class="pre">nltk.tokenize.api.TokenizerI</span></code></a></p>
<p>Syllabifies words based on the Legality Principle and Onset Maximization.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">nltk.tokenize</span> <span class="kn">import</span> <span class="n">LegalitySyllableTokenizer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">nltk</span> <span class="kn">import</span> <span class="n">word_tokenize</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">nltk.corpus</span> <span class="kn">import</span> <span class="n">words</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">text</span> <span class="o">=</span> <span class="s2">&quot;This is a wonderful sentence.&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">text_words</span> <span class="o">=</span> <span class="n">word_tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">LP</span> <span class="o">=</span> <span class="n">LegalitySyllableTokenizer</span><span class="p">(</span><span class="n">words</span><span class="o">.</span><span class="n">words</span><span class="p">())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="n">LP</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">word</span><span class="p">)</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">text_words</span><span class="p">]</span>
<span class="go">[[&#39;This&#39;], [&#39;is&#39;], [&#39;a&#39;], [&#39;won&#39;, &#39;der&#39;, &#39;ful&#39;], [&#39;sen&#39;, &#39;ten&#39;, &#39;ce&#39;], [&#39;.&#39;]]</span>
</pre></div>
</div>
<dl class="py method">
<dt id="nltk.tokenize.legality_principle.LegalitySyllableTokenizer.find_legal_onsets">
<code class="sig-name descname"><span class="pre">find_legal_onsets</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">words</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tokenize/legality_principle.html#LegalitySyllableTokenizer.find_legal_onsets"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.legality_principle.LegalitySyllableTokenizer.find_legal_onsets" title="Permalink to this definition">¶</a></dt>
<dd><p>Gathers all onsets and then return only those above the frequency threshold</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>words</strong> (<em>list</em><em>(</em><em>str</em><em>)</em>) – List of words in a language</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Set of legal onsets</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>set(str)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="nltk.tokenize.legality_principle.LegalitySyllableTokenizer.onset">
<code class="sig-name descname"><span class="pre">onset</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">word</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tokenize/legality_principle.html#LegalitySyllableTokenizer.onset"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.legality_principle.LegalitySyllableTokenizer.onset" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns consonant cluster of word, i.e. all characters until the first vowel.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>word</strong> (<em>str</em>) – Single word or token</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>String of characters of onset</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>str</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="nltk.tokenize.legality_principle.LegalitySyllableTokenizer.tokenize">
<code class="sig-name descname"><span class="pre">tokenize</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">token</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tokenize/legality_principle.html#LegalitySyllableTokenizer.tokenize"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.legality_principle.LegalitySyllableTokenizer.tokenize" title="Permalink to this definition">¶</a></dt>
<dd><p>Apply the Legality Principle in combination with
Onset Maximization to return a list of syllables.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>token</strong> (<em>str</em>) – Single word or token</p>
</dd>
<dt class="field-even">Return syllable_list</dt>
<dd class="field-even"><p>Single word or token broken up into syllables.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>list(str)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-nltk.tokenize.mwe">
<span id="nltk-tokenize-mwe-module"></span><h2>nltk.tokenize.mwe module<a class="headerlink" href="#module-nltk.tokenize.mwe" title="Permalink to this headline">¶</a></h2>
<p>Multi-Word Expression Tokenizer</p>
<p>A <code class="docutils literal notranslate"><span class="pre">MWETokenizer</span></code> takes a string which has already been divided into tokens and
retokenizes it, merging multi-word expressions into single tokens, using a lexicon
of MWEs:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">nltk.tokenize</span> <span class="kn">import</span> <span class="n">MWETokenizer</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">MWETokenizer</span><span class="p">([(</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;little&#39;</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;little&#39;</span><span class="p">,</span> <span class="s1">&#39;bit&#39;</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;lot&#39;</span><span class="p">)])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span><span class="o">.</span><span class="n">add_mwe</span><span class="p">((</span><span class="s1">&#39;in&#39;</span><span class="p">,</span> <span class="s1">&#39;spite&#39;</span><span class="p">,</span> <span class="s1">&#39;of&#39;</span><span class="p">))</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="s1">&#39;Testing testing testing one two three&#39;</span><span class="o">.</span><span class="n">split</span><span class="p">())</span>
<span class="go">[&#39;Testing&#39;, &#39;testing&#39;, &#39;testing&#39;, &#39;one&#39;, &#39;two&#39;, &#39;three&#39;]</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="s1">&#39;This is a test in spite&#39;</span><span class="o">.</span><span class="n">split</span><span class="p">())</span>
<span class="go">[&#39;This&#39;, &#39;is&#39;, &#39;a&#39;, &#39;test&#39;, &#39;in&#39;, &#39;spite&#39;]</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="s1">&#39;In a little or a little bit or a lot in spite of&#39;</span><span class="o">.</span><span class="n">split</span><span class="p">())</span>
<span class="go">[&#39;In&#39;, &#39;a_little&#39;, &#39;or&#39;, &#39;a_little_bit&#39;, &#39;or&#39;, &#39;a_lot&#39;, &#39;in_spite_of&#39;]</span>
</pre></div>
</div>
<dl class="py class">
<dt id="nltk.tokenize.mwe.MWETokenizer">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">nltk.tokenize.mwe.</span></code><code class="sig-name descname"><span class="pre">MWETokenizer</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">mwes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">separator</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'_'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tokenize/mwe.html#MWETokenizer"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.mwe.MWETokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#nltk.tokenize.api.TokenizerI" title="nltk.tokenize.api.TokenizerI"><code class="xref py py-class docutils literal notranslate"><span class="pre">nltk.tokenize.api.TokenizerI</span></code></a></p>
<p>A tokenizer that processes tokenized text and merges multi-word expressions
into single tokens.</p>
<dl class="py method">
<dt id="nltk.tokenize.mwe.MWETokenizer.add_mwe">
<code class="sig-name descname"><span class="pre">add_mwe</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">mwe</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tokenize/mwe.html#MWETokenizer.add_mwe"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.mwe.MWETokenizer.add_mwe" title="Permalink to this definition">¶</a></dt>
<dd><p>Add a multi-word expression to the lexicon (stored as a word trie)</p>
<p>We use <code class="docutils literal notranslate"><span class="pre">util.Trie</span></code> to represent the trie. Its form is a dict of dicts. 
The key True marks the end of a valid MWE.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>mwe</strong> (<em>tuple</em><em>(</em><em>str</em><em>) or </em><em>list</em><em>(</em><em>str</em><em>)</em>) – The multi-word expression we’re adding into the word trie</p>
</dd>
<dt class="field-even">Example</dt>
<dd class="field-even"><p></p></dd>
</dl>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">MWETokenizer</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span><span class="o">.</span><span class="n">add_mwe</span><span class="p">((</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span><span class="o">.</span><span class="n">add_mwe</span><span class="p">((</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;c&#39;</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span><span class="o">.</span><span class="n">add_mwe</span><span class="p">((</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;x&#39;</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">expected</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;a&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="p">{</span><span class="kc">True</span><span class="p">:</span> <span class="kc">None</span><span class="p">},</span> <span class="s1">&#39;b&#39;</span><span class="p">:</span> <span class="p">{</span><span class="kc">True</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;c&#39;</span><span class="p">:</span> <span class="p">{</span><span class="kc">True</span><span class="p">:</span> <span class="kc">None</span><span class="p">}}}}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span><span class="o">.</span><span class="n">_mwes</span> <span class="o">==</span> <span class="n">expected</span>
<span class="go">True</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt id="nltk.tokenize.mwe.MWETokenizer.tokenize">
<code class="sig-name descname"><span class="pre">tokenize</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">text</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tokenize/mwe.html#MWETokenizer.tokenize"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.mwe.MWETokenizer.tokenize" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>text</strong> (<em>list</em><em>(</em><em>str</em><em>)</em>) – A list containing tokenized text</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A list of the tokenized text with multi-words merged together</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>list(str)</p>
</dd>
<dt class="field-even">Example</dt>
<dd class="field-even"><p></p></dd>
</dl>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">MWETokenizer</span><span class="p">([(</span><span class="s1">&#39;hors&#39;</span><span class="p">,</span> <span class="s2">&quot;d&#39;oeuvre&quot;</span><span class="p">)],</span> <span class="n">separator</span><span class="o">=</span><span class="s1">&#39;+&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="s2">&quot;An hors d&#39;oeuvre tonight, sir?&quot;</span><span class="o">.</span><span class="n">split</span><span class="p">())</span>
<span class="go">[&#39;An&#39;, &quot;hors+d&#39;oeuvre&quot;, &#39;tonight,&#39;, &#39;sir?&#39;]</span>
</pre></div>
</div>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="nltk-tokenize-nist-module">
<h2>nltk.tokenize.nist module<a class="headerlink" href="#nltk-tokenize-nist-module" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="module-nltk.tokenize.punkt">
<span id="nltk-tokenize-punkt-module"></span><h2>nltk.tokenize.punkt module<a class="headerlink" href="#module-nltk.tokenize.punkt" title="Permalink to this headline">¶</a></h2>
<p>Punkt Sentence Tokenizer</p>
<p>This tokenizer divides a text into a list of sentences
by using an unsupervised algorithm to build a model for abbreviation
words, collocations, and words that start sentences.  It must be
trained on a large collection of plaintext in the target language
before it can be used.</p>
<p>The NLTK data package includes a pre-trained Punkt tokenizer for
English.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">nltk.data</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;&#39;&#39;</span>
<span class="gp">... </span><span class="s1">Punkt knows that the periods in Mr. Smith and Johann S. Bach</span>
<span class="gp">... </span><span class="s1">do not mark sentence boundaries.  And sometimes sentences</span>
<span class="gp">... </span><span class="s1">can start with non-capitalized words.  i is a good variable</span>
<span class="gp">... </span><span class="s1">name.</span>
<span class="gp">... </span><span class="s1">&#39;&#39;&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sent_detector</span> <span class="o">=</span> <span class="n">nltk</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;tokenizers/punkt/english.pickle&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">-----</span><span class="se">\n</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">sent_detector</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">text</span><span class="o">.</span><span class="n">strip</span><span class="p">())))</span>
<span class="go">Punkt knows that the periods in Mr. Smith and Johann S. Bach</span>
<span class="go">do not mark sentence boundaries.</span>
<span class="go">-----</span>
<span class="go">And sometimes sentences</span>
<span class="go">can start with non-capitalized words.</span>
<span class="go">-----</span>
<span class="go">i is a good variable</span>
<span class="go">name.</span>
</pre></div>
</div>
<p>(Note that whitespace from the original text, including newlines, is
retained in the output.)</p>
<p>Punctuation following sentences is also included by default
(from NLTK 3.0 onwards). It can be excluded with the realign_boundaries
flag.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;&#39;&#39;</span>
<span class="gp">... </span><span class="s1">(How does it deal with this parenthesis?)  &quot;It should be part of the</span>
<span class="gp">... </span><span class="s1">previous sentence.&quot; &quot;(And the same with this one.)&quot; (&#39;And this one!&#39;)</span>
<span class="gp">... </span><span class="s1">&quot;(&#39;(And (this)) &#39;?)&quot; [(and this. )]</span>
<span class="gp">... </span><span class="s1">&#39;&#39;&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">-----</span><span class="se">\n</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">sent_detector</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">text</span><span class="o">.</span><span class="n">strip</span><span class="p">())))</span>
<span class="go">(How does it deal with this parenthesis?)</span>
<span class="go">-----</span>
<span class="go">&quot;It should be part of the</span>
<span class="go">previous sentence.&quot;</span>
<span class="go">-----</span>
<span class="go">&quot;(And the same with this one.)&quot;</span>
<span class="go">-----</span>
<span class="go">(&#39;And this one!&#39;)</span>
<span class="go">-----</span>
<span class="go">&quot;(&#39;(And (this)) &#39;?)&quot;</span>
<span class="go">-----</span>
<span class="go">[(and this. )]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">-----</span><span class="se">\n</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">sent_detector</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">text</span><span class="o">.</span><span class="n">strip</span><span class="p">(),</span> <span class="n">realign_boundaries</span><span class="o">=</span><span class="kc">False</span><span class="p">)))</span>
<span class="go">(How does it deal with this parenthesis?</span>
<span class="go">-----</span>
<span class="go">)  &quot;It should be part of the</span>
<span class="go">previous sentence.</span>
<span class="go">-----</span>
<span class="go">&quot; &quot;(And the same with this one.</span>
<span class="go">-----</span>
<span class="go">)&quot; (&#39;And this one!</span>
<span class="go">-----</span>
<span class="go">&#39;)</span>
<span class="go">&quot;(&#39;(And (this)) &#39;?</span>
<span class="go">-----</span>
<span class="go">)&quot; [(and this.</span>
<span class="go">-----</span>
<span class="go">)]</span>
</pre></div>
</div>
<p>However, Punkt is designed to learn parameters (a list of abbreviations, etc.)
unsupervised from a corpus similar to the target domain. The pre-packaged models
may therefore be unsuitable: use <code class="docutils literal notranslate"><span class="pre">PunktSentenceTokenizer(text)</span></code> to learn
parameters from the given text.</p>
<p><a class="reference internal" href="#nltk.tokenize.punkt.PunktTrainer" title="nltk.tokenize.punkt.PunktTrainer"><code class="xref py py-class docutils literal notranslate"><span class="pre">PunktTrainer</span></code></a> learns parameters such as a list of abbreviations
(without supervision) from portions of text. Using a <code class="docutils literal notranslate"><span class="pre">PunktTrainer</span></code> directly
allows for incremental training and modification of the hyper-parameters used
to decide what is considered an abbreviation, etc.</p>
<p>The algorithm for this tokenizer is described in:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Kiss</span><span class="p">,</span> <span class="n">Tibor</span> <span class="ow">and</span> <span class="n">Strunk</span><span class="p">,</span> <span class="n">Jan</span> <span class="p">(</span><span class="mi">2006</span><span class="p">):</span> <span class="n">Unsupervised</span> <span class="n">Multilingual</span> <span class="n">Sentence</span>
  <span class="n">Boundary</span> <span class="n">Detection</span><span class="o">.</span>  <span class="n">Computational</span> <span class="n">Linguistics</span> <span class="mi">32</span><span class="p">:</span> <span class="mi">485</span><span class="o">-</span><span class="mf">525.</span>
</pre></div>
</div>
<dl class="py class">
<dt id="nltk.tokenize.punkt.PunktBaseClass">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">nltk.tokenize.punkt.</span></code><code class="sig-name descname"><span class="pre">PunktBaseClass</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="pre">lang_vars=None</span></em>, <em class="sig-param"><span class="pre">token_cls=&lt;class</span> <span class="pre">'nltk.tokenize.punkt.PunktToken'&gt;</span></em>, <em class="sig-param"><span class="pre">params=None</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tokenize/punkt.html#PunktBaseClass"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.punkt.PunktBaseClass" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Includes common components of PunktTrainer and PunktSentenceTokenizer.</p>
</dd></dl>

<dl class="py class">
<dt id="nltk.tokenize.punkt.PunktLanguageVars">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">nltk.tokenize.punkt.</span></code><code class="sig-name descname"><span class="pre">PunktLanguageVars</span></code><a class="reference internal" href="../_modules/nltk/tokenize/punkt.html#PunktLanguageVars"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.punkt.PunktLanguageVars" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Stores variables, mostly regular expressions, which may be
language-dependent for correct application of the algorithm.
An extension of this class may modify its properties to suit
a language other than English; an instance can then be passed
as an argument to PunktSentenceTokenizer and PunktTrainer
constructors.</p>
<dl class="py attribute">
<dt id="nltk.tokenize.punkt.PunktLanguageVars.internal_punctuation">
<code class="sig-name descname"><span class="pre">internal_punctuation</span></code><em class="property"> <span class="pre">=</span> <span class="pre">',:;'</span></em><a class="headerlink" href="#nltk.tokenize.punkt.PunktLanguageVars.internal_punctuation" title="Permalink to this definition">¶</a></dt>
<dd><p>sentence internal punctuation, which indicates an abbreviation if
preceded by a period-final token.</p>
</dd></dl>

<dl class="py method">
<dt id="nltk.tokenize.punkt.PunktLanguageVars.period_context_re">
<code class="sig-name descname"><span class="pre">period_context_re</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tokenize/punkt.html#PunktLanguageVars.period_context_re"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.punkt.PunktLanguageVars.period_context_re" title="Permalink to this definition">¶</a></dt>
<dd><p>Compiles and returns a regular expression to find contexts
including possible sentence boundaries.</p>
</dd></dl>

<dl class="py attribute">
<dt id="nltk.tokenize.punkt.PunktLanguageVars.re_boundary_realignment">
<code class="sig-name descname"><span class="pre">re_boundary_realignment</span></code><em class="property"> <span class="pre">=</span> <span class="pre">re.compile('[&quot;\\\')\\]}]+?(?:\\s+|(?=--)|$)',</span> <span class="pre">re.MULTILINE)</span></em><a class="headerlink" href="#nltk.tokenize.punkt.PunktLanguageVars.re_boundary_realignment" title="Permalink to this definition">¶</a></dt>
<dd><p>Used to realign punctuation that should be included in a sentence
although it follows the period (or ?, !).</p>
</dd></dl>

<dl class="py attribute">
<dt id="nltk.tokenize.punkt.PunktLanguageVars.sent_end_chars">
<code class="sig-name descname"><span class="pre">sent_end_chars</span></code><em class="property"> <span class="pre">=</span> <span class="pre">('.',</span> <span class="pre">'?',</span> <span class="pre">'!')</span></em><a class="headerlink" href="#nltk.tokenize.punkt.PunktLanguageVars.sent_end_chars" title="Permalink to this definition">¶</a></dt>
<dd><p>Characters which are candidates for sentence boundaries</p>
</dd></dl>

<dl class="py method">
<dt id="nltk.tokenize.punkt.PunktLanguageVars.word_tokenize">
<code class="sig-name descname"><span class="pre">word_tokenize</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">s</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tokenize/punkt.html#PunktLanguageVars.word_tokenize"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.punkt.PunktLanguageVars.word_tokenize" title="Permalink to this definition">¶</a></dt>
<dd><p>Tokenize a string to split off punctuation other than periods</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="nltk.tokenize.punkt.PunktParameters">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">nltk.tokenize.punkt.</span></code><code class="sig-name descname"><span class="pre">PunktParameters</span></code><a class="reference internal" href="../_modules/nltk/tokenize/punkt.html#PunktParameters"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.punkt.PunktParameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Stores data used to perform sentence boundary detection with Punkt.</p>
<dl class="py attribute">
<dt id="nltk.tokenize.punkt.PunktParameters.abbrev_types">
<code class="sig-name descname"><span class="pre">abbrev_types</span></code><a class="headerlink" href="#nltk.tokenize.punkt.PunktParameters.abbrev_types" title="Permalink to this definition">¶</a></dt>
<dd><p>A set of word types for known abbreviations.</p>
</dd></dl>

<dl class="py method">
<dt id="nltk.tokenize.punkt.PunktParameters.add_ortho_context">
<code class="sig-name descname"><span class="pre">add_ortho_context</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">typ</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">flag</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tokenize/punkt.html#PunktParameters.add_ortho_context"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.punkt.PunktParameters.add_ortho_context" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="nltk.tokenize.punkt.PunktParameters.clear_abbrevs">
<code class="sig-name descname"><span class="pre">clear_abbrevs</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tokenize/punkt.html#PunktParameters.clear_abbrevs"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.punkt.PunktParameters.clear_abbrevs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="nltk.tokenize.punkt.PunktParameters.clear_collocations">
<code class="sig-name descname"><span class="pre">clear_collocations</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tokenize/punkt.html#PunktParameters.clear_collocations"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.punkt.PunktParameters.clear_collocations" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="nltk.tokenize.punkt.PunktParameters.clear_ortho_context">
<code class="sig-name descname"><span class="pre">clear_ortho_context</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tokenize/punkt.html#PunktParameters.clear_ortho_context"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.punkt.PunktParameters.clear_ortho_context" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="nltk.tokenize.punkt.PunktParameters.clear_sent_starters">
<code class="sig-name descname"><span class="pre">clear_sent_starters</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tokenize/punkt.html#PunktParameters.clear_sent_starters"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.punkt.PunktParameters.clear_sent_starters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="nltk.tokenize.punkt.PunktParameters.collocations">
<code class="sig-name descname"><span class="pre">collocations</span></code><a class="headerlink" href="#nltk.tokenize.punkt.PunktParameters.collocations" title="Permalink to this definition">¶</a></dt>
<dd><p>A set of word type tuples for known common collocations
where the first word ends in a period.  E.g., (‘S.’, ‘Bach’)
is a common collocation in a text that discusses ‘Johann
S. Bach’.  These count as negative evidence for sentence
boundaries.</p>
</dd></dl>

<dl class="py attribute">
<dt id="nltk.tokenize.punkt.PunktParameters.ortho_context">
<code class="sig-name descname"><span class="pre">ortho_context</span></code><a class="headerlink" href="#nltk.tokenize.punkt.PunktParameters.ortho_context" title="Permalink to this definition">¶</a></dt>
<dd><p>A dictionary mapping word types to the set of orthographic
contexts that word type appears in.  Contexts are represented
by adding orthographic context flags: …</p>
</dd></dl>

<dl class="py attribute">
<dt id="nltk.tokenize.punkt.PunktParameters.sent_starters">
<code class="sig-name descname"><span class="pre">sent_starters</span></code><a class="headerlink" href="#nltk.tokenize.punkt.PunktParameters.sent_starters" title="Permalink to this definition">¶</a></dt>
<dd><p>A set of word types for words that often appear at the
beginning of sentences.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="nltk.tokenize.punkt.PunktSentenceTokenizer">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">nltk.tokenize.punkt.</span></code><code class="sig-name descname"><span class="pre">PunktSentenceTokenizer</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="pre">train_text=None</span></em>, <em class="sig-param"><span class="pre">verbose=False</span></em>, <em class="sig-param"><span class="pre">lang_vars=None</span></em>, <em class="sig-param"><span class="pre">token_cls=&lt;class</span> <span class="pre">'nltk.tokenize.punkt.PunktToken'&gt;</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tokenize/punkt.html#PunktSentenceTokenizer"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.punkt.PunktSentenceTokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#nltk.tokenize.punkt.PunktBaseClass" title="nltk.tokenize.punkt.PunktBaseClass"><code class="xref py py-class docutils literal notranslate"><span class="pre">nltk.tokenize.punkt.PunktBaseClass</span></code></a>, <a class="reference internal" href="#nltk.tokenize.api.TokenizerI" title="nltk.tokenize.api.TokenizerI"><code class="xref py py-class docutils literal notranslate"><span class="pre">nltk.tokenize.api.TokenizerI</span></code></a></p>
<p>A sentence tokenizer which uses an unsupervised algorithm to build
a model for abbreviation words, collocations, and words that start
sentences; and then uses that model to find sentence boundaries.
This approach has been shown to work well for many European
languages.</p>
<dl class="py attribute">
<dt id="nltk.tokenize.punkt.PunktSentenceTokenizer.PUNCTUATION">
<code class="sig-name descname"><span class="pre">PUNCTUATION</span></code><em class="property"> <span class="pre">=</span> <span class="pre">(';',</span> <span class="pre">':',</span> <span class="pre">',',</span> <span class="pre">'.',</span> <span class="pre">'!',</span> <span class="pre">'?')</span></em><a class="headerlink" href="#nltk.tokenize.punkt.PunktSentenceTokenizer.PUNCTUATION" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="nltk.tokenize.punkt.PunktSentenceTokenizer.debug_decisions">
<code class="sig-name descname"><span class="pre">debug_decisions</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">text</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tokenize/punkt.html#PunktSentenceTokenizer.debug_decisions"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.punkt.PunktSentenceTokenizer.debug_decisions" title="Permalink to this definition">¶</a></dt>
<dd><p>Classifies candidate periods as sentence breaks, yielding a dict for
each that may be used to understand why the decision was made.</p>
<p>See format_debug_decision() to help make this output readable.</p>
</dd></dl>

<dl class="py method">
<dt id="nltk.tokenize.punkt.PunktSentenceTokenizer.dump">
<code class="sig-name descname"><span class="pre">dump</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tokens</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tokenize/punkt.html#PunktSentenceTokenizer.dump"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.punkt.PunktSentenceTokenizer.dump" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="nltk.tokenize.punkt.PunktSentenceTokenizer.sentences_from_text">
<code class="sig-name descname"><span class="pre">sentences_from_text</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">text</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">realign_boundaries</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tokenize/punkt.html#PunktSentenceTokenizer.sentences_from_text"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.punkt.PunktSentenceTokenizer.sentences_from_text" title="Permalink to this definition">¶</a></dt>
<dd><p>Given a text, generates the sentences in that text by only
testing candidate sentence breaks. If realign_boundaries is
True, includes in the sentence closing punctuation that
follows the period.</p>
</dd></dl>

<dl class="py method">
<dt id="nltk.tokenize.punkt.PunktSentenceTokenizer.sentences_from_text_legacy">
<code class="sig-name descname"><span class="pre">sentences_from_text_legacy</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">text</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tokenize/punkt.html#PunktSentenceTokenizer.sentences_from_text_legacy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.punkt.PunktSentenceTokenizer.sentences_from_text_legacy" title="Permalink to this definition">¶</a></dt>
<dd><p>Given a text, generates the sentences in that text. Annotates all
tokens, rather than just those with possible sentence breaks. Should
produce the same results as <code class="docutils literal notranslate"><span class="pre">sentences_from_text</span></code>.</p>
</dd></dl>

<dl class="py method">
<dt id="nltk.tokenize.punkt.PunktSentenceTokenizer.sentences_from_tokens">
<code class="sig-name descname"><span class="pre">sentences_from_tokens</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tokens</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tokenize/punkt.html#PunktSentenceTokenizer.sentences_from_tokens"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.punkt.PunktSentenceTokenizer.sentences_from_tokens" title="Permalink to this definition">¶</a></dt>
<dd><p>Given a sequence of tokens, generates lists of tokens, each list
corresponding to a sentence.</p>
</dd></dl>

<dl class="py method">
<dt id="nltk.tokenize.punkt.PunktSentenceTokenizer.span_tokenize">
<code class="sig-name descname"><span class="pre">span_tokenize</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">text</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">realign_boundaries</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tokenize/punkt.html#PunktSentenceTokenizer.span_tokenize"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.punkt.PunktSentenceTokenizer.span_tokenize" title="Permalink to this definition">¶</a></dt>
<dd><p>Given a text, generates (start, end) spans of sentences
in the text.</p>
</dd></dl>

<dl class="py method">
<dt id="nltk.tokenize.punkt.PunktSentenceTokenizer.text_contains_sentbreak">
<code class="sig-name descname"><span class="pre">text_contains_sentbreak</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">text</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tokenize/punkt.html#PunktSentenceTokenizer.text_contains_sentbreak"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.punkt.PunktSentenceTokenizer.text_contains_sentbreak" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns True if the given text includes a sentence break.</p>
</dd></dl>

<dl class="py method">
<dt id="nltk.tokenize.punkt.PunktSentenceTokenizer.tokenize">
<code class="sig-name descname"><span class="pre">tokenize</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">text</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">realign_boundaries</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tokenize/punkt.html#PunktSentenceTokenizer.tokenize"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.punkt.PunktSentenceTokenizer.tokenize" title="Permalink to this definition">¶</a></dt>
<dd><p>Given a text, returns a list of the sentences in that text.</p>
</dd></dl>

<dl class="py method">
<dt id="nltk.tokenize.punkt.PunktSentenceTokenizer.train">
<code class="sig-name descname"><span class="pre">train</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">train_text</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tokenize/punkt.html#PunktSentenceTokenizer.train"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.punkt.PunktSentenceTokenizer.train" title="Permalink to this definition">¶</a></dt>
<dd><p>Derives parameters from a given training text, or uses the parameters
given. Repeated calls to this method destroy previous parameters. For
incremental training, instantiate a separate PunktTrainer instance.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="nltk.tokenize.punkt.PunktToken">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">nltk.tokenize.punkt.</span></code><code class="sig-name descname"><span class="pre">PunktToken</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tok</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">params</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tokenize/punkt.html#PunktToken"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.punkt.PunktToken" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Stores a token of text with annotations produced during
sentence boundary detection.</p>
<dl class="py attribute">
<dt id="nltk.tokenize.punkt.PunktToken.abbr">
<code class="sig-name descname"><span class="pre">abbr</span></code><a class="headerlink" href="#nltk.tokenize.punkt.PunktToken.abbr" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="nltk.tokenize.punkt.PunktToken.ellipsis">
<code class="sig-name descname"><span class="pre">ellipsis</span></code><a class="headerlink" href="#nltk.tokenize.punkt.PunktToken.ellipsis" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="nltk.tokenize.punkt.PunktToken.first_case">
<em class="property"><span class="pre">property</span> </em><code class="sig-name descname"><span class="pre">first_case</span></code><a class="headerlink" href="#nltk.tokenize.punkt.PunktToken.first_case" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="nltk.tokenize.punkt.PunktToken.first_lower">
<em class="property"><span class="pre">property</span> </em><code class="sig-name descname"><span class="pre">first_lower</span></code><a class="headerlink" href="#nltk.tokenize.punkt.PunktToken.first_lower" title="Permalink to this definition">¶</a></dt>
<dd><p>True if the token’s first character is lowercase.</p>
</dd></dl>

<dl class="py method">
<dt id="nltk.tokenize.punkt.PunktToken.first_upper">
<em class="property"><span class="pre">property</span> </em><code class="sig-name descname"><span class="pre">first_upper</span></code><a class="headerlink" href="#nltk.tokenize.punkt.PunktToken.first_upper" title="Permalink to this definition">¶</a></dt>
<dd><p>True if the token’s first character is uppercase.</p>
</dd></dl>

<dl class="py method">
<dt id="nltk.tokenize.punkt.PunktToken.is_alpha">
<em class="property"><span class="pre">property</span> </em><code class="sig-name descname"><span class="pre">is_alpha</span></code><a class="headerlink" href="#nltk.tokenize.punkt.PunktToken.is_alpha" title="Permalink to this definition">¶</a></dt>
<dd><p>True if the token text is all alphabetic.</p>
</dd></dl>

<dl class="py method">
<dt id="nltk.tokenize.punkt.PunktToken.is_ellipsis">
<em class="property"><span class="pre">property</span> </em><code class="sig-name descname"><span class="pre">is_ellipsis</span></code><a class="headerlink" href="#nltk.tokenize.punkt.PunktToken.is_ellipsis" title="Permalink to this definition">¶</a></dt>
<dd><p>True if the token text is that of an ellipsis.</p>
</dd></dl>

<dl class="py method">
<dt id="nltk.tokenize.punkt.PunktToken.is_initial">
<em class="property"><span class="pre">property</span> </em><code class="sig-name descname"><span class="pre">is_initial</span></code><a class="headerlink" href="#nltk.tokenize.punkt.PunktToken.is_initial" title="Permalink to this definition">¶</a></dt>
<dd><p>True if the token text is that of an initial.</p>
</dd></dl>

<dl class="py method">
<dt id="nltk.tokenize.punkt.PunktToken.is_non_punct">
<em class="property"><span class="pre">property</span> </em><code class="sig-name descname"><span class="pre">is_non_punct</span></code><a class="headerlink" href="#nltk.tokenize.punkt.PunktToken.is_non_punct" title="Permalink to this definition">¶</a></dt>
<dd><p>True if the token is either a number or is alphabetic.</p>
</dd></dl>

<dl class="py method">
<dt id="nltk.tokenize.punkt.PunktToken.is_number">
<em class="property"><span class="pre">property</span> </em><code class="sig-name descname"><span class="pre">is_number</span></code><a class="headerlink" href="#nltk.tokenize.punkt.PunktToken.is_number" title="Permalink to this definition">¶</a></dt>
<dd><p>True if the token text is that of a number.</p>
</dd></dl>

<dl class="py attribute">
<dt id="nltk.tokenize.punkt.PunktToken.linestart">
<code class="sig-name descname"><span class="pre">linestart</span></code><a class="headerlink" href="#nltk.tokenize.punkt.PunktToken.linestart" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="nltk.tokenize.punkt.PunktToken.parastart">
<code class="sig-name descname"><span class="pre">parastart</span></code><a class="headerlink" href="#nltk.tokenize.punkt.PunktToken.parastart" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="nltk.tokenize.punkt.PunktToken.period_final">
<code class="sig-name descname"><span class="pre">period_final</span></code><a class="headerlink" href="#nltk.tokenize.punkt.PunktToken.period_final" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="nltk.tokenize.punkt.PunktToken.sentbreak">
<code class="sig-name descname"><span class="pre">sentbreak</span></code><a class="headerlink" href="#nltk.tokenize.punkt.PunktToken.sentbreak" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="nltk.tokenize.punkt.PunktToken.tok">
<code class="sig-name descname"><span class="pre">tok</span></code><a class="headerlink" href="#nltk.tokenize.punkt.PunktToken.tok" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="nltk.tokenize.punkt.PunktToken.type">
<code class="sig-name descname"><span class="pre">type</span></code><a class="headerlink" href="#nltk.tokenize.punkt.PunktToken.type" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="nltk.tokenize.punkt.PunktToken.type_no_period">
<em class="property"><span class="pre">property</span> </em><code class="sig-name descname"><span class="pre">type_no_period</span></code><a class="headerlink" href="#nltk.tokenize.punkt.PunktToken.type_no_period" title="Permalink to this definition">¶</a></dt>
<dd><p>The type with its final period removed if it has one.</p>
</dd></dl>

<dl class="py method">
<dt id="nltk.tokenize.punkt.PunktToken.type_no_sentperiod">
<em class="property"><span class="pre">property</span> </em><code class="sig-name descname"><span class="pre">type_no_sentperiod</span></code><a class="headerlink" href="#nltk.tokenize.punkt.PunktToken.type_no_sentperiod" title="Permalink to this definition">¶</a></dt>
<dd><p>The type with its final period removed if it is marked as a
sentence break.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="nltk.tokenize.punkt.PunktTrainer">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">nltk.tokenize.punkt.</span></code><code class="sig-name descname"><span class="pre">PunktTrainer</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="pre">train_text=None</span></em>, <em class="sig-param"><span class="pre">verbose=False</span></em>, <em class="sig-param"><span class="pre">lang_vars=None</span></em>, <em class="sig-param"><span class="pre">token_cls=&lt;class</span> <span class="pre">'nltk.tokenize.punkt.PunktToken'&gt;</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tokenize/punkt.html#PunktTrainer"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.punkt.PunktTrainer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#nltk.tokenize.punkt.PunktBaseClass" title="nltk.tokenize.punkt.PunktBaseClass"><code class="xref py py-class docutils literal notranslate"><span class="pre">nltk.tokenize.punkt.PunktBaseClass</span></code></a></p>
<p>Learns parameters used in Punkt sentence boundary detection.</p>
<dl class="py attribute">
<dt id="nltk.tokenize.punkt.PunktTrainer.ABBREV">
<code class="sig-name descname"><span class="pre">ABBREV</span></code><em class="property"> <span class="pre">=</span> <span class="pre">0.3</span></em><a class="headerlink" href="#nltk.tokenize.punkt.PunktTrainer.ABBREV" title="Permalink to this definition">¶</a></dt>
<dd><p>cut-off value whether a ‘token’ is an abbreviation</p>
</dd></dl>

<dl class="py attribute">
<dt id="nltk.tokenize.punkt.PunktTrainer.ABBREV_BACKOFF">
<code class="sig-name descname"><span class="pre">ABBREV_BACKOFF</span></code><em class="property"> <span class="pre">=</span> <span class="pre">5</span></em><a class="headerlink" href="#nltk.tokenize.punkt.PunktTrainer.ABBREV_BACKOFF" title="Permalink to this definition">¶</a></dt>
<dd><p>upper cut-off for Mikheev’s(2002) abbreviation detection algorithm</p>
</dd></dl>

<dl class="py attribute">
<dt id="nltk.tokenize.punkt.PunktTrainer.COLLOCATION">
<code class="sig-name descname"><span class="pre">COLLOCATION</span></code><em class="property"> <span class="pre">=</span> <span class="pre">7.88</span></em><a class="headerlink" href="#nltk.tokenize.punkt.PunktTrainer.COLLOCATION" title="Permalink to this definition">¶</a></dt>
<dd><p>minimal log-likelihood value that two tokens need to be considered
as a collocation</p>
</dd></dl>

<dl class="py attribute">
<dt id="nltk.tokenize.punkt.PunktTrainer.IGNORE_ABBREV_PENALTY">
<code class="sig-name descname"><span class="pre">IGNORE_ABBREV_PENALTY</span></code><em class="property"> <span class="pre">=</span> <span class="pre">False</span></em><a class="headerlink" href="#nltk.tokenize.punkt.PunktTrainer.IGNORE_ABBREV_PENALTY" title="Permalink to this definition">¶</a></dt>
<dd><p>allows the disabling of the abbreviation penalty heuristic, which
exponentially disadvantages words that are found at times without a
final period.</p>
</dd></dl>

<dl class="py attribute">
<dt id="nltk.tokenize.punkt.PunktTrainer.INCLUDE_ABBREV_COLLOCS">
<code class="sig-name descname"><span class="pre">INCLUDE_ABBREV_COLLOCS</span></code><em class="property"> <span class="pre">=</span> <span class="pre">False</span></em><a class="headerlink" href="#nltk.tokenize.punkt.PunktTrainer.INCLUDE_ABBREV_COLLOCS" title="Permalink to this definition">¶</a></dt>
<dd><p>this includes as potential collocations all word pairs where the first
word is an abbreviation. Such collocations override the orthographic
heuristic, but not the sentence starter heuristic. This is overridden by
INCLUDE_ALL_COLLOCS, and if both are false, only collocations with initials
and ordinals are considered.</p>
</dd></dl>

<dl class="py attribute">
<dt id="nltk.tokenize.punkt.PunktTrainer.INCLUDE_ALL_COLLOCS">
<code class="sig-name descname"><span class="pre">INCLUDE_ALL_COLLOCS</span></code><em class="property"> <span class="pre">=</span> <span class="pre">False</span></em><a class="headerlink" href="#nltk.tokenize.punkt.PunktTrainer.INCLUDE_ALL_COLLOCS" title="Permalink to this definition">¶</a></dt>
<dd><p>this includes as potential collocations all word pairs where the first
word ends in a period. It may be useful in corpora where there is a lot
of variation that makes abbreviations like Mr difficult to identify.</p>
</dd></dl>

<dl class="py attribute">
<dt id="nltk.tokenize.punkt.PunktTrainer.MIN_COLLOC_FREQ">
<code class="sig-name descname"><span class="pre">MIN_COLLOC_FREQ</span></code><em class="property"> <span class="pre">=</span> <span class="pre">1</span></em><a class="headerlink" href="#nltk.tokenize.punkt.PunktTrainer.MIN_COLLOC_FREQ" title="Permalink to this definition">¶</a></dt>
<dd><p>this sets a minimum bound on the number of times a bigram needs to
appear before it can be considered a collocation, in addition to log
likelihood statistics. This is useful when INCLUDE_ALL_COLLOCS is True.</p>
</dd></dl>

<dl class="py attribute">
<dt id="nltk.tokenize.punkt.PunktTrainer.SENT_STARTER">
<code class="sig-name descname"><span class="pre">SENT_STARTER</span></code><em class="property"> <span class="pre">=</span> <span class="pre">30</span></em><a class="headerlink" href="#nltk.tokenize.punkt.PunktTrainer.SENT_STARTER" title="Permalink to this definition">¶</a></dt>
<dd><p>minimal log-likelihood value that a token requires to be considered
as a frequent sentence starter</p>
</dd></dl>

<dl class="py method">
<dt id="nltk.tokenize.punkt.PunktTrainer.finalize_training">
<code class="sig-name descname"><span class="pre">finalize_training</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tokenize/punkt.html#PunktTrainer.finalize_training"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.punkt.PunktTrainer.finalize_training" title="Permalink to this definition">¶</a></dt>
<dd><p>Uses data that has been gathered in training to determine likely
collocations and sentence starters.</p>
</dd></dl>

<dl class="py method">
<dt id="nltk.tokenize.punkt.PunktTrainer.find_abbrev_types">
<code class="sig-name descname"><span class="pre">find_abbrev_types</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tokenize/punkt.html#PunktTrainer.find_abbrev_types"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.punkt.PunktTrainer.find_abbrev_types" title="Permalink to this definition">¶</a></dt>
<dd><p>Recalculates abbreviations given type frequencies, despite no prior
determination of abbreviations.
This fails to include abbreviations otherwise found as “rare”.</p>
</dd></dl>

<dl class="py method">
<dt id="nltk.tokenize.punkt.PunktTrainer.freq_threshold">
<code class="sig-name descname"><span class="pre">freq_threshold</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ortho_thresh</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">type_thresh</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">colloc_thres</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sentstart_thresh</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tokenize/punkt.html#PunktTrainer.freq_threshold"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.punkt.PunktTrainer.freq_threshold" title="Permalink to this definition">¶</a></dt>
<dd><p>Allows memory use to be reduced after much training by removing data
about rare tokens that are unlikely to have a statistical effect with
further training. Entries occurring above the given thresholds will be
retained.</p>
</dd></dl>

<dl class="py method">
<dt id="nltk.tokenize.punkt.PunktTrainer.get_params">
<code class="sig-name descname"><span class="pre">get_params</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tokenize/punkt.html#PunktTrainer.get_params"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.punkt.PunktTrainer.get_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculates and returns parameters for sentence boundary detection as
derived from training.</p>
</dd></dl>

<dl class="py method">
<dt id="nltk.tokenize.punkt.PunktTrainer.train">
<code class="sig-name descname"><span class="pre">train</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">text</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">finalize</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tokenize/punkt.html#PunktTrainer.train"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.punkt.PunktTrainer.train" title="Permalink to this definition">¶</a></dt>
<dd><p>Collects training data from a given text. If finalize is True, it
will determine all the parameters for sentence boundary detection. If
not, this will be delayed until get_params() or finalize_training() is
called. If verbose is True, abbreviations found will be listed.</p>
</dd></dl>

<dl class="py method">
<dt id="nltk.tokenize.punkt.PunktTrainer.train_tokens">
<code class="sig-name descname"><span class="pre">train_tokens</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tokens</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">finalize</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tokenize/punkt.html#PunktTrainer.train_tokens"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.punkt.PunktTrainer.train_tokens" title="Permalink to this definition">¶</a></dt>
<dd><p>Collects training data from a given list of tokens.</p>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt id="nltk.tokenize.punkt.demo">
<code class="sig-prename descclassname"><span class="pre">nltk.tokenize.punkt.</span></code><code class="sig-name descname"><span class="pre">demo</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="pre">text</span></em>, <em class="sig-param"><span class="pre">tok_cls=&lt;class</span> <span class="pre">'nltk.tokenize.punkt.PunktSentenceTokenizer'&gt;</span></em>, <em class="sig-param"><span class="pre">train_cls=&lt;class</span> <span class="pre">'nltk.tokenize.punkt.PunktTrainer'&gt;</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tokenize/punkt.html#demo"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.punkt.demo" title="Permalink to this definition">¶</a></dt>
<dd><p>Builds a punkt model and applies it to the same text</p>
</dd></dl>

<dl class="py function">
<dt id="nltk.tokenize.punkt.format_debug_decision">
<code class="sig-prename descclassname"><span class="pre">nltk.tokenize.punkt.</span></code><code class="sig-name descname"><span class="pre">format_debug_decision</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">d</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tokenize/punkt.html#format_debug_decision"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.punkt.format_debug_decision" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="module-nltk.tokenize.regexp">
<span id="nltk-tokenize-regexp-module"></span><h2>nltk.tokenize.regexp module<a class="headerlink" href="#module-nltk.tokenize.regexp" title="Permalink to this headline">¶</a></h2>
<p>Regular-Expression Tokenizers</p>
<p>A <code class="docutils literal notranslate"><span class="pre">RegexpTokenizer</span></code> splits a string into substrings using a regular expression.
For example, the following tokenizer forms tokens out of alphabetic sequences,
money expressions, and any other non-whitespace sequences:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">nltk.tokenize</span> <span class="kn">import</span> <span class="n">RegexpTokenizer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s</span> <span class="o">=</span> <span class="s2">&quot;Good muffins cost $3.88</span><span class="se">\n</span><span class="s2">in New York.  Please buy me</span><span class="se">\n</span><span class="s2">two of them.</span><span class="se">\n\n</span><span class="s2">Thanks.&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">RegexpTokenizer</span><span class="p">(</span><span class="s1">&#39;\w+|\$[\d\.]+|\S+&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
<span class="go">[&#39;Good&#39;, &#39;muffins&#39;, &#39;cost&#39;, &#39;$3.88&#39;, &#39;in&#39;, &#39;New&#39;, &#39;York&#39;, &#39;.&#39;,</span>
<span class="go">&#39;Please&#39;, &#39;buy&#39;, &#39;me&#39;, &#39;two&#39;, &#39;of&#39;, &#39;them&#39;, &#39;.&#39;, &#39;Thanks&#39;, &#39;.&#39;]</span>
</pre></div>
</div>
<p>A <code class="docutils literal notranslate"><span class="pre">RegexpTokenizer</span></code> can use its regexp to match delimiters instead:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">RegexpTokenizer</span><span class="p">(</span><span class="s1">&#39;\s+&#39;</span><span class="p">,</span> <span class="n">gaps</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
<span class="go">[&#39;Good&#39;, &#39;muffins&#39;, &#39;cost&#39;, &#39;$3.88&#39;, &#39;in&#39;, &#39;New&#39;, &#39;York.&#39;,</span>
<span class="go">&#39;Please&#39;, &#39;buy&#39;, &#39;me&#39;, &#39;two&#39;, &#39;of&#39;, &#39;them.&#39;, &#39;Thanks.&#39;]</span>
</pre></div>
</div>
<p>Note that empty tokens are not returned when the delimiter appears at
the start or end of the string.</p>
<p>The material between the tokens is discarded.  For example,
the following tokenizer selects just the capitalized words:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">capword_tokenizer</span> <span class="o">=</span> <span class="n">RegexpTokenizer</span><span class="p">(</span><span class="s1">&#39;[A-Z]\w+&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">capword_tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
<span class="go">[&#39;Good&#39;, &#39;New&#39;, &#39;York&#39;, &#39;Please&#39;, &#39;Thanks&#39;]</span>
</pre></div>
</div>
<p>This module contains several subclasses of <code class="docutils literal notranslate"><span class="pre">RegexpTokenizer</span></code>
that use pre-defined regular expressions.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">nltk.tokenize</span> <span class="kn">import</span> <span class="n">BlanklineTokenizer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Uses &#39;\s*\n\s*\n\s*&#39;:</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">BlanklineTokenizer</span><span class="p">()</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
<span class="go">[&#39;Good muffins cost $3.88\nin New York.  Please buy me\ntwo of them.&#39;,</span>
<span class="go">&#39;Thanks.&#39;]</span>
</pre></div>
</div>
<p>All of the regular expression tokenizers are also available as functions:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">nltk.tokenize</span> <span class="kn">import</span> <span class="n">regexp_tokenize</span><span class="p">,</span> <span class="n">wordpunct_tokenize</span><span class="p">,</span> <span class="n">blankline_tokenize</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">regexp_tokenize</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">pattern</span><span class="o">=</span><span class="s1">&#39;\w+|\$[\d\.]+|\S+&#39;</span><span class="p">)</span>
<span class="go">[&#39;Good&#39;, &#39;muffins&#39;, &#39;cost&#39;, &#39;$3.88&#39;, &#39;in&#39;, &#39;New&#39;, &#39;York&#39;, &#39;.&#39;,</span>
<span class="go">&#39;Please&#39;, &#39;buy&#39;, &#39;me&#39;, &#39;two&#39;, &#39;of&#39;, &#39;them&#39;, &#39;.&#39;, &#39;Thanks&#39;, &#39;.&#39;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">wordpunct_tokenize</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
<span class="go">[&#39;Good&#39;, &#39;muffins&#39;, &#39;cost&#39;, &#39;$&#39;, &#39;3&#39;, &#39;.&#39;, &#39;88&#39;, &#39;in&#39;, &#39;New&#39;, &#39;York&#39;,</span>
<span class="go"> &#39;.&#39;, &#39;Please&#39;, &#39;buy&#39;, &#39;me&#39;, &#39;two&#39;, &#39;of&#39;, &#39;them&#39;, &#39;.&#39;, &#39;Thanks&#39;, &#39;.&#39;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">blankline_tokenize</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
<span class="go">[&#39;Good muffins cost $3.88\nin New York.  Please buy me\ntwo of them.&#39;, &#39;Thanks.&#39;]</span>
</pre></div>
</div>
<p>Caution: The function <code class="docutils literal notranslate"><span class="pre">regexp_tokenize()</span></code> takes the text as its
first argument, and the regular expression pattern as its second
argument.  This differs from the conventions used by Python’s
<code class="docutils literal notranslate"><span class="pre">re</span></code> functions, where the pattern is always the first argument.
(This is for consistency with the other NLTK tokenizers.)</p>
<dl class="py class">
<dt id="nltk.tokenize.regexp.BlanklineTokenizer">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">nltk.tokenize.regexp.</span></code><code class="sig-name descname"><span class="pre">BlanklineTokenizer</span></code><a class="reference internal" href="../_modules/nltk/tokenize/regexp.html#BlanklineTokenizer"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.regexp.BlanklineTokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#nltk.tokenize.regexp.RegexpTokenizer" title="nltk.tokenize.regexp.RegexpTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">nltk.tokenize.regexp.RegexpTokenizer</span></code></a></p>
<p>Tokenize a string, treating any sequence of blank lines as a delimiter.
Blank lines are defined as lines containing no characters, except for
space or tab characters.</p>
</dd></dl>

<dl class="py class">
<dt id="nltk.tokenize.regexp.RegexpTokenizer">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">nltk.tokenize.regexp.</span></code><code class="sig-name descname"><span class="pre">RegexpTokenizer</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">pattern</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gaps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">discard_empty</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">flags</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">re.UNICODE</span> <span class="pre">|</span> <span class="pre">re.MULTILINE</span> <span class="pre">|</span> <span class="pre">re.DOTALL</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tokenize/regexp.html#RegexpTokenizer"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.regexp.RegexpTokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#nltk.tokenize.api.TokenizerI" title="nltk.tokenize.api.TokenizerI"><code class="xref py py-class docutils literal notranslate"><span class="pre">nltk.tokenize.api.TokenizerI</span></code></a></p>
<p>A tokenizer that splits a string using a regular expression, which
matches either the tokens or the separators between tokens.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">RegexpTokenizer</span><span class="p">(</span><span class="s1">&#39;\w+|\$[\d\.]+|\S+&#39;</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>pattern</strong> (<em>str</em>) – The pattern used to build this tokenizer.
(This pattern must not contain capturing parentheses;
Use non-capturing parentheses, e.g. (?:…), instead)</p></li>
<li><p><strong>gaps</strong> (<em>bool</em>) – True if this tokenizer’s pattern should be used
to find separators between tokens; False if this
tokenizer’s pattern should be used to find the tokens
themselves.</p></li>
<li><p><strong>discard_empty</strong> (<em>bool</em>) – True if any empty tokens <cite>‘’</cite>
generated by the tokenizer should be discarded.  Empty
tokens can only be generated if <cite>_gaps == True</cite>.</p></li>
<li><p><strong>flags</strong> (<em>int</em>) – The regexp flags used to compile this
tokenizer’s pattern.  By default, the following flags are
used: <cite>re.UNICODE | re.MULTILINE | re.DOTALL</cite>.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="nltk.tokenize.regexp.RegexpTokenizer.span_tokenize">
<code class="sig-name descname"><span class="pre">span_tokenize</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">text</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tokenize/regexp.html#RegexpTokenizer.span_tokenize"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.regexp.RegexpTokenizer.span_tokenize" title="Permalink to this definition">¶</a></dt>
<dd><p>Identify the tokens using integer offsets <code class="docutils literal notranslate"><span class="pre">(start_i,</span> <span class="pre">end_i)</span></code>,
where <code class="docutils literal notranslate"><span class="pre">s[start_i:end_i]</span></code> is the corresponding token.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>iter(tuple(int, int))</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="nltk.tokenize.regexp.RegexpTokenizer.tokenize">
<code class="sig-name descname"><span class="pre">tokenize</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">text</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tokenize/regexp.html#RegexpTokenizer.tokenize"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.regexp.RegexpTokenizer.tokenize" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a tokenized copy of <em>s</em>.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>list of str</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="nltk.tokenize.regexp.WhitespaceTokenizer">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">nltk.tokenize.regexp.</span></code><code class="sig-name descname"><span class="pre">WhitespaceTokenizer</span></code><a class="reference internal" href="../_modules/nltk/tokenize/regexp.html#WhitespaceTokenizer"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.regexp.WhitespaceTokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#nltk.tokenize.regexp.RegexpTokenizer" title="nltk.tokenize.regexp.RegexpTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">nltk.tokenize.regexp.RegexpTokenizer</span></code></a></p>
<p>Tokenize a string on whitespace (space, tab, newline).
In general, users should use the string <code class="docutils literal notranslate"><span class="pre">split()</span></code> method instead.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">nltk.tokenize</span> <span class="kn">import</span> <span class="n">WhitespaceTokenizer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s</span> <span class="o">=</span> <span class="s2">&quot;Good muffins cost $3.88</span><span class="se">\n</span><span class="s2">in New York.  Please buy me</span><span class="se">\n</span><span class="s2">two of them.</span><span class="se">\n\n</span><span class="s2">Thanks.&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">WhitespaceTokenizer</span><span class="p">()</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
<span class="go">[&#39;Good&#39;, &#39;muffins&#39;, &#39;cost&#39;, &#39;$3.88&#39;, &#39;in&#39;, &#39;New&#39;, &#39;York.&#39;,</span>
<span class="go">&#39;Please&#39;, &#39;buy&#39;, &#39;me&#39;, &#39;two&#39;, &#39;of&#39;, &#39;them.&#39;, &#39;Thanks.&#39;]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt id="nltk.tokenize.regexp.WordPunctTokenizer">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">nltk.tokenize.regexp.</span></code><code class="sig-name descname"><span class="pre">WordPunctTokenizer</span></code><a class="reference internal" href="../_modules/nltk/tokenize/regexp.html#WordPunctTokenizer"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.regexp.WordPunctTokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#nltk.tokenize.regexp.RegexpTokenizer" title="nltk.tokenize.regexp.RegexpTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">nltk.tokenize.regexp.RegexpTokenizer</span></code></a></p>
<p>Tokenize a text into a sequence of alphabetic and
non-alphabetic characters, using the regexp <code class="docutils literal notranslate"><span class="pre">\w+|[^\w\s]+</span></code>.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">nltk.tokenize</span> <span class="kn">import</span> <span class="n">WordPunctTokenizer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s</span> <span class="o">=</span> <span class="s2">&quot;Good muffins cost $3.88</span><span class="se">\n</span><span class="s2">in New York.  Please buy me</span><span class="se">\n</span><span class="s2">two of them.</span><span class="se">\n\n</span><span class="s2">Thanks.&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">WordPunctTokenizer</span><span class="p">()</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
<span class="go">[&#39;Good&#39;, &#39;muffins&#39;, &#39;cost&#39;, &#39;$&#39;, &#39;3&#39;, &#39;.&#39;, &#39;88&#39;, &#39;in&#39;, &#39;New&#39;, &#39;York&#39;,</span>
<span class="go">&#39;.&#39;, &#39;Please&#39;, &#39;buy&#39;, &#39;me&#39;, &#39;two&#39;, &#39;of&#39;, &#39;them&#39;, &#39;.&#39;, &#39;Thanks&#39;, &#39;.&#39;]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt id="nltk.tokenize.regexp.blankline_tokenize">
<code class="sig-prename descclassname"><span class="pre">nltk.tokenize.regexp.</span></code><code class="sig-name descname"><span class="pre">blankline_tokenize</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">text</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#nltk.tokenize.regexp.blankline_tokenize" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a tokenized copy of <em>s</em>.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>list of str</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="nltk.tokenize.regexp.regexp_tokenize">
<code class="sig-prename descclassname"><span class="pre">nltk.tokenize.regexp.</span></code><code class="sig-name descname"><span class="pre">regexp_tokenize</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">text</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pattern</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gaps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">discard_empty</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">flags</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">re.UNICODE</span> <span class="pre">|</span> <span class="pre">re.MULTILINE</span> <span class="pre">|</span> <span class="pre">re.DOTALL</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tokenize/regexp.html#regexp_tokenize"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.regexp.regexp_tokenize" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a tokenized copy of <em>text</em>.  See <a class="reference internal" href="#nltk.tokenize.regexp.RegexpTokenizer" title="nltk.tokenize.regexp.RegexpTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">RegexpTokenizer</span></code></a>
for descriptions of the arguments.</p>
</dd></dl>

<dl class="py function">
<dt id="nltk.tokenize.regexp.wordpunct_tokenize">
<code class="sig-prename descclassname"><span class="pre">nltk.tokenize.regexp.</span></code><code class="sig-name descname"><span class="pre">wordpunct_tokenize</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">text</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#nltk.tokenize.regexp.wordpunct_tokenize" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a tokenized copy of <em>s</em>.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>list of str</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="module-nltk.tokenize.repp">
<span id="nltk-tokenize-repp-module"></span><h2>nltk.tokenize.repp module<a class="headerlink" href="#module-nltk.tokenize.repp" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="nltk.tokenize.repp.ReppTokenizer">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">nltk.tokenize.repp.</span></code><code class="sig-name descname"><span class="pre">ReppTokenizer</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">repp_dir</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">encoding</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'utf8'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tokenize/repp.html#ReppTokenizer"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.repp.ReppTokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#nltk.tokenize.api.TokenizerI" title="nltk.tokenize.api.TokenizerI"><code class="xref py py-class docutils literal notranslate"><span class="pre">nltk.tokenize.api.TokenizerI</span></code></a></p>
<p>A class for word tokenization using the REPP parser described in
Rebecca Dridan and Stephan Oepen (2012) Tokenization: Returning to a
Long Solved Problem - A Survey, Contrastive  Experiment, Recommendations,
and Toolkit. In ACL. <a class="reference external" href="http://anthology.aclweb.org/P/P12/P12-2.pdf#page=406">http://anthology.aclweb.org/P/P12/P12-2.pdf#page=406</a></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">sents</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Tokenization is widely regarded as a solved problem due to the high accuracy that rulebased tokenizers achieve.&#39;</span> <span class="p">,</span>
<span class="gp">... </span><span class="s1">&#39;But rule-based tokenizers are hard to maintain and their rules language specific.&#39;</span> <span class="p">,</span>
<span class="gp">... </span><span class="s1">&#39;We evaluated our method on three languages and obtained error rates of 0.27% (English), 0.35% (Dutch) and 0.76% (Italian) for our best models.&#39;</span>
<span class="gp">... </span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">ReppTokenizer</span><span class="p">(</span><span class="s1">&#39;/home/alvas/repp/&#39;</span><span class="p">)</span> 
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">sent</span> <span class="ow">in</span> <span class="n">sents</span><span class="p">:</span>                             
<span class="gp">... </span>    <span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">sent</span><span class="p">)</span>                   
<span class="gp">...</span>
<span class="go">(u&#39;Tokenization&#39;, u&#39;is&#39;, u&#39;widely&#39;, u&#39;regarded&#39;, u&#39;as&#39;, u&#39;a&#39;, u&#39;solved&#39;, u&#39;problem&#39;, u&#39;due&#39;, u&#39;to&#39;, u&#39;the&#39;, u&#39;high&#39;, u&#39;accuracy&#39;, u&#39;that&#39;, u&#39;rulebased&#39;, u&#39;tokenizers&#39;, u&#39;achieve&#39;, u&#39;.&#39;)</span>
<span class="go">(u&#39;But&#39;, u&#39;rule-based&#39;, u&#39;tokenizers&#39;, u&#39;are&#39;, u&#39;hard&#39;, u&#39;to&#39;, u&#39;maintain&#39;, u&#39;and&#39;, u&#39;their&#39;, u&#39;rules&#39;, u&#39;language&#39;, u&#39;specific&#39;, u&#39;.&#39;)</span>
<span class="go">(u&#39;We&#39;, u&#39;evaluated&#39;, u&#39;our&#39;, u&#39;method&#39;, u&#39;on&#39;, u&#39;three&#39;, u&#39;languages&#39;, u&#39;and&#39;, u&#39;obtained&#39;, u&#39;error&#39;, u&#39;rates&#39;, u&#39;of&#39;, u&#39;0.27&#39;, u&#39;%&#39;, u&#39;(&#39;, u&#39;English&#39;, u&#39;)&#39;, u&#39;,&#39;, u&#39;0.35&#39;, u&#39;%&#39;, u&#39;(&#39;, u&#39;Dutch&#39;, u&#39;)&#39;, u&#39;and&#39;, u&#39;0.76&#39;, u&#39;%&#39;, u&#39;(&#39;, u&#39;Italian&#39;, u&#39;)&#39;, u&#39;for&#39;, u&#39;our&#39;, u&#39;best&#39;, u&#39;models&#39;, u&#39;.&#39;)</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">sent</span> <span class="ow">in</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize_sents</span><span class="p">(</span><span class="n">sents</span><span class="p">):</span> 
<span class="gp">... </span>    <span class="nb">print</span><span class="p">(</span><span class="n">sent</span><span class="p">)</span>                              
<span class="gp">...</span>
<span class="go">(u&#39;Tokenization&#39;, u&#39;is&#39;, u&#39;widely&#39;, u&#39;regarded&#39;, u&#39;as&#39;, u&#39;a&#39;, u&#39;solved&#39;, u&#39;problem&#39;, u&#39;due&#39;, u&#39;to&#39;, u&#39;the&#39;, u&#39;high&#39;, u&#39;accuracy&#39;, u&#39;that&#39;, u&#39;rulebased&#39;, u&#39;tokenizers&#39;, u&#39;achieve&#39;, u&#39;.&#39;)</span>
<span class="go">(u&#39;But&#39;, u&#39;rule-based&#39;, u&#39;tokenizers&#39;, u&#39;are&#39;, u&#39;hard&#39;, u&#39;to&#39;, u&#39;maintain&#39;, u&#39;and&#39;, u&#39;their&#39;, u&#39;rules&#39;, u&#39;language&#39;, u&#39;specific&#39;, u&#39;.&#39;)</span>
<span class="go">(u&#39;We&#39;, u&#39;evaluated&#39;, u&#39;our&#39;, u&#39;method&#39;, u&#39;on&#39;, u&#39;three&#39;, u&#39;languages&#39;, u&#39;and&#39;, u&#39;obtained&#39;, u&#39;error&#39;, u&#39;rates&#39;, u&#39;of&#39;, u&#39;0.27&#39;, u&#39;%&#39;, u&#39;(&#39;, u&#39;English&#39;, u&#39;)&#39;, u&#39;,&#39;, u&#39;0.35&#39;, u&#39;%&#39;, u&#39;(&#39;, u&#39;Dutch&#39;, u&#39;)&#39;, u&#39;and&#39;, u&#39;0.76&#39;, u&#39;%&#39;, u&#39;(&#39;, u&#39;Italian&#39;, u&#39;)&#39;, u&#39;for&#39;, u&#39;our&#39;, u&#39;best&#39;, u&#39;models&#39;, u&#39;.&#39;)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">sent</span> <span class="ow">in</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize_sents</span><span class="p">(</span><span class="n">sents</span><span class="p">,</span> <span class="n">keep_token_positions</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span> 
<span class="gp">... </span>    <span class="nb">print</span><span class="p">(</span><span class="n">sent</span><span class="p">)</span>                                                         
<span class="gp">...</span>
<span class="go">[(u&#39;Tokenization&#39;, 0, 12), (u&#39;is&#39;, 13, 15), (u&#39;widely&#39;, 16, 22), (u&#39;regarded&#39;, 23, 31), (u&#39;as&#39;, 32, 34), (u&#39;a&#39;, 35, 36), (u&#39;solved&#39;, 37, 43), (u&#39;problem&#39;, 44, 51), (u&#39;due&#39;, 52, 55), (u&#39;to&#39;, 56, 58), (u&#39;the&#39;, 59, 62), (u&#39;high&#39;, 63, 67), (u&#39;accuracy&#39;, 68, 76), (u&#39;that&#39;, 77, 81), (u&#39;rulebased&#39;, 82, 91), (u&#39;tokenizers&#39;, 92, 102), (u&#39;achieve&#39;, 103, 110), (u&#39;.&#39;, 110, 111)]</span>
<span class="go">[(u&#39;But&#39;, 0, 3), (u&#39;rule-based&#39;, 4, 14), (u&#39;tokenizers&#39;, 15, 25), (u&#39;are&#39;, 26, 29), (u&#39;hard&#39;, 30, 34), (u&#39;to&#39;, 35, 37), (u&#39;maintain&#39;, 38, 46), (u&#39;and&#39;, 47, 50), (u&#39;their&#39;, 51, 56), (u&#39;rules&#39;, 57, 62), (u&#39;language&#39;, 63, 71), (u&#39;specific&#39;, 72, 80), (u&#39;.&#39;, 80, 81)]</span>
<span class="go">[(u&#39;We&#39;, 0, 2), (u&#39;evaluated&#39;, 3, 12), (u&#39;our&#39;, 13, 16), (u&#39;method&#39;, 17, 23), (u&#39;on&#39;, 24, 26), (u&#39;three&#39;, 27, 32), (u&#39;languages&#39;, 33, 42), (u&#39;and&#39;, 43, 46), (u&#39;obtained&#39;, 47, 55), (u&#39;error&#39;, 56, 61), (u&#39;rates&#39;, 62, 67), (u&#39;of&#39;, 68, 70), (u&#39;0.27&#39;, 71, 75), (u&#39;%&#39;, 75, 76), (u&#39;(&#39;, 77, 78), (u&#39;English&#39;, 78, 85), (u&#39;)&#39;, 85, 86), (u&#39;,&#39;, 86, 87), (u&#39;0.35&#39;, 88, 92), (u&#39;%&#39;, 92, 93), (u&#39;(&#39;, 94, 95), (u&#39;Dutch&#39;, 95, 100), (u&#39;)&#39;, 100, 101), (u&#39;and&#39;, 102, 105), (u&#39;0.76&#39;, 106, 110), (u&#39;%&#39;, 110, 111), (u&#39;(&#39;, 112, 113), (u&#39;Italian&#39;, 113, 120), (u&#39;)&#39;, 120, 121), (u&#39;for&#39;, 122, 125), (u&#39;our&#39;, 126, 129), (u&#39;best&#39;, 130, 134), (u&#39;models&#39;, 135, 141), (u&#39;.&#39;, 141, 142)]</span>
</pre></div>
</div>
<dl class="py method">
<dt id="nltk.tokenize.repp.ReppTokenizer.find_repptokenizer">
<code class="sig-name descname"><span class="pre">find_repptokenizer</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">repp_dirname</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tokenize/repp.html#ReppTokenizer.find_repptokenizer"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.repp.ReppTokenizer.find_repptokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>A module to find REPP tokenizer binary and its <em>repp.set</em> config file.</p>
</dd></dl>

<dl class="py method">
<dt id="nltk.tokenize.repp.ReppTokenizer.generate_repp_command">
<code class="sig-name descname"><span class="pre">generate_repp_command</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputfilename</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tokenize/repp.html#ReppTokenizer.generate_repp_command"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.repp.ReppTokenizer.generate_repp_command" title="Permalink to this definition">¶</a></dt>
<dd><p>This module generates the REPP command to be used at the terminal.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>inputfilename</strong> (<em>str</em>) – path to the input file</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="nltk.tokenize.repp.ReppTokenizer.parse_repp_outputs">
<em class="property"><span class="pre">static</span> </em><code class="sig-name descname"><span class="pre">parse_repp_outputs</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">repp_output</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tokenize/repp.html#ReppTokenizer.parse_repp_outputs"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.repp.ReppTokenizer.parse_repp_outputs" title="Permalink to this definition">¶</a></dt>
<dd><p>This module parses the tri-tuple format that REPP outputs using the
“–format triple” option and returns an generator with tuple of string
tokens.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>repp_output</strong> (<em>type</em>) – </p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>an iterable of the tokenized sentences as tuples of strings</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>iter(tuple)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="nltk.tokenize.repp.ReppTokenizer.tokenize">
<code class="sig-name descname"><span class="pre">tokenize</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sentence</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tokenize/repp.html#ReppTokenizer.tokenize"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.repp.ReppTokenizer.tokenize" title="Permalink to this definition">¶</a></dt>
<dd><p>Use Repp to tokenize a single sentence.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>sentence</strong> (<em>str</em>) – A single sentence string.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A tuple of tokens.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>tuple(str)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="nltk.tokenize.repp.ReppTokenizer.tokenize_sents">
<code class="sig-name descname"><span class="pre">tokenize_sents</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sentences</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keep_token_positions</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tokenize/repp.html#ReppTokenizer.tokenize_sents"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.repp.ReppTokenizer.tokenize_sents" title="Permalink to this definition">¶</a></dt>
<dd><p>Tokenize multiple sentences using Repp.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>sentences</strong> (<em>list</em><em>(</em><em>str</em><em>)</em>) – A list of sentence strings.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A list of tuples of tokens</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>iter(tuple(str))</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-nltk.tokenize.sexpr">
<span id="nltk-tokenize-sexpr-module"></span><h2>nltk.tokenize.sexpr module<a class="headerlink" href="#module-nltk.tokenize.sexpr" title="Permalink to this headline">¶</a></h2>
<p>S-Expression Tokenizer</p>
<p><code class="docutils literal notranslate"><span class="pre">SExprTokenizer</span></code> is used to find parenthesized expressions in a
string.  In particular, it divides a string into a sequence of
substrings that are either parenthesized expressions (including any
nested parenthesized expressions), or other whitespace-separated
tokens.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">nltk.tokenize</span> <span class="kn">import</span> <span class="n">SExprTokenizer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">SExprTokenizer</span><span class="p">()</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="s1">&#39;(a b (c d)) e f (g)&#39;</span><span class="p">)</span>
<span class="go">[&#39;(a b (c d))&#39;, &#39;e&#39;, &#39;f&#39;, &#39;(g)&#39;]</span>
</pre></div>
</div>
<p>By default, <cite>SExprTokenizer</cite> will raise a <code class="docutils literal notranslate"><span class="pre">ValueError</span></code> exception if
used to tokenize an expression with non-matching parentheses:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">SExprTokenizer</span><span class="p">()</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="s1">&#39;c) d) e (f (g&#39;</span><span class="p">)</span>
<span class="gt">Traceback (most recent call last):</span>
  <span class="c">...</span>
<span class="gr">ValueError</span>: <span class="n">Un-matched close paren at char 1</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">strict</span></code> argument can be set to False to allow for
non-matching parentheses.  Any unmatched close parentheses will be
listed as their own s-expression; and the last partial sexpr with
unmatched open parentheses will be listed as its own sexpr:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">SExprTokenizer</span><span class="p">(</span><span class="n">strict</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="s1">&#39;c) d) e (f (g&#39;</span><span class="p">)</span>
<span class="go">[&#39;c&#39;, &#39;)&#39;, &#39;d&#39;, &#39;)&#39;, &#39;e&#39;, &#39;(f (g&#39;]</span>
</pre></div>
</div>
<p>The characters used for open and close parentheses may be customized
using the <code class="docutils literal notranslate"><span class="pre">parens</span></code> argument to the <cite>SExprTokenizer</cite> constructor:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">SExprTokenizer</span><span class="p">(</span><span class="n">parens</span><span class="o">=</span><span class="s1">&#39;</span><span class="si">{}</span><span class="s1">&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="s1">&#39;{a b {c d}} e f </span><span class="si">{g}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="go">[&#39;{a b {c d}}&#39;, &#39;e&#39;, &#39;f&#39;, &#39;{g}&#39;]</span>
</pre></div>
</div>
<p>The s-expression tokenizer is also available as a function:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">nltk.tokenize</span> <span class="kn">import</span> <span class="n">sexpr_tokenize</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sexpr_tokenize</span><span class="p">(</span><span class="s1">&#39;(a b (c d)) e f (g)&#39;</span><span class="p">)</span>
<span class="go">[&#39;(a b (c d))&#39;, &#39;e&#39;, &#39;f&#39;, &#39;(g)&#39;]</span>
</pre></div>
</div>
<dl class="py class">
<dt id="nltk.tokenize.sexpr.SExprTokenizer">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">nltk.tokenize.sexpr.</span></code><code class="sig-name descname"><span class="pre">SExprTokenizer</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">parens</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'()'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strict</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tokenize/sexpr.html#SExprTokenizer"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.sexpr.SExprTokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#nltk.tokenize.api.TokenizerI" title="nltk.tokenize.api.TokenizerI"><code class="xref py py-class docutils literal notranslate"><span class="pre">nltk.tokenize.api.TokenizerI</span></code></a></p>
<p>A tokenizer that divides strings into s-expressions.
An s-expresion can be either:</p>
<blockquote>
<div><ul class="simple">
<li><p>a parenthesized expression, including any nested parenthesized
expressions, or</p></li>
<li><p>a sequence of non-whitespace non-parenthesis characters.</p></li>
</ul>
</div></blockquote>
<p>For example, the string <code class="docutils literal notranslate"><span class="pre">(a</span> <span class="pre">(b</span> <span class="pre">c))</span> <span class="pre">d</span> <span class="pre">e</span> <span class="pre">(f)</span></code> consists of four
s-expressions: <code class="docutils literal notranslate"><span class="pre">(a</span> <span class="pre">(b</span> <span class="pre">c))</span></code>, <code class="docutils literal notranslate"><span class="pre">d</span></code>, <code class="docutils literal notranslate"><span class="pre">e</span></code>, and <code class="docutils literal notranslate"><span class="pre">(f)</span></code>.</p>
<p>By default, the characters <code class="docutils literal notranslate"><span class="pre">(</span></code> and <code class="docutils literal notranslate"><span class="pre">)</span></code> are treated as open and
close parentheses, but alternative strings may be specified.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>parens</strong> (<em>str</em><em> or </em><em>list</em>) – A two-element sequence specifying the open and close parentheses
that should be used to find sexprs.  This will typically be either a
two-character string, or a list of two strings.</p></li>
<li><p><strong>strict</strong> – If true, then raise an exception when tokenizing an ill-formed sexpr.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="nltk.tokenize.sexpr.SExprTokenizer.tokenize">
<code class="sig-name descname"><span class="pre">tokenize</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">text</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tokenize/sexpr.html#SExprTokenizer.tokenize"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.sexpr.SExprTokenizer.tokenize" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a list of s-expressions extracted from <em>text</em>.
For example:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">SExprTokenizer</span><span class="p">()</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="s1">&#39;(a b (c d)) e f (g)&#39;</span><span class="p">)</span>
<span class="go">[&#39;(a b (c d))&#39;, &#39;e&#39;, &#39;f&#39;, &#39;(g)&#39;]</span>
</pre></div>
</div>
<p>All parentheses are assumed to mark s-expressions.
(No special processing is done to exclude parentheses that occur
inside strings, or following backslash characters.)</p>
<p>If the given expression contains non-matching parentheses,
then the behavior of the tokenizer depends on the <code class="docutils literal notranslate"><span class="pre">strict</span></code>
parameter to the constructor.  If <code class="docutils literal notranslate"><span class="pre">strict</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>, then
raise a <code class="docutils literal notranslate"><span class="pre">ValueError</span></code>.  If <code class="docutils literal notranslate"><span class="pre">strict</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, then any
unmatched close parentheses will be listed as their own
s-expression; and the last partial s-expression with unmatched open
parentheses will be listed as its own s-expression:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">SExprTokenizer</span><span class="p">(</span><span class="n">strict</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="s1">&#39;c) d) e (f (g&#39;</span><span class="p">)</span>
<span class="go">[&#39;c&#39;, &#39;)&#39;, &#39;d&#39;, &#39;)&#39;, &#39;e&#39;, &#39;(f (g&#39;]</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>text</strong> (<em>str</em><em> or </em><em>iter</em><em>(</em><em>str</em><em>)</em>) – the string to be tokenized</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>iter(str)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt id="nltk.tokenize.sexpr.sexpr_tokenize">
<code class="sig-prename descclassname"><span class="pre">nltk.tokenize.sexpr.</span></code><code class="sig-name descname"><span class="pre">sexpr_tokenize</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">text</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#nltk.tokenize.sexpr.sexpr_tokenize" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a list of s-expressions extracted from <em>text</em>.
For example:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">SExprTokenizer</span><span class="p">()</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="s1">&#39;(a b (c d)) e f (g)&#39;</span><span class="p">)</span>
<span class="go">[&#39;(a b (c d))&#39;, &#39;e&#39;, &#39;f&#39;, &#39;(g)&#39;]</span>
</pre></div>
</div>
<p>All parentheses are assumed to mark s-expressions.
(No special processing is done to exclude parentheses that occur
inside strings, or following backslash characters.)</p>
<p>If the given expression contains non-matching parentheses,
then the behavior of the tokenizer depends on the <code class="docutils literal notranslate"><span class="pre">strict</span></code>
parameter to the constructor.  If <code class="docutils literal notranslate"><span class="pre">strict</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>, then
raise a <code class="docutils literal notranslate"><span class="pre">ValueError</span></code>.  If <code class="docutils literal notranslate"><span class="pre">strict</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, then any
unmatched close parentheses will be listed as their own
s-expression; and the last partial s-expression with unmatched open
parentheses will be listed as its own s-expression:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">SExprTokenizer</span><span class="p">(</span><span class="n">strict</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="s1">&#39;c) d) e (f (g&#39;</span><span class="p">)</span>
<span class="go">[&#39;c&#39;, &#39;)&#39;, &#39;d&#39;, &#39;)&#39;, &#39;e&#39;, &#39;(f (g&#39;]</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>text</strong> (<em>str</em><em> or </em><em>iter</em><em>(</em><em>str</em><em>)</em>) – the string to be tokenized</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>iter(str)</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="module-nltk.tokenize.simple">
<span id="nltk-tokenize-simple-module"></span><h2>nltk.tokenize.simple module<a class="headerlink" href="#module-nltk.tokenize.simple" title="Permalink to this headline">¶</a></h2>
<p>Simple Tokenizers</p>
<p>These tokenizers divide strings into substrings using the string
<code class="docutils literal notranslate"><span class="pre">split()</span></code> method.
When tokenizing using a particular delimiter string, use
the string <code class="docutils literal notranslate"><span class="pre">split()</span></code> method directly, as this is more efficient.</p>
<p>The simple tokenizers are <em>not</em> available as separate functions;
instead, you should just use the string <code class="docutils literal notranslate"><span class="pre">split()</span></code> method directly:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">s</span> <span class="o">=</span> <span class="s2">&quot;Good muffins cost $3.88</span><span class="se">\n</span><span class="s2">in New York.  Please buy me</span><span class="se">\n</span><span class="s2">two of them.</span><span class="se">\n\n</span><span class="s2">Thanks.&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
<span class="go">[&#39;Good&#39;, &#39;muffins&#39;, &#39;cost&#39;, &#39;$3.88&#39;, &#39;in&#39;, &#39;New&#39;, &#39;York.&#39;,</span>
<span class="go">&#39;Please&#39;, &#39;buy&#39;, &#39;me&#39;, &#39;two&#39;, &#39;of&#39;, &#39;them.&#39;, &#39;Thanks.&#39;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39; &#39;</span><span class="p">)</span>
<span class="go">[&#39;Good&#39;, &#39;muffins&#39;, &#39;cost&#39;, &#39;$3.88\nin&#39;, &#39;New&#39;, &#39;York.&#39;, &#39;&#39;,</span>
<span class="go">&#39;Please&#39;, &#39;buy&#39;, &#39;me\ntwo&#39;, &#39;of&#39;, &#39;them.\n\nThanks.&#39;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="go">[&#39;Good muffins cost $3.88&#39;, &#39;in New York.  Please buy me&#39;,</span>
<span class="go">&#39;two of them.&#39;, &#39;&#39;, &#39;Thanks.&#39;]</span>
</pre></div>
</div>
<p>The simple tokenizers are mainly useful because they follow the
standard <code class="docutils literal notranslate"><span class="pre">TokenizerI</span></code> interface, and so can be used with any code
that expects a tokenizer.  For example, these tokenizers can be used
to specify the tokenization conventions when building a <cite>CorpusReader</cite>.</p>
<dl class="py class">
<dt id="nltk.tokenize.simple.CharTokenizer">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">nltk.tokenize.simple.</span></code><code class="sig-name descname"><span class="pre">CharTokenizer</span></code><a class="reference internal" href="../_modules/nltk/tokenize/simple.html#CharTokenizer"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.simple.CharTokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#nltk.tokenize.api.StringTokenizer" title="nltk.tokenize.api.StringTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">nltk.tokenize.api.StringTokenizer</span></code></a></p>
<p>Tokenize a string into individual characters.  If this functionality
is ever required directly, use <code class="docutils literal notranslate"><span class="pre">for</span> <span class="pre">char</span> <span class="pre">in</span> <span class="pre">string</span></code>.</p>
<dl class="py method">
<dt id="nltk.tokenize.simple.CharTokenizer.span_tokenize">
<code class="sig-name descname"><span class="pre">span_tokenize</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">s</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tokenize/simple.html#CharTokenizer.span_tokenize"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.simple.CharTokenizer.span_tokenize" title="Permalink to this definition">¶</a></dt>
<dd><p>Identify the tokens using integer offsets <code class="docutils literal notranslate"><span class="pre">(start_i,</span> <span class="pre">end_i)</span></code>,
where <code class="docutils literal notranslate"><span class="pre">s[start_i:end_i]</span></code> is the corresponding token.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>iter(tuple(int, int))</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="nltk.tokenize.simple.CharTokenizer.tokenize">
<code class="sig-name descname"><span class="pre">tokenize</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">s</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tokenize/simple.html#CharTokenizer.tokenize"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.simple.CharTokenizer.tokenize" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a tokenized copy of <em>s</em>.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>list of str</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="nltk.tokenize.simple.LineTokenizer">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">nltk.tokenize.simple.</span></code><code class="sig-name descname"><span class="pre">LineTokenizer</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">blanklines</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'discard'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tokenize/simple.html#LineTokenizer"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.simple.LineTokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#nltk.tokenize.api.TokenizerI" title="nltk.tokenize.api.TokenizerI"><code class="xref py py-class docutils literal notranslate"><span class="pre">nltk.tokenize.api.TokenizerI</span></code></a></p>
<p>Tokenize a string into its lines, optionally discarding blank lines.
This is similar to <code class="docutils literal notranslate"><span class="pre">s.split('\n')</span></code>.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">nltk.tokenize</span> <span class="kn">import</span> <span class="n">LineTokenizer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s</span> <span class="o">=</span> <span class="s2">&quot;Good muffins cost $3.88</span><span class="se">\n</span><span class="s2">in New York.  Please buy me</span><span class="se">\n</span><span class="s2">two of them.</span><span class="se">\n\n</span><span class="s2">Thanks.&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">LineTokenizer</span><span class="p">(</span><span class="n">blanklines</span><span class="o">=</span><span class="s1">&#39;keep&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
<span class="go">[&#39;Good muffins cost $3.88&#39;, &#39;in New York.  Please buy me&#39;,</span>
<span class="go">&#39;two of them.&#39;, &#39;&#39;, &#39;Thanks.&#39;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># same as [l for l in s.split(&#39;\n&#39;) if l.strip()]:</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">LineTokenizer</span><span class="p">(</span><span class="n">blanklines</span><span class="o">=</span><span class="s1">&#39;discard&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
<span class="go">[&#39;Good muffins cost $3.88&#39;, &#39;in New York.  Please buy me&#39;,</span>
<span class="go">&#39;two of them.&#39;, &#39;Thanks.&#39;]</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>blanklines</strong> – <p>Indicates how blank lines should be handled.  Valid values are:</p>
<ul class="simple">
<li><dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">discard</span></code>: strip blank lines out of the token list before returning it.</dt><dd><p>A line is considered blank if it contains only whitespace characters.</p>
</dd>
</dl>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">keep</span></code>: leave all blank lines in the token list.</p></li>
<li><dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">discard-eof</span></code>: if the string ends with a newline, then do not generate</dt><dd><p>a corresponding token <code class="docutils literal notranslate"><span class="pre">''</span></code> after that newline.</p>
</dd>
</dl>
</li>
</ul>
</p>
</dd>
</dl>
<dl class="py method">
<dt id="nltk.tokenize.simple.LineTokenizer.span_tokenize">
<code class="sig-name descname"><span class="pre">span_tokenize</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">s</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tokenize/simple.html#LineTokenizer.span_tokenize"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.simple.LineTokenizer.span_tokenize" title="Permalink to this definition">¶</a></dt>
<dd><p>Identify the tokens using integer offsets <code class="docutils literal notranslate"><span class="pre">(start_i,</span> <span class="pre">end_i)</span></code>,
where <code class="docutils literal notranslate"><span class="pre">s[start_i:end_i]</span></code> is the corresponding token.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>iter(tuple(int, int))</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="nltk.tokenize.simple.LineTokenizer.tokenize">
<code class="sig-name descname"><span class="pre">tokenize</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">s</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tokenize/simple.html#LineTokenizer.tokenize"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.simple.LineTokenizer.tokenize" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a tokenized copy of <em>s</em>.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>list of str</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="nltk.tokenize.simple.SpaceTokenizer">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">nltk.tokenize.simple.</span></code><code class="sig-name descname"><span class="pre">SpaceTokenizer</span></code><a class="reference internal" href="../_modules/nltk/tokenize/simple.html#SpaceTokenizer"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.simple.SpaceTokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#nltk.tokenize.api.StringTokenizer" title="nltk.tokenize.api.StringTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">nltk.tokenize.api.StringTokenizer</span></code></a></p>
<p>Tokenize a string using the space character as a delimiter,
which is the same as <code class="docutils literal notranslate"><span class="pre">s.split('</span> <span class="pre">')</span></code>.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">nltk.tokenize</span> <span class="kn">import</span> <span class="n">SpaceTokenizer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s</span> <span class="o">=</span> <span class="s2">&quot;Good muffins cost $3.88</span><span class="se">\n</span><span class="s2">in New York.  Please buy me</span><span class="se">\n</span><span class="s2">two of them.</span><span class="se">\n\n</span><span class="s2">Thanks.&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">SpaceTokenizer</span><span class="p">()</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
<span class="go">[&#39;Good&#39;, &#39;muffins&#39;, &#39;cost&#39;, &#39;$3.88\nin&#39;, &#39;New&#39;, &#39;York.&#39;, &#39;&#39;,</span>
<span class="go">&#39;Please&#39;, &#39;buy&#39;, &#39;me\ntwo&#39;, &#39;of&#39;, &#39;them.\n\nThanks.&#39;]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt id="nltk.tokenize.simple.TabTokenizer">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">nltk.tokenize.simple.</span></code><code class="sig-name descname"><span class="pre">TabTokenizer</span></code><a class="reference internal" href="../_modules/nltk/tokenize/simple.html#TabTokenizer"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.simple.TabTokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#nltk.tokenize.api.StringTokenizer" title="nltk.tokenize.api.StringTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">nltk.tokenize.api.StringTokenizer</span></code></a></p>
<p>Tokenize a string use the tab character as a delimiter,
the same as <code class="docutils literal notranslate"><span class="pre">s.split('\t')</span></code>.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">nltk.tokenize</span> <span class="kn">import</span> <span class="n">TabTokenizer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">TabTokenizer</span><span class="p">()</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="s1">&#39;a</span><span class="se">\t</span><span class="s1">b c</span><span class="se">\n\t</span><span class="s1"> d&#39;</span><span class="p">)</span>
<span class="go">[&#39;a&#39;, &#39;b c\n&#39;, &#39; d&#39;]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt id="nltk.tokenize.simple.line_tokenize">
<code class="sig-prename descclassname"><span class="pre">nltk.tokenize.simple.</span></code><code class="sig-name descname"><span class="pre">line_tokenize</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">text</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">blanklines</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'discard'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tokenize/simple.html#line_tokenize"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.simple.line_tokenize" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="module-nltk.tokenize.sonority_sequencing">
<span id="nltk-tokenize-sonority-sequencing-module"></span><h2>nltk.tokenize.sonority_sequencing module<a class="headerlink" href="#module-nltk.tokenize.sonority_sequencing" title="Permalink to this headline">¶</a></h2>
<p>The Sonority Sequencing Principle (SSP) is a language agnostic algorithm proposed
by Otto Jesperson in 1904. The sonorous quality of a phoneme is judged by the
openness of the lips. Syllable breaks occur before troughs in sonority. For more
on the SSP see Selkirk (1984).</p>
<p>The default implementation uses the English alphabet, but the <cite>sonority_hiearchy</cite>
can be modified to IPA or any other alphabet for the use-case. The SSP is a
universal syllabification algorithm, but that does not mean it performs equally
across languages. Bartlett et al. (2009) is a good benchmark for English accuracy
if utilizing IPA (pg. 311).</p>
<p>Importantly, if a custom hiearchy is supplied and vowels span across more than
one level, they should be given separately to the <cite>vowels</cite> class attribute.</p>
<p>References:
- Otto Jespersen. 1904. Lehrbuch der Phonetik.</p>
<blockquote>
<div><p>Leipzig, Teubner. Chapter 13, Silbe, pp. 185-203.</p>
</div></blockquote>
<ul class="simple">
<li><p>Elisabeth Selkirk. 1984. On the major class features and syllable theory.
In Aronoff &amp; Oehrle (eds.) Language Sound Structure: Studies in Phonology.
Cambridge, MIT Press. pp. 107-136.</p></li>
<li><p>Susan Bartlett, et al. 2009. On the Syllabification of Phonemes.
In HLT-NAACL. pp. 308-316.</p></li>
</ul>
<dl class="py class">
<dt id="nltk.tokenize.sonority_sequencing.SyllableTokenizer">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">nltk.tokenize.sonority_sequencing.</span></code><code class="sig-name descname"><span class="pre">SyllableTokenizer</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">lang</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'en'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sonority_hierarchy</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tokenize/sonority_sequencing.html#SyllableTokenizer"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.sonority_sequencing.SyllableTokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#nltk.tokenize.api.TokenizerI" title="nltk.tokenize.api.TokenizerI"><code class="xref py py-class docutils literal notranslate"><span class="pre">nltk.tokenize.api.TokenizerI</span></code></a></p>
<p>Syllabifies words based on the Sonority Sequencing Principle (SSP).</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">nltk.tokenize</span> <span class="kn">import</span> <span class="n">SyllableTokenizer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">nltk</span> <span class="kn">import</span> <span class="n">word_tokenize</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">SSP</span> <span class="o">=</span> <span class="n">SyllableTokenizer</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">SSP</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="s1">&#39;justification&#39;</span><span class="p">)</span>
<span class="go">[&#39;jus&#39;, &#39;ti&#39;, &#39;fi&#39;, &#39;ca&#39;, &#39;tion&#39;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">text</span> <span class="o">=</span> <span class="s2">&quot;This is a foobar-like sentence.&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="n">SSP</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">token</span><span class="p">)</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">word_tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)]</span>
<span class="go">[[&#39;This&#39;], [&#39;is&#39;], [&#39;a&#39;], [&#39;foo&#39;, &#39;bar&#39;, &#39;-&#39;, &#39;li&#39;, &#39;ke&#39;], [&#39;sen&#39;, &#39;ten&#39;, &#39;ce&#39;], [&#39;.&#39;]]</span>
</pre></div>
</div>
<dl class="py method">
<dt id="nltk.tokenize.sonority_sequencing.SyllableTokenizer.assign_values">
<code class="sig-name descname"><span class="pre">assign_values</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">token</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tokenize/sonority_sequencing.html#SyllableTokenizer.assign_values"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.sonority_sequencing.SyllableTokenizer.assign_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Assigns each phoneme its value from the sonority hierarchy.
Note: Sentence/text has to be tokenized first.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>token</strong> (<em>str</em>) – Single word or token</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>List of tuples, first element is character/phoneme and
second is the soronity value.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>list(tuple(str, int))</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="nltk.tokenize.sonority_sequencing.SyllableTokenizer.tokenize">
<code class="sig-name descname"><span class="pre">tokenize</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">token</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tokenize/sonority_sequencing.html#SyllableTokenizer.tokenize"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.sonority_sequencing.SyllableTokenizer.tokenize" title="Permalink to this definition">¶</a></dt>
<dd><p>Apply the SSP to return a list of syllables.
Note: Sentence/text has to be tokenized first.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>token</strong> (<em>str</em>) – Single word or token</p>
</dd>
<dt class="field-even">Return syllable_list</dt>
<dd class="field-even"><p>Single word or token broken up into syllables.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>list(str)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="nltk.tokenize.sonority_sequencing.SyllableTokenizer.validate_syllables">
<code class="sig-name descname"><span class="pre">validate_syllables</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">syllable_list</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tokenize/sonority_sequencing.html#SyllableTokenizer.validate_syllables"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.sonority_sequencing.SyllableTokenizer.validate_syllables" title="Permalink to this definition">¶</a></dt>
<dd><p>Ensures each syllable has at least one vowel.
If the following syllable doesn’t have vowel, add it to the current one.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>syllable_list</strong> (<em>list</em><em>(</em><em>str</em><em>)</em>) – Single word or token broken up into syllables.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Single word or token broken up into syllables
(with added syllables if necessary)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>list(str)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-nltk.tokenize.stanford">
<span id="nltk-tokenize-stanford-module"></span><h2>nltk.tokenize.stanford module<a class="headerlink" href="#module-nltk.tokenize.stanford" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="nltk.tokenize.stanford.StanfordTokenizer">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">nltk.tokenize.stanford.</span></code><code class="sig-name descname"><span class="pre">StanfordTokenizer</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">path_to_jar</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">encoding</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'utf8'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">options</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">java_options</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'-mx1000m'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tokenize/stanford.html#StanfordTokenizer"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.stanford.StanfordTokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#nltk.tokenize.api.TokenizerI" title="nltk.tokenize.api.TokenizerI"><code class="xref py py-class docutils literal notranslate"><span class="pre">nltk.tokenize.api.TokenizerI</span></code></a></p>
<p>Interface to the Stanford Tokenizer</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">nltk.tokenize.stanford</span> <span class="kn">import</span> <span class="n">StanfordTokenizer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s</span> <span class="o">=</span> <span class="s2">&quot;Good muffins cost $3.88</span><span class="se">\n</span><span class="s2">in New York.  Please buy me</span><span class="se">\n</span><span class="s2">two of them.</span><span class="se">\n</span><span class="s2">Thanks.&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">StanfordTokenizer</span><span class="p">()</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
<span class="go">[&#39;Good&#39;, &#39;muffins&#39;, &#39;cost&#39;, &#39;$&#39;, &#39;3.88&#39;, &#39;in&#39;, &#39;New&#39;, &#39;York&#39;, &#39;.&#39;, &#39;Please&#39;, &#39;buy&#39;, &#39;me&#39;, &#39;two&#39;, &#39;of&#39;, &#39;them&#39;, &#39;.&#39;, &#39;Thanks&#39;, &#39;.&#39;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s</span> <span class="o">=</span> <span class="s2">&quot;The colour of the wall is blue.&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">StanfordTokenizer</span><span class="p">(</span><span class="n">options</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;americanize&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">})</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
<span class="go">[&#39;The&#39;, &#39;color&#39;, &#39;of&#39;, &#39;the&#39;, &#39;wall&#39;, &#39;is&#39;, &#39;blue&#39;, &#39;.&#39;]</span>
</pre></div>
</div>
<dl class="py method">
<dt id="nltk.tokenize.stanford.StanfordTokenizer.tokenize">
<code class="sig-name descname"><span class="pre">tokenize</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">s</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tokenize/stanford.html#StanfordTokenizer.tokenize"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.stanford.StanfordTokenizer.tokenize" title="Permalink to this definition">¶</a></dt>
<dd><p>Use stanford tokenizer’s PTBTokenizer to tokenize multiple sentences.</p>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt id="nltk.tokenize.stanford.setup_module">
<code class="sig-prename descclassname"><span class="pre">nltk.tokenize.stanford.</span></code><code class="sig-name descname"><span class="pre">setup_module</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">module</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tokenize/stanford.html#setup_module"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.stanford.setup_module" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="module-nltk.tokenize.stanford_segmenter">
<span id="nltk-tokenize-stanford-segmenter-module"></span><h2>nltk.tokenize.stanford_segmenter module<a class="headerlink" href="#module-nltk.tokenize.stanford_segmenter" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="nltk.tokenize.stanford_segmenter.StanfordSegmenter">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">nltk.tokenize.stanford_segmenter.</span></code><code class="sig-name descname"><span class="pre">StanfordSegmenter</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">path_to_jar</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">path_to_slf4j</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">java_class</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">path_to_model</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">path_to_dict</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">path_to_sihan_corpora_dict</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sihan_post_processing</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'false'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keep_whitespaces</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'false'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">encoding</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'UTF-8'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">options</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">java_options</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'-mx2g'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tokenize/stanford_segmenter.html#StanfordSegmenter"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.stanford_segmenter.StanfordSegmenter" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#nltk.tokenize.api.TokenizerI" title="nltk.tokenize.api.TokenizerI"><code class="xref py py-class docutils literal notranslate"><span class="pre">nltk.tokenize.api.TokenizerI</span></code></a></p>
<p>Interface to the Stanford Segmenter</p>
<p>If stanford-segmenter version is older than 2016-10-31, then path_to_slf4j
should be provieded, for example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">seg</span> <span class="o">=</span> <span class="n">StanfordSegmenter</span><span class="p">(</span><span class="n">path_to_slf4j</span><span class="o">=</span><span class="s1">&#39;/YOUR_PATH/slf4j-api.jar&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">nltk.tokenize.stanford_segmenter</span> <span class="kn">import</span> <span class="n">StanfordSegmenter</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">seg</span> <span class="o">=</span> <span class="n">StanfordSegmenter</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">seg</span><span class="o">.</span><span class="n">default_config</span><span class="p">(</span><span class="s1">&#39;zh&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sent</span> <span class="o">=</span> <span class="sa">u</span><span class="s1">&#39;这是斯坦福中文分词器测试&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">seg</span><span class="o">.</span><span class="n">segment</span><span class="p">(</span><span class="n">sent</span><span class="p">))</span>
<span class="go">这 是 斯坦福 中文 分词器 测试</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">seg</span><span class="o">.</span><span class="n">default_config</span><span class="p">(</span><span class="s1">&#39;ar&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sent</span> <span class="o">=</span> <span class="sa">u</span><span class="s1">&#39;هذا هو تصنيف ستانفورد العربي للكلمات&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">seg</span><span class="o">.</span><span class="n">segment</span><span class="p">(</span><span class="n">sent</span><span class="o">.</span><span class="n">split</span><span class="p">()))</span>
<span class="go">هذا هو تصنيف ستانفورد العربي ل الكلمات</span>
</pre></div>
</div>
<dl class="py method">
<dt id="nltk.tokenize.stanford_segmenter.StanfordSegmenter.default_config">
<code class="sig-name descname"><span class="pre">default_config</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">lang</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tokenize/stanford_segmenter.html#StanfordSegmenter.default_config"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.stanford_segmenter.StanfordSegmenter.default_config" title="Permalink to this definition">¶</a></dt>
<dd><p>Attempt to intialize Stanford Word Segmenter for the specified language
using the STANFORD_SEGMENTER and STANFORD_MODELS environment variables</p>
</dd></dl>

<dl class="py method">
<dt id="nltk.tokenize.stanford_segmenter.StanfordSegmenter.segment">
<code class="sig-name descname"><span class="pre">segment</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tokens</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tokenize/stanford_segmenter.html#StanfordSegmenter.segment"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.stanford_segmenter.StanfordSegmenter.segment" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="nltk.tokenize.stanford_segmenter.StanfordSegmenter.segment_file">
<code class="sig-name descname"><span class="pre">segment_file</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_file_path</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tokenize/stanford_segmenter.html#StanfordSegmenter.segment_file"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.stanford_segmenter.StanfordSegmenter.segment_file" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="nltk.tokenize.stanford_segmenter.StanfordSegmenter.segment_sents">
<code class="sig-name descname"><span class="pre">segment_sents</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sentences</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tokenize/stanford_segmenter.html#StanfordSegmenter.segment_sents"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.stanford_segmenter.StanfordSegmenter.segment_sents" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="nltk.tokenize.stanford_segmenter.StanfordSegmenter.tokenize">
<code class="sig-name descname"><span class="pre">tokenize</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">s</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tokenize/stanford_segmenter.html#StanfordSegmenter.tokenize"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.stanford_segmenter.StanfordSegmenter.tokenize" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a tokenized copy of <em>s</em>.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>list of str</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt id="nltk.tokenize.stanford_segmenter.setup_module">
<code class="sig-prename descclassname"><span class="pre">nltk.tokenize.stanford_segmenter.</span></code><code class="sig-name descname"><span class="pre">setup_module</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">module</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tokenize/stanford_segmenter.html#setup_module"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.stanford_segmenter.setup_module" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="module-nltk.tokenize.texttiling">
<span id="nltk-tokenize-texttiling-module"></span><h2>nltk.tokenize.texttiling module<a class="headerlink" href="#module-nltk.tokenize.texttiling" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="nltk.tokenize.texttiling.TextTilingTokenizer">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">nltk.tokenize.texttiling.</span></code><code class="sig-name descname"><span class="pre">TextTilingTokenizer</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">w</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">20</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">k</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">similarity_method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stopwords</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">smoothing_method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">[0]</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">smoothing_width</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">smoothing_rounds</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cutoff_policy</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">demo_mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tokenize/texttiling.html#TextTilingTokenizer"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.texttiling.TextTilingTokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#nltk.tokenize.api.TokenizerI" title="nltk.tokenize.api.TokenizerI"><code class="xref py py-class docutils literal notranslate"><span class="pre">nltk.tokenize.api.TokenizerI</span></code></a></p>
<p>Tokenize a document into topical sections using the TextTiling algorithm.
This algorithm detects subtopic shifts based on the analysis of lexical
co-occurrence patterns.</p>
<p>The process starts by tokenizing the text into pseudosentences of
a fixed size w. Then, depending on the method used, similarity
scores are assigned at sentence gaps. The algorithm proceeds by
detecting the peak differences between these scores and marking
them as boundaries. The boundaries are normalized to the closest
paragraph break and the segmented text is returned.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>w</strong> (<em>int</em>) – Pseudosentence size</p></li>
<li><p><strong>k</strong> (<em>int</em>) – Size (in sentences) of the block used in the block comparison method</p></li>
<li><p><strong>similarity_method</strong> (<em>constant</em>) – The method used for determining similarity scores:
<cite>BLOCK_COMPARISON</cite> (default) or <cite>VOCABULARY_INTRODUCTION</cite>.</p></li>
<li><p><strong>stopwords</strong> (<em>list</em><em>(</em><em>str</em><em>)</em>) – A list of stopwords that are filtered out (defaults to NLTK’s stopwords corpus)</p></li>
<li><p><strong>smoothing_method</strong> (<em>constant</em>) – The method used for smoothing the score plot:
<cite>DEFAULT_SMOOTHING</cite> (default)</p></li>
<li><p><strong>smoothing_width</strong> (<em>int</em>) – The width of the window used by the smoothing method</p></li>
<li><p><strong>smoothing_rounds</strong> (<em>int</em>) – The number of smoothing passes</p></li>
<li><p><strong>cutoff_policy</strong> (<em>constant</em>) – The policy used to determine the number of boundaries:
<cite>HC</cite> (default) or <cite>LC</cite></p></li>
</ul>
</dd>
</dl>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">nltk.corpus</span> <span class="kn">import</span> <span class="n">brown</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tt</span> <span class="o">=</span> <span class="n">TextTilingTokenizer</span><span class="p">(</span><span class="n">demo_mode</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">text</span> <span class="o">=</span> <span class="n">brown</span><span class="o">.</span><span class="n">raw</span><span class="p">()[:</span><span class="mi">4000</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s</span><span class="p">,</span> <span class="n">ss</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">tt</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span>
<span class="go">[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0]</span>
</pre></div>
</div>
<dl class="py method">
<dt id="nltk.tokenize.texttiling.TextTilingTokenizer.tokenize">
<code class="sig-name descname"><span class="pre">tokenize</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">text</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tokenize/texttiling.html#TextTilingTokenizer.tokenize"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.texttiling.TextTilingTokenizer.tokenize" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a tokenized copy of <em>text</em>, where each “token” represents
a separate topic.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="nltk.tokenize.texttiling.TokenSequence">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">nltk.tokenize.texttiling.</span></code><code class="sig-name descname"><span class="pre">TokenSequence</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">index</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">wrdindex_list</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">original_length</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tokenize/texttiling.html#TokenSequence"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.texttiling.TokenSequence" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>A token list with its original length and its index</p>
</dd></dl>

<dl class="py class">
<dt id="nltk.tokenize.texttiling.TokenTableField">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">nltk.tokenize.texttiling.</span></code><code class="sig-name descname"><span class="pre">TokenTableField</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">first_pos</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ts_occurences</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">total_count</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">par_count</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">last_par</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">last_tok_seq</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tokenize/texttiling.html#TokenTableField"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.texttiling.TokenTableField" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>A field in the token table holding parameters for each token,
used later in the process</p>
</dd></dl>

<dl class="py function">
<dt id="nltk.tokenize.texttiling.demo">
<code class="sig-prename descclassname"><span class="pre">nltk.tokenize.texttiling.</span></code><code class="sig-name descname"><span class="pre">demo</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">text</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tokenize/texttiling.html#demo"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.texttiling.demo" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="nltk.tokenize.texttiling.smooth">
<code class="sig-prename descclassname"><span class="pre">nltk.tokenize.texttiling.</span></code><code class="sig-name descname"><span class="pre">smooth</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">window_len</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">11</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">window</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'flat'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tokenize/texttiling.html#smooth"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.texttiling.smooth" title="Permalink to this definition">¶</a></dt>
<dd><p>smooth the data using a window with requested size.</p>
<p>This method is based on the convolution of a scaled window with the signal.
The signal is prepared by introducing reflected copies of the signal
(with the window size) in both ends so that transient parts are minimized
in the beginning and end part of the output signal.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> – the input signal</p></li>
<li><p><strong>window_len</strong> – the dimension of the smoothing window; should be an odd integer</p></li>
<li><p><strong>window</strong> – the type of window from ‘flat’, ‘hanning’, ‘hamming’, ‘bartlett’, ‘blackman’
flat window will produce a moving average smoothing.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the smoothed signal</p>
</dd>
</dl>
<p>example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">t</span><span class="o">=</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">x</span><span class="o">=</span><span class="n">sin</span><span class="p">(</span><span class="n">t</span><span class="p">)</span><span class="o">+</span><span class="n">randn</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">t</span><span class="p">))</span><span class="o">*</span><span class="mf">0.1</span>
<span class="n">y</span><span class="o">=</span><span class="n">smooth</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">See also</dt>
<dd class="field-odd"><p>numpy.hanning, numpy.hamming, numpy.bartlett, numpy.blackman, numpy.convolve,
scipy.signal.lfilter</p>
</dd>
</dl>
<p>TODO: the window parameter could be the window itself if an array instead of a string</p>
</dd></dl>

</div>
<div class="section" id="module-nltk.tokenize.toktok">
<span id="nltk-tokenize-toktok-module"></span><h2>nltk.tokenize.toktok module<a class="headerlink" href="#module-nltk.tokenize.toktok" title="Permalink to this headline">¶</a></h2>
<p>The tok-tok tokenizer is a simple, general tokenizer, where the input has one
sentence per line; thus only final period is tokenized.</p>
<p>Tok-tok has been tested on, and gives reasonably good results for English,
Persian, Russian, Czech, French, German, Vietnamese, Tajik, and a few others.
The input should be in UTF-8 encoding.</p>
<p>Reference:
Jon Dehdari. 2014. A Neurophysiologically-Inspired Statistical Language
Model (Doctoral dissertation). Columbus, OH, USA: The Ohio State University.</p>
<dl class="py class">
<dt id="nltk.tokenize.toktok.ToktokTokenizer">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">nltk.tokenize.toktok.</span></code><code class="sig-name descname"><span class="pre">ToktokTokenizer</span></code><a class="reference internal" href="../_modules/nltk/tokenize/toktok.html#ToktokTokenizer"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.toktok.ToktokTokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#nltk.tokenize.api.TokenizerI" title="nltk.tokenize.api.TokenizerI"><code class="xref py py-class docutils literal notranslate"><span class="pre">nltk.tokenize.api.TokenizerI</span></code></a></p>
<p>This is a Python port of the tok-tok.pl from
<a class="reference external" href="https://github.com/jonsafari/tok-tok/blob/master/tok-tok.pl">https://github.com/jonsafari/tok-tok/blob/master/tok-tok.pl</a></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">toktok</span> <span class="o">=</span> <span class="n">ToktokTokenizer</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">text</span> <span class="o">=</span> <span class="sa">u</span><span class="s1">&#39;Is 9.5 or 525,600 my favorite number?&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">toktok</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">return_str</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
<span class="go">Is 9.5 or 525,600 my favorite number ?</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">text</span> <span class="o">=</span> <span class="sa">u</span><span class="s1">&#39;The https://github.com/jonsafari/tok-tok/blob/master/tok-tok.pl is a website with/and/or slashes and sort of weird : things&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">toktok</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">return_str</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
<span class="go">The https://github.com/jonsafari/tok-tok/blob/master/tok-tok.pl is a website with/and/or slashes and sort of weird : things</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">text</span> <span class="o">=</span> <span class="sa">u</span><span class="s1">&#39;¡This, is a sentence with weird» symbols… appearing everywhere¿&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">expected</span> <span class="o">=</span> <span class="sa">u</span><span class="s1">&#39;¡ This , is a sentence with weird » symbols … appearing everywhere ¿&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="n">toktok</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">return_str</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">==</span> <span class="n">expected</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">toktok</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span> <span class="o">==</span> <span class="p">[</span><span class="sa">u</span><span class="s1">&#39;¡&#39;</span><span class="p">,</span> <span class="sa">u</span><span class="s1">&#39;This&#39;</span><span class="p">,</span> <span class="sa">u</span><span class="s1">&#39;,&#39;</span><span class="p">,</span> <span class="sa">u</span><span class="s1">&#39;is&#39;</span><span class="p">,</span> <span class="sa">u</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="sa">u</span><span class="s1">&#39;sentence&#39;</span><span class="p">,</span> <span class="sa">u</span><span class="s1">&#39;with&#39;</span><span class="p">,</span> <span class="sa">u</span><span class="s1">&#39;weird&#39;</span><span class="p">,</span> <span class="sa">u</span><span class="s1">&#39;»&#39;</span><span class="p">,</span> <span class="sa">u</span><span class="s1">&#39;symbols&#39;</span><span class="p">,</span> <span class="sa">u</span><span class="s1">&#39;…&#39;</span><span class="p">,</span> <span class="sa">u</span><span class="s1">&#39;appearing&#39;</span><span class="p">,</span> <span class="sa">u</span><span class="s1">&#39;everywhere&#39;</span><span class="p">,</span> <span class="sa">u</span><span class="s1">&#39;¿&#39;</span><span class="p">]</span>
<span class="go">True</span>
</pre></div>
</div>
<dl class="py attribute">
<dt id="nltk.tokenize.toktok.ToktokTokenizer.AMPERCENT">
<code class="sig-name descname"><span class="pre">AMPERCENT</span></code><em class="property"> <span class="pre">=</span> <span class="pre">(re.compile('&amp;</span> <span class="pre">'),</span> <span class="pre">'&amp;amp;</span> <span class="pre">')</span></em><a class="headerlink" href="#nltk.tokenize.toktok.ToktokTokenizer.AMPERCENT" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="nltk.tokenize.toktok.ToktokTokenizer.CLOSE_PUNCT">
<code class="sig-name descname"><span class="pre">CLOSE_PUNCT</span></code><em class="property"> <span class="pre">=</span> <span class="pre">')]}༻༽᚜⁆⁾₎〉❩❫❭❯❱❳❵⟆⟧⟩⟫⟭⟯⦄⦆⦈⦊⦌⦎⦐⦒⦔⦖⦘⧙⧛⧽⸣⸥⸧⸩〉》」』】〕〗〙〛〞〟﴿︘︶︸︺︼︾﹀﹂﹄﹈﹚﹜﹞）］｝｠｣'</span></em><a class="headerlink" href="#nltk.tokenize.toktok.ToktokTokenizer.CLOSE_PUNCT" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="nltk.tokenize.toktok.ToktokTokenizer.CLOSE_PUNCT_RE">
<code class="sig-name descname"><span class="pre">CLOSE_PUNCT_RE</span></code><em class="property"> <span class="pre">=</span> <span class="pre">(re.compile('([)]}༻༽᚜⁆⁾₎〉❩❫❭❯❱❳❵⟆⟧⟩⟫⟭⟯⦄⦆⦈⦊⦌⦎⦐⦒⦔⦖⦘⧙⧛⧽⸣⸥⸧⸩〉》」』】〕〗〙〛〞〟﴿︘︶︸︺︼︾﹀﹂﹄﹈﹚﹜﹞）］｝｠｣])'),</span> <span class="pre">'\\1</span> <span class="pre">')</span></em><a class="headerlink" href="#nltk.tokenize.toktok.ToktokTokenizer.CLOSE_PUNCT_RE" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="nltk.tokenize.toktok.ToktokTokenizer.COMMA_IN_NUM">
<code class="sig-name descname"><span class="pre">COMMA_IN_NUM</span></code><em class="property"> <span class="pre">=</span> <span class="pre">(re.compile('(?&lt;!,)([,،])(?![,\\d])'),</span> <span class="pre">'</span> <span class="pre">\\1</span> <span class="pre">')</span></em><a class="headerlink" href="#nltk.tokenize.toktok.ToktokTokenizer.COMMA_IN_NUM" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="nltk.tokenize.toktok.ToktokTokenizer.CURRENCY_SYM">
<code class="sig-name descname"><span class="pre">CURRENCY_SYM</span></code><em class="property"> <span class="pre">=</span> <span class="pre">'$¢£¤¥֏؋৲৳৻૱௹฿៛₠₡₢₣₤₥₦₧₨₩₪₫€₭₮₯₰₱₲₳₴₵₶₷₸₹₺꠸﷼﹩＄￠￡￥￦'</span></em><a class="headerlink" href="#nltk.tokenize.toktok.ToktokTokenizer.CURRENCY_SYM" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="nltk.tokenize.toktok.ToktokTokenizer.CURRENCY_SYM_RE">
<code class="sig-name descname"><span class="pre">CURRENCY_SYM_RE</span></code><em class="property"> <span class="pre">=</span> <span class="pre">(re.compile('([$¢£¤¥֏؋৲৳৻૱௹฿៛₠₡₢₣₤₥₦₧₨₩₪₫€₭₮₯₰₱₲₳₴₵₶₷₸₹₺꠸﷼﹩＄￠￡￥￦])'),</span> <span class="pre">'\\1</span> <span class="pre">')</span></em><a class="headerlink" href="#nltk.tokenize.toktok.ToktokTokenizer.CURRENCY_SYM_RE" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="nltk.tokenize.toktok.ToktokTokenizer.EN_EM_DASHES">
<code class="sig-name descname"><span class="pre">EN_EM_DASHES</span></code><em class="property"> <span class="pre">=</span> <span class="pre">(re.compile('([–—])'),</span> <span class="pre">'</span> <span class="pre">\\1</span> <span class="pre">')</span></em><a class="headerlink" href="#nltk.tokenize.toktok.ToktokTokenizer.EN_EM_DASHES" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="nltk.tokenize.toktok.ToktokTokenizer.FINAL_PERIOD_1">
<code class="sig-name descname"><span class="pre">FINAL_PERIOD_1</span></code><em class="property"> <span class="pre">=</span> <span class="pre">(re.compile('(?&lt;!\\.)\\.$'),</span> <span class="pre">'</span> <span class="pre">.')</span></em><a class="headerlink" href="#nltk.tokenize.toktok.ToktokTokenizer.FINAL_PERIOD_1" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="nltk.tokenize.toktok.ToktokTokenizer.FINAL_PERIOD_2">
<code class="sig-name descname"><span class="pre">FINAL_PERIOD_2</span></code><em class="property"> <span class="pre">=</span> <span class="pre">(re.compile('(?&lt;!\\.)\\.\\s*([&quot;\'’»›”])</span> <span class="pre">*$'),</span> <span class="pre">'</span> <span class="pre">.</span> <span class="pre">\\1')</span></em><a class="headerlink" href="#nltk.tokenize.toktok.ToktokTokenizer.FINAL_PERIOD_2" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="nltk.tokenize.toktok.ToktokTokenizer.FUNKY_PUNCT_1">
<code class="sig-name descname"><span class="pre">FUNKY_PUNCT_1</span></code><em class="property"> <span class="pre">=</span> <span class="pre">(re.compile('([،;؛¿!&quot;\\])}»›”؟¡%٪°±©®।॥…])'),</span> <span class="pre">'</span> <span class="pre">\\1</span> <span class="pre">')</span></em><a class="headerlink" href="#nltk.tokenize.toktok.ToktokTokenizer.FUNKY_PUNCT_1" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="nltk.tokenize.toktok.ToktokTokenizer.FUNKY_PUNCT_2">
<code class="sig-name descname"><span class="pre">FUNKY_PUNCT_2</span></code><em class="property"> <span class="pre">=</span> <span class="pre">(re.compile('([({\\[“‘„‚«‹「『])'),</span> <span class="pre">'</span> <span class="pre">\\1</span> <span class="pre">')</span></em><a class="headerlink" href="#nltk.tokenize.toktok.ToktokTokenizer.FUNKY_PUNCT_2" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="nltk.tokenize.toktok.ToktokTokenizer.LSTRIP">
<code class="sig-name descname"><span class="pre">LSTRIP</span></code><em class="property"> <span class="pre">=</span> <span class="pre">(re.compile('^</span> <span class="pre">+'),</span> <span class="pre">'')</span></em><a class="headerlink" href="#nltk.tokenize.toktok.ToktokTokenizer.LSTRIP" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="nltk.tokenize.toktok.ToktokTokenizer.MULTI_COMMAS">
<code class="sig-name descname"><span class="pre">MULTI_COMMAS</span></code><em class="property"> <span class="pre">=</span> <span class="pre">(re.compile('(,{2,})'),</span> <span class="pre">'</span> <span class="pre">\\1</span> <span class="pre">')</span></em><a class="headerlink" href="#nltk.tokenize.toktok.ToktokTokenizer.MULTI_COMMAS" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="nltk.tokenize.toktok.ToktokTokenizer.MULTI_DASHES">
<code class="sig-name descname"><span class="pre">MULTI_DASHES</span></code><em class="property"> <span class="pre">=</span> <span class="pre">(re.compile('(-{2,})'),</span> <span class="pre">'</span> <span class="pre">\\1</span> <span class="pre">')</span></em><a class="headerlink" href="#nltk.tokenize.toktok.ToktokTokenizer.MULTI_DASHES" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="nltk.tokenize.toktok.ToktokTokenizer.MULTI_DOTS">
<code class="sig-name descname"><span class="pre">MULTI_DOTS</span></code><em class="property"> <span class="pre">=</span> <span class="pre">(re.compile('(\\.{2,})'),</span> <span class="pre">'</span> <span class="pre">\\1</span> <span class="pre">')</span></em><a class="headerlink" href="#nltk.tokenize.toktok.ToktokTokenizer.MULTI_DOTS" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="nltk.tokenize.toktok.ToktokTokenizer.NON_BREAKING">
<code class="sig-name descname"><span class="pre">NON_BREAKING</span></code><em class="property"> <span class="pre">=</span> <span class="pre">(re.compile('\xa0'),</span> <span class="pre">'</span> <span class="pre">')</span></em><a class="headerlink" href="#nltk.tokenize.toktok.ToktokTokenizer.NON_BREAKING" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="nltk.tokenize.toktok.ToktokTokenizer.ONE_SPACE">
<code class="sig-name descname"><span class="pre">ONE_SPACE</span></code><em class="property"> <span class="pre">=</span> <span class="pre">(re.compile('</span> <span class="pre">{2,}'),</span> <span class="pre">'</span> <span class="pre">')</span></em><a class="headerlink" href="#nltk.tokenize.toktok.ToktokTokenizer.ONE_SPACE" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="nltk.tokenize.toktok.ToktokTokenizer.OPEN_PUNCT">
<code class="sig-name descname"><span class="pre">OPEN_PUNCT</span></code><em class="property"> <span class="pre">=</span> <span class="pre">'([{༺༼᚛‚„⁅⁽₍〈❨❪❬❮❰❲❴⟅⟦⟨⟪⟬⟮⦃⦅⦇⦉⦋⦍⦏⦑⦓⦕⦗⧘⧚⧼⸢⸤⸦⸨〈《「『【〔〖〘〚〝﴾︗︵︷︹︻︽︿﹁﹃﹇﹙﹛﹝（［｛｟｢'</span></em><a class="headerlink" href="#nltk.tokenize.toktok.ToktokTokenizer.OPEN_PUNCT" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="nltk.tokenize.toktok.ToktokTokenizer.OPEN_PUNCT_RE">
<code class="sig-name descname"><span class="pre">OPEN_PUNCT_RE</span></code><em class="property"> <span class="pre">=</span> <span class="pre">(re.compile('([([{༺༼᚛‚„⁅⁽₍〈❨❪❬❮❰❲❴⟅⟦⟨⟪⟬⟮⦃⦅⦇⦉⦋⦍⦏⦑⦓⦕⦗⧘⧚⧼⸢⸤⸦⸨〈《「『【〔〖〘〚〝﴾︗︵︷︹︻︽︿﹁﹃﹇﹙﹛﹝（［｛｟｢])'),</span> <span class="pre">'\\1</span> <span class="pre">')</span></em><a class="headerlink" href="#nltk.tokenize.toktok.ToktokTokenizer.OPEN_PUNCT_RE" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="nltk.tokenize.toktok.ToktokTokenizer.PIPE">
<code class="sig-name descname"><span class="pre">PIPE</span></code><em class="property"> <span class="pre">=</span> <span class="pre">(re.compile('\\|'),</span> <span class="pre">'</span> <span class="pre">&amp;#124;</span> <span class="pre">')</span></em><a class="headerlink" href="#nltk.tokenize.toktok.ToktokTokenizer.PIPE" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="nltk.tokenize.toktok.ToktokTokenizer.PROB_SINGLE_QUOTES">
<code class="sig-name descname"><span class="pre">PROB_SINGLE_QUOTES</span></code><em class="property"> <span class="pre">=</span> <span class="pre">(re.compile(&quot;(['’`])&quot;),</span> <span class="pre">'</span> <span class="pre">\\1</span> <span class="pre">')</span></em><a class="headerlink" href="#nltk.tokenize.toktok.ToktokTokenizer.PROB_SINGLE_QUOTES" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="nltk.tokenize.toktok.ToktokTokenizer.RSTRIP">
<code class="sig-name descname"><span class="pre">RSTRIP</span></code><em class="property"> <span class="pre">=</span> <span class="pre">(re.compile('\\s+$'),</span> <span class="pre">'\n')</span></em><a class="headerlink" href="#nltk.tokenize.toktok.ToktokTokenizer.RSTRIP" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="nltk.tokenize.toktok.ToktokTokenizer.STUPID_QUOTES_1">
<code class="sig-name descname"><span class="pre">STUPID_QUOTES_1</span></code><em class="property"> <span class="pre">=</span> <span class="pre">(re.compile('</span> <span class="pre">`</span> <span class="pre">`</span> <span class="pre">'),</span> <span class="pre">'</span> <span class="pre">``</span> <span class="pre">')</span></em><a class="headerlink" href="#nltk.tokenize.toktok.ToktokTokenizer.STUPID_QUOTES_1" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="nltk.tokenize.toktok.ToktokTokenizer.STUPID_QUOTES_2">
<code class="sig-name descname"><span class="pre">STUPID_QUOTES_2</span></code><em class="property"> <span class="pre">=</span> <span class="pre">(re.compile(&quot;</span> <span class="pre">'</span> <span class="pre">'</span> <span class="pre">&quot;),</span> <span class="pre">&quot;</span> <span class="pre">''</span> <span class="pre">&quot;)</span></em><a class="headerlink" href="#nltk.tokenize.toktok.ToktokTokenizer.STUPID_QUOTES_2" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="nltk.tokenize.toktok.ToktokTokenizer.TAB">
<code class="sig-name descname"><span class="pre">TAB</span></code><em class="property"> <span class="pre">=</span> <span class="pre">(re.compile('\t'),</span> <span class="pre">'</span> <span class="pre">&amp;#9;</span> <span class="pre">')</span></em><a class="headerlink" href="#nltk.tokenize.toktok.ToktokTokenizer.TAB" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="nltk.tokenize.toktok.ToktokTokenizer.TOKTOK_REGEXES">
<code class="sig-name descname"><span class="pre">TOKTOK_REGEXES</span></code><em class="property"> <span class="pre">=</span> <span class="pre">[(re.compile('\xa0'),</span> <span class="pre">'</span> <span class="pre">'),</span> <span class="pre">(re.compile('([،;؛¿!&quot;\\])}»›”؟¡%٪°±©®।॥…])'),</span> <span class="pre">'</span> <span class="pre">\\1</span> <span class="pre">'),</span> <span class="pre">(re.compile(':(?!//)'),</span> <span class="pre">'</span> <span class="pre">:</span> <span class="pre">'),</span> <span class="pre">(re.compile('\\?(?!\\S)'),</span> <span class="pre">'</span> <span class="pre">?</span> <span class="pre">'),</span> <span class="pre">(re.compile('(:\\/\\/)[\\S+\\.\\S+\\/\\S+][\\/]'),</span> <span class="pre">'</span> <span class="pre">/</span> <span class="pre">'),</span> <span class="pre">(re.compile('</span> <span class="pre">/'),</span> <span class="pre">'</span> <span class="pre">/</span> <span class="pre">'),</span> <span class="pre">(re.compile('&amp;</span> <span class="pre">'),</span> <span class="pre">'&amp;amp;</span> <span class="pre">'),</span> <span class="pre">(re.compile('\t'),</span> <span class="pre">'</span> <span class="pre">&amp;#9;</span> <span class="pre">'),</span> <span class="pre">(re.compile('\\|'),</span> <span class="pre">'</span> <span class="pre">&amp;#124;</span> <span class="pre">'),</span> <span class="pre">(re.compile('([([{༺༼᚛‚„⁅⁽₍〈❨❪❬❮❰❲❴⟅⟦⟨⟪⟬⟮⦃⦅⦇⦉⦋⦍⦏⦑⦓⦕⦗⧘⧚⧼⸢⸤⸦⸨〈《「『【〔〖〘〚〝﴾︗︵︷︹︻︽︿﹁﹃﹇﹙﹛﹝（［｛｟｢])'),</span> <span class="pre">'\\1</span> <span class="pre">'),</span> <span class="pre">(re.compile('([)]}༻༽᚜⁆⁾₎〉❩❫❭❯❱❳❵⟆⟧⟩⟫⟭⟯⦄⦆⦈⦊⦌⦎⦐⦒⦔⦖⦘⧙⧛⧽⸣⸥⸧⸩〉》」』】〕〗〙〛〞〟﴿︘︶︸︺︼︾﹀﹂﹄﹈﹚﹜﹞）］｝｠｣])'),</span> <span class="pre">'\\1</span> <span class="pre">'),</span> <span class="pre">(re.compile('(,{2,})'),</span> <span class="pre">'</span> <span class="pre">\\1</span> <span class="pre">'),</span> <span class="pre">(re.compile('(?&lt;!,)([,،])(?![,\\d])'),</span> <span class="pre">'</span> <span class="pre">\\1</span> <span class="pre">'),</span> <span class="pre">(re.compile('(?&lt;!\\.)\\.\\s*([&quot;\'’»›”])</span> <span class="pre">*$'),</span> <span class="pre">'</span> <span class="pre">.</span> <span class="pre">\\1'),</span> <span class="pre">(re.compile(&quot;(['’`])&quot;),</span> <span class="pre">'</span> <span class="pre">\\1</span> <span class="pre">'),</span> <span class="pre">(re.compile('</span> <span class="pre">`</span> <span class="pre">`</span> <span class="pre">'),</span> <span class="pre">'</span> <span class="pre">``</span> <span class="pre">'),</span> <span class="pre">(re.compile(&quot;</span> <span class="pre">'</span> <span class="pre">'</span> <span class="pre">&quot;),</span> <span class="pre">&quot;</span> <span class="pre">''</span> <span class="pre">&quot;),</span> <span class="pre">(re.compile('([$¢£¤¥֏؋৲৳৻૱௹฿៛₠₡₢₣₤₥₦₧₨₩₪₫€₭₮₯₰₱₲₳₴₵₶₷₸₹₺꠸﷼﹩＄￠￡￥￦])'),</span> <span class="pre">'\\1</span> <span class="pre">'),</span> <span class="pre">(re.compile('([–—])'),</span> <span class="pre">'</span> <span class="pre">\\1</span> <span class="pre">'),</span> <span class="pre">(re.compile('(-{2,})'),</span> <span class="pre">'</span> <span class="pre">\\1</span> <span class="pre">'),</span> <span class="pre">(re.compile('(\\.{2,})'),</span> <span class="pre">'</span> <span class="pre">\\1</span> <span class="pre">'),</span> <span class="pre">(re.compile('(?&lt;!\\.)\\.$'),</span> <span class="pre">'</span> <span class="pre">.'),</span> <span class="pre">(re.compile('(?&lt;!\\.)\\.\\s*([&quot;\'’»›”])</span> <span class="pre">*$'),</span> <span class="pre">'</span> <span class="pre">.</span> <span class="pre">\\1'),</span> <span class="pre">(re.compile('</span> <span class="pre">{2,}'),</span> <span class="pre">'</span> <span class="pre">')]</span></em><a class="headerlink" href="#nltk.tokenize.toktok.ToktokTokenizer.TOKTOK_REGEXES" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="nltk.tokenize.toktok.ToktokTokenizer.URL_FOE_1">
<code class="sig-name descname"><span class="pre">URL_FOE_1</span></code><em class="property"> <span class="pre">=</span> <span class="pre">(re.compile(':(?!//)'),</span> <span class="pre">'</span> <span class="pre">:</span> <span class="pre">')</span></em><a class="headerlink" href="#nltk.tokenize.toktok.ToktokTokenizer.URL_FOE_1" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="nltk.tokenize.toktok.ToktokTokenizer.URL_FOE_2">
<code class="sig-name descname"><span class="pre">URL_FOE_2</span></code><em class="property"> <span class="pre">=</span> <span class="pre">(re.compile('\\?(?!\\S)'),</span> <span class="pre">'</span> <span class="pre">?</span> <span class="pre">')</span></em><a class="headerlink" href="#nltk.tokenize.toktok.ToktokTokenizer.URL_FOE_2" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="nltk.tokenize.toktok.ToktokTokenizer.URL_FOE_3">
<code class="sig-name descname"><span class="pre">URL_FOE_3</span></code><em class="property"> <span class="pre">=</span> <span class="pre">(re.compile('(:\\/\\/)[\\S+\\.\\S+\\/\\S+][\\/]'),</span> <span class="pre">'</span> <span class="pre">/</span> <span class="pre">')</span></em><a class="headerlink" href="#nltk.tokenize.toktok.ToktokTokenizer.URL_FOE_3" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="nltk.tokenize.toktok.ToktokTokenizer.URL_FOE_4">
<code class="sig-name descname"><span class="pre">URL_FOE_4</span></code><em class="property"> <span class="pre">=</span> <span class="pre">(re.compile('</span> <span class="pre">/'),</span> <span class="pre">'</span> <span class="pre">/</span> <span class="pre">')</span></em><a class="headerlink" href="#nltk.tokenize.toktok.ToktokTokenizer.URL_FOE_4" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="nltk.tokenize.toktok.ToktokTokenizer.tokenize">
<code class="sig-name descname"><span class="pre">tokenize</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">text</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_str</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tokenize/toktok.html#ToktokTokenizer.tokenize"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.toktok.ToktokTokenizer.tokenize" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a tokenized copy of <em>s</em>.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>list of str</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-nltk.tokenize.treebank">
<span id="nltk-tokenize-treebank-module"></span><h2>nltk.tokenize.treebank module<a class="headerlink" href="#module-nltk.tokenize.treebank" title="Permalink to this headline">¶</a></h2>
<p>Penn Treebank Tokenizer</p>
<p>The Treebank tokenizer uses regular expressions to tokenize text as in Penn Treebank.
This implementation is a port of the tokenizer sed script written by Robert McIntyre
and available at <a class="reference external" href="http://www.cis.upenn.edu/~treebank/tokenizer.sed">http://www.cis.upenn.edu/~treebank/tokenizer.sed</a>.</p>
<dl class="py class">
<dt id="nltk.tokenize.treebank.TreebankWordDetokenizer">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">nltk.tokenize.treebank.</span></code><code class="sig-name descname"><span class="pre">TreebankWordDetokenizer</span></code><a class="reference internal" href="../_modules/nltk/tokenize/treebank.html#TreebankWordDetokenizer"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.treebank.TreebankWordDetokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#nltk.tokenize.api.TokenizerI" title="nltk.tokenize.api.TokenizerI"><code class="xref py py-class docutils literal notranslate"><span class="pre">nltk.tokenize.api.TokenizerI</span></code></a></p>
<p>The Treebank detokenizer uses the reverse regex operations corresponding to
the Treebank tokenizer’s regexes.</p>
<p>Note:
- There’re additional assumption mades when undoing the padding of [;&#64;#$%&amp;]</p>
<blockquote>
<div><p>punctuation symbols that isn’t presupposed in the TreebankTokenizer.</p>
</div></blockquote>
<ul>
<li><dl class="simple">
<dt>There’re additional regexes added in reversing the parentheses tokenization,</dt><dd><ul class="simple">
<li><p>the r’([])}&gt;])s([:;,.])’ removes the additional right padding added
to the closing parentheses precedding [:;,.].</p></li>
</ul>
</dd>
</dl>
</li>
<li><p>It’s not possible to return the original whitespaces as they were because
there wasn’t explicit records of where ‘n’, ‘t’ or ‘s’ were removed at
the text.split() operation.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">nltk.tokenize.treebank</span> <span class="kn">import</span> <span class="n">TreebankWordTokenizer</span><span class="p">,</span> <span class="n">TreebankWordDetokenizer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s</span> <span class="o">=</span> <span class="s1">&#39;&#39;&#39;Good muffins cost $3.88</span><span class="se">\n</span><span class="s1">in New York.  Please buy me</span><span class="se">\n</span><span class="s1">two of them.</span><span class="se">\n</span><span class="s1">Thanks.&#39;&#39;&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span> <span class="o">=</span> <span class="n">TreebankWordDetokenizer</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t</span> <span class="o">=</span> <span class="n">TreebankWordTokenizer</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">toks</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span><span class="o">.</span><span class="n">detokenize</span><span class="p">(</span><span class="n">toks</span><span class="p">)</span>
<span class="go">&#39;Good muffins cost $3.88 in New York. Please buy me two of them. Thanks.&#39;</span>
</pre></div>
</div>
</li>
</ul>
<p>The MXPOST parentheses substitution can be undone using the <cite>convert_parentheses</cite>
parameter:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">s</span> <span class="o">=</span> <span class="s1">&#39;&#39;&#39;Good muffins cost $3.88</span><span class="se">\n</span><span class="s1">in New (York).  Please (buy) me</span><span class="se">\n</span><span class="s1">two of them.</span><span class="se">\n</span><span class="s1">(Thanks).&#39;&#39;&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">expected_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Good&#39;</span><span class="p">,</span> <span class="s1">&#39;muffins&#39;</span><span class="p">,</span> <span class="s1">&#39;cost&#39;</span><span class="p">,</span> <span class="s1">&#39;$&#39;</span><span class="p">,</span> <span class="s1">&#39;3.88&#39;</span><span class="p">,</span> <span class="s1">&#39;in&#39;</span><span class="p">,</span>
<span class="gp">... </span><span class="s1">&#39;New&#39;</span><span class="p">,</span> <span class="s1">&#39;-LRB-&#39;</span><span class="p">,</span> <span class="s1">&#39;York&#39;</span><span class="p">,</span> <span class="s1">&#39;-RRB-&#39;</span><span class="p">,</span> <span class="s1">&#39;.&#39;</span><span class="p">,</span> <span class="s1">&#39;Please&#39;</span><span class="p">,</span> <span class="s1">&#39;-LRB-&#39;</span><span class="p">,</span> <span class="s1">&#39;buy&#39;</span><span class="p">,</span>
<span class="gp">... </span><span class="s1">&#39;-RRB-&#39;</span><span class="p">,</span> <span class="s1">&#39;me&#39;</span><span class="p">,</span> <span class="s1">&#39;two&#39;</span><span class="p">,</span> <span class="s1">&#39;of&#39;</span><span class="p">,</span> <span class="s1">&#39;them.&#39;</span><span class="p">,</span> <span class="s1">&#39;-LRB-&#39;</span><span class="p">,</span> <span class="s1">&#39;Thanks&#39;</span><span class="p">,</span> <span class="s1">&#39;-RRB-&#39;</span><span class="p">,</span> <span class="s1">&#39;.&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">expected_tokens</span> <span class="o">==</span> <span class="n">t</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">convert_parentheses</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">expected_detoken</span> <span class="o">=</span> <span class="s1">&#39;Good muffins cost $3.88 in New (York). Please (buy) me two of them. (Thanks).&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">expected_detoken</span> <span class="o">==</span> <span class="n">d</span><span class="o">.</span><span class="n">detokenize</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">convert_parentheses</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span> <span class="n">convert_parentheses</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="go">True</span>
</pre></div>
</div>
<p>During tokenization it’s safe to add more spaces but during detokenization,
simply undoing the padding doesn’t really help.</p>
<ul class="simple">
<li><p>During tokenization, left and right pad is added to [!?], when
detokenizing, only left shift the [!?] is needed.
Thus (re.compile(r’s([?!])’), r’g&lt;1&gt;’)</p></li>
<li><p>During tokenization [:,] are left and right padded but when detokenizing,
only left shift is necessary and we keep right pad after comma/colon
if the string after is a non-digit.
Thus (re.compile(r’s([:,])s([^d])’), r’1 2’)</p></li>
</ul>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">nltk.tokenize.treebank</span> <span class="kn">import</span> <span class="n">TreebankWordDetokenizer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">toks</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;hello&#39;</span><span class="p">,</span> <span class="s1">&#39;,&#39;</span><span class="p">,</span> <span class="s1">&#39;i&#39;</span><span class="p">,</span> <span class="s1">&#39;ca&#39;</span><span class="p">,</span> <span class="s2">&quot;n&#39;t&quot;</span><span class="p">,</span> <span class="s1">&#39;feel&#39;</span><span class="p">,</span> <span class="s1">&#39;my&#39;</span><span class="p">,</span> <span class="s1">&#39;feet&#39;</span><span class="p">,</span> <span class="s1">&#39;!&#39;</span><span class="p">,</span> <span class="s1">&#39;Help&#39;</span><span class="p">,</span> <span class="s1">&#39;!&#39;</span><span class="p">,</span> <span class="s1">&#39;!&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">twd</span> <span class="o">=</span> <span class="n">TreebankWordDetokenizer</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">twd</span><span class="o">.</span><span class="n">detokenize</span><span class="p">(</span><span class="n">toks</span><span class="p">)</span>
<span class="go">&quot;hello, i can&#39;t feel my feet! Help!!&quot;</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">toks</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;hello&#39;</span><span class="p">,</span> <span class="s1">&#39;,&#39;</span><span class="p">,</span> <span class="s1">&#39;i&#39;</span><span class="p">,</span> <span class="s2">&quot;can&#39;t&quot;</span><span class="p">,</span> <span class="s1">&#39;feel&#39;</span><span class="p">,</span> <span class="s1">&#39;;&#39;</span><span class="p">,</span> <span class="s1">&#39;my&#39;</span><span class="p">,</span> <span class="s1">&#39;feet&#39;</span><span class="p">,</span> <span class="s1">&#39;!&#39;</span><span class="p">,</span>
<span class="gp">... </span><span class="s1">&#39;Help&#39;</span><span class="p">,</span> <span class="s1">&#39;!&#39;</span><span class="p">,</span> <span class="s1">&#39;!&#39;</span><span class="p">,</span> <span class="s1">&#39;He&#39;</span><span class="p">,</span> <span class="s1">&#39;said&#39;</span><span class="p">,</span> <span class="s1">&#39;:&#39;</span><span class="p">,</span> <span class="s1">&#39;Help&#39;</span><span class="p">,</span> <span class="s1">&#39;,&#39;</span><span class="p">,</span> <span class="s1">&#39;help&#39;</span><span class="p">,</span> <span class="s1">&#39;?&#39;</span><span class="p">,</span> <span class="s1">&#39;!&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">twd</span><span class="o">.</span><span class="n">detokenize</span><span class="p">(</span><span class="n">toks</span><span class="p">)</span>
<span class="go">&quot;hello, i can&#39;t feel; my feet! Help!! He said: Help, help?!&quot;</span>
</pre></div>
</div>
<dl class="py attribute">
<dt id="nltk.tokenize.treebank.TreebankWordDetokenizer.CONTRACTIONS2">
<code class="sig-name descname"><span class="pre">CONTRACTIONS2</span></code><em class="property"> <span class="pre">=</span> <span class="pre">[re.compile('(?i)\\b(can)\\s(not)\\b',</span> <span class="pre">re.IGNORECASE),</span> <span class="pre">re.compile(&quot;(?i)\\b(d)\\s('ye)\\b&quot;,</span> <span class="pre">re.IGNORECASE),</span> <span class="pre">re.compile('(?i)\\b(gim)\\s(me)\\b',</span> <span class="pre">re.IGNORECASE),</span> <span class="pre">re.compile('(?i)\\b(gon)\\s(na)\\b',</span> <span class="pre">re.IGNORECASE),</span> <span class="pre">re.compile('(?i)\\b(got)\\s(ta)\\b',</span> <span class="pre">re.IGNORECASE),</span> <span class="pre">re.compile('(?i)\\b(lem)\\s(me)\\b',</span> <span class="pre">re.IGNORECASE),</span> <span class="pre">re.compile(&quot;(?i)\\b(more)\\s('n)\\b&quot;,</span> <span class="pre">re.IGNORECASE),</span> <span class="pre">re.compile('(?i)\\b(wan)\\s(na)\\s',</span> <span class="pre">re.IGNORECASE)]</span></em><a class="headerlink" href="#nltk.tokenize.treebank.TreebankWordDetokenizer.CONTRACTIONS2" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="nltk.tokenize.treebank.TreebankWordDetokenizer.CONTRACTIONS3">
<code class="sig-name descname"><span class="pre">CONTRACTIONS3</span></code><em class="property"> <span class="pre">=</span> <span class="pre">[re.compile(&quot;(?i)</span> <span class="pre">('t)\\s(is)\\b&quot;,</span> <span class="pre">re.IGNORECASE),</span> <span class="pre">re.compile(&quot;(?i)</span> <span class="pre">('t)\\s(was)\\b&quot;,</span> <span class="pre">re.IGNORECASE)]</span></em><a class="headerlink" href="#nltk.tokenize.treebank.TreebankWordDetokenizer.CONTRACTIONS3" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="nltk.tokenize.treebank.TreebankWordDetokenizer.CONVERT_PARENTHESES">
<code class="sig-name descname"><span class="pre">CONVERT_PARENTHESES</span></code><em class="property"> <span class="pre">=</span> <span class="pre">[(re.compile('-LRB-'),</span> <span class="pre">'('),</span> <span class="pre">(re.compile('-RRB-'),</span> <span class="pre">')'),</span> <span class="pre">(re.compile('-LSB-'),</span> <span class="pre">'['),</span> <span class="pre">(re.compile('-RSB-'),</span> <span class="pre">']'),</span> <span class="pre">(re.compile('-LCB-'),</span> <span class="pre">'{'),</span> <span class="pre">(re.compile('-RCB-'),</span> <span class="pre">'}')]</span></em><a class="headerlink" href="#nltk.tokenize.treebank.TreebankWordDetokenizer.CONVERT_PARENTHESES" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="nltk.tokenize.treebank.TreebankWordDetokenizer.DOUBLE_DASHES">
<code class="sig-name descname"><span class="pre">DOUBLE_DASHES</span></code><em class="property"> <span class="pre">=</span> <span class="pre">(re.compile('</span> <span class="pre">--</span> <span class="pre">'),</span> <span class="pre">'--')</span></em><a class="headerlink" href="#nltk.tokenize.treebank.TreebankWordDetokenizer.DOUBLE_DASHES" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="nltk.tokenize.treebank.TreebankWordDetokenizer.ENDING_QUOTES">
<code class="sig-name descname"><span class="pre">ENDING_QUOTES</span></code><em class="property"> <span class="pre">=</span> <span class="pre">[(re.compile(&quot;([^'</span> <span class="pre">])\\s('ll|'LL|'re|'RE|'ve|'VE|n't|N'T)</span> <span class="pre">&quot;),</span> <span class="pre">'\\1\\2</span> <span class="pre">'),</span> <span class="pre">(re.compile(&quot;([^'</span> <span class="pre">])\\s('[sS]|'[mM]|'[dD]|')</span> <span class="pre">&quot;),</span> <span class="pre">'\\1\\2</span> <span class="pre">'),</span> <span class="pre">(re.compile(&quot;(\\S)\\s(\\'\\')&quot;),</span> <span class="pre">'\\1\\2'),</span> <span class="pre">(re.compile(&quot;(\\'\\')\\s([.,:)\\]&gt;};%])&quot;),</span> <span class="pre">'\\1\\2'),</span> <span class="pre">(re.compile(&quot;''&quot;),</span> <span class="pre">'&quot;')]</span></em><a class="headerlink" href="#nltk.tokenize.treebank.TreebankWordDetokenizer.ENDING_QUOTES" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="nltk.tokenize.treebank.TreebankWordDetokenizer.PARENS_BRACKETS">
<code class="sig-name descname"><span class="pre">PARENS_BRACKETS</span></code><em class="property"> <span class="pre">=</span> <span class="pre">[(re.compile('([\\[\\(\\{\\&lt;])\\s'),</span> <span class="pre">'\\g&lt;1&gt;'),</span> <span class="pre">(re.compile('\\s([\\]\\)\\}\\&gt;])'),</span> <span class="pre">'\\g&lt;1&gt;'),</span> <span class="pre">(re.compile('([\\]\\)\\}\\&gt;])\\s([:;,.])'),</span> <span class="pre">'\\1\\2')]</span></em><a class="headerlink" href="#nltk.tokenize.treebank.TreebankWordDetokenizer.PARENS_BRACKETS" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="nltk.tokenize.treebank.TreebankWordDetokenizer.PUNCTUATION">
<code class="sig-name descname"><span class="pre">PUNCTUATION</span></code><em class="property"> <span class="pre">=</span> <span class="pre">[(re.compile(&quot;([^'])\\s'\\s&quot;),</span> <span class="pre">&quot;\\1'</span> <span class="pre">&quot;),</span> <span class="pre">(re.compile('\\s([?!])'),</span> <span class="pre">'\\g&lt;1&gt;'),</span> <span class="pre">(re.compile('([^\\.])\\s(\\.)([\\]\\)}&gt;&quot;\\\']*)\\s*$'),</span> <span class="pre">'\\1\\2\\3'),</span> <span class="pre">(re.compile('([#$])\\s'),</span> <span class="pre">'\\g&lt;1&gt;'),</span> <span class="pre">(re.compile('\\s([;%])'),</span> <span class="pre">'\\g&lt;1&gt;'),</span> <span class="pre">(re.compile('\\s\\.\\.\\.\\s'),</span> <span class="pre">'...'),</span> <span class="pre">(re.compile('\\s([:,])'),</span> <span class="pre">'\\1')]</span></em><a class="headerlink" href="#nltk.tokenize.treebank.TreebankWordDetokenizer.PUNCTUATION" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="nltk.tokenize.treebank.TreebankWordDetokenizer.STARTING_QUOTES">
<code class="sig-name descname"><span class="pre">STARTING_QUOTES</span></code><em class="property"> <span class="pre">=</span> <span class="pre">[(re.compile('([</span> <span class="pre">(\\[{&lt;])\\s``'),</span> <span class="pre">'\\1``'),</span> <span class="pre">(re.compile('(``)\\s'),</span> <span class="pre">'\\1'),</span> <span class="pre">(re.compile('``'),</span> <span class="pre">'&quot;')]</span></em><a class="headerlink" href="#nltk.tokenize.treebank.TreebankWordDetokenizer.STARTING_QUOTES" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="nltk.tokenize.treebank.TreebankWordDetokenizer.detokenize">
<code class="sig-name descname"><span class="pre">detokenize</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tokens</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">convert_parentheses</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tokenize/treebank.html#TreebankWordDetokenizer.detokenize"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.treebank.TreebankWordDetokenizer.detokenize" title="Permalink to this definition">¶</a></dt>
<dd><p>Duck-typing the abstract <em>tokenize()</em>.</p>
</dd></dl>

<dl class="py method">
<dt id="nltk.tokenize.treebank.TreebankWordDetokenizer.tokenize">
<code class="sig-name descname"><span class="pre">tokenize</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tokens</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">convert_parentheses</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tokenize/treebank.html#TreebankWordDetokenizer.tokenize"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.treebank.TreebankWordDetokenizer.tokenize" title="Permalink to this definition">¶</a></dt>
<dd><p>Treebank detokenizer, created by undoing the regexes from
the TreebankWordTokenizer.tokenize.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>tokens</strong> (<em>list</em><em>(</em><em>str</em><em>)</em>) – A list of strings, i.e. tokenized text.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>str</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="nltk.tokenize.treebank.TreebankWordTokenizer">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">nltk.tokenize.treebank.</span></code><code class="sig-name descname"><span class="pre">TreebankWordTokenizer</span></code><a class="reference internal" href="../_modules/nltk/tokenize/treebank.html#TreebankWordTokenizer"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.treebank.TreebankWordTokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#nltk.tokenize.api.TokenizerI" title="nltk.tokenize.api.TokenizerI"><code class="xref py py-class docutils literal notranslate"><span class="pre">nltk.tokenize.api.TokenizerI</span></code></a></p>
<p>The Treebank tokenizer uses regular expressions to tokenize text as in Penn Treebank.
This is the method that is invoked by <code class="docutils literal notranslate"><span class="pre">word_tokenize()</span></code>.  It assumes that the
text has already been segmented into sentences, e.g. using <code class="docutils literal notranslate"><span class="pre">sent_tokenize()</span></code>.</p>
<p>This tokenizer performs the following steps:</p>
<ul>
<li><p>split standard contractions, e.g. <code class="docutils literal notranslate"><span class="pre">don't</span></code> -&gt; <code class="docutils literal notranslate"><span class="pre">do</span> <span class="pre">n't</span></code> and <code class="docutils literal notranslate"><span class="pre">they'll</span></code> -&gt; <code class="docutils literal notranslate"><span class="pre">they</span> <span class="pre">'ll</span></code></p></li>
<li><p>treat most punctuation characters as separate tokens</p></li>
<li><p>split off commas and single quotes, when followed by whitespace</p></li>
<li><p>separate periods that appear at the end of line</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">nltk.tokenize</span> <span class="kn">import</span> <span class="n">TreebankWordTokenizer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s</span> <span class="o">=</span> <span class="s1">&#39;&#39;&#39;Good muffins cost $3.88</span><span class="se">\n</span><span class="s1">in New York.  Please buy me</span><span class="se">\n</span><span class="s1">two of them.</span><span class="se">\n</span><span class="s1">Thanks.&#39;&#39;&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">TreebankWordTokenizer</span><span class="p">()</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
<span class="go">[&#39;Good&#39;, &#39;muffins&#39;, &#39;cost&#39;, &#39;$&#39;, &#39;3.88&#39;, &#39;in&#39;, &#39;New&#39;, &#39;York.&#39;, &#39;Please&#39;, &#39;buy&#39;, &#39;me&#39;, &#39;two&#39;, &#39;of&#39;, &#39;them.&#39;, &#39;Thanks&#39;, &#39;.&#39;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s</span> <span class="o">=</span> <span class="s2">&quot;They&#39;ll save and invest more.&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">TreebankWordTokenizer</span><span class="p">()</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
<span class="go">[&#39;They&#39;, &quot;&#39;ll&quot;, &#39;save&#39;, &#39;and&#39;, &#39;invest&#39;, &#39;more&#39;, &#39;.&#39;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s</span> <span class="o">=</span> <span class="s2">&quot;hi, my name can&#39;t hello,&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">TreebankWordTokenizer</span><span class="p">()</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
<span class="go">[&#39;hi&#39;, &#39;,&#39;, &#39;my&#39;, &#39;name&#39;, &#39;ca&#39;, &quot;n&#39;t&quot;, &#39;hello&#39;, &#39;,&#39;]</span>
</pre></div>
</div>
</li>
</ul>
<dl class="py attribute">
<dt id="nltk.tokenize.treebank.TreebankWordTokenizer.CONTRACTIONS2">
<code class="sig-name descname"><span class="pre">CONTRACTIONS2</span></code><em class="property"> <span class="pre">=</span> <span class="pre">[re.compile('(?i)\\b(can)(?#X)(not)\\b',</span> <span class="pre">re.IGNORECASE),</span> <span class="pre">re.compile(&quot;(?i)\\b(d)(?#X)('ye)\\b&quot;,</span> <span class="pre">re.IGNORECASE),</span> <span class="pre">re.compile('(?i)\\b(gim)(?#X)(me)\\b',</span> <span class="pre">re.IGNORECASE),</span> <span class="pre">re.compile('(?i)\\b(gon)(?#X)(na)\\b',</span> <span class="pre">re.IGNORECASE),</span> <span class="pre">re.compile('(?i)\\b(got)(?#X)(ta)\\b',</span> <span class="pre">re.IGNORECASE),</span> <span class="pre">re.compile('(?i)\\b(lem)(?#X)(me)\\b',</span> <span class="pre">re.IGNORECASE),</span> <span class="pre">re.compile(&quot;(?i)\\b(more)(?#X)('n)\\b&quot;,</span> <span class="pre">re.IGNORECASE),</span> <span class="pre">re.compile('(?i)\\b(wan)(?#X)(na)\\s',</span> <span class="pre">re.IGNORECASE)]</span></em><a class="headerlink" href="#nltk.tokenize.treebank.TreebankWordTokenizer.CONTRACTIONS2" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="nltk.tokenize.treebank.TreebankWordTokenizer.CONTRACTIONS3">
<code class="sig-name descname"><span class="pre">CONTRACTIONS3</span></code><em class="property"> <span class="pre">=</span> <span class="pre">[re.compile(&quot;(?i)</span> <span class="pre">('t)(?#X)(is)\\b&quot;,</span> <span class="pre">re.IGNORECASE),</span> <span class="pre">re.compile(&quot;(?i)</span> <span class="pre">('t)(?#X)(was)\\b&quot;,</span> <span class="pre">re.IGNORECASE)]</span></em><a class="headerlink" href="#nltk.tokenize.treebank.TreebankWordTokenizer.CONTRACTIONS3" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="nltk.tokenize.treebank.TreebankWordTokenizer.CONVERT_PARENTHESES">
<code class="sig-name descname"><span class="pre">CONVERT_PARENTHESES</span></code><em class="property"> <span class="pre">=</span> <span class="pre">[(re.compile('\\('),</span> <span class="pre">'-LRB-'),</span> <span class="pre">(re.compile('\\)'),</span> <span class="pre">'-RRB-'),</span> <span class="pre">(re.compile('\\['),</span> <span class="pre">'-LSB-'),</span> <span class="pre">(re.compile('\\]'),</span> <span class="pre">'-RSB-'),</span> <span class="pre">(re.compile('\\{'),</span> <span class="pre">'-LCB-'),</span> <span class="pre">(re.compile('\\}'),</span> <span class="pre">'-RCB-')]</span></em><a class="headerlink" href="#nltk.tokenize.treebank.TreebankWordTokenizer.CONVERT_PARENTHESES" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="nltk.tokenize.treebank.TreebankWordTokenizer.DOUBLE_DASHES">
<code class="sig-name descname"><span class="pre">DOUBLE_DASHES</span></code><em class="property"> <span class="pre">=</span> <span class="pre">(re.compile('--'),</span> <span class="pre">'</span> <span class="pre">--</span> <span class="pre">')</span></em><a class="headerlink" href="#nltk.tokenize.treebank.TreebankWordTokenizer.DOUBLE_DASHES" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="nltk.tokenize.treebank.TreebankWordTokenizer.ENDING_QUOTES">
<code class="sig-name descname"><span class="pre">ENDING_QUOTES</span></code><em class="property"> <span class="pre">=</span> <span class="pre">[(re.compile('&quot;'),</span> <span class="pre">&quot;</span> <span class="pre">''</span> <span class="pre">&quot;),</span> <span class="pre">(re.compile(&quot;(\\S)(\\'\\')&quot;),</span> <span class="pre">'\\1</span> <span class="pre">\\2</span> <span class="pre">'),</span> <span class="pre">(re.compile(&quot;([^'</span> <span class="pre">])('[sS]|'[mM]|'[dD]|')</span> <span class="pre">&quot;),</span> <span class="pre">'\\1</span> <span class="pre">\\2</span> <span class="pre">'),</span> <span class="pre">(re.compile(&quot;([^'</span> <span class="pre">])('ll|'LL|'re|'RE|'ve|'VE|n't|N'T)</span> <span class="pre">&quot;),</span> <span class="pre">'\\1</span> <span class="pre">\\2</span> <span class="pre">')]</span></em><a class="headerlink" href="#nltk.tokenize.treebank.TreebankWordTokenizer.ENDING_QUOTES" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="nltk.tokenize.treebank.TreebankWordTokenizer.PARENS_BRACKETS">
<code class="sig-name descname"><span class="pre">PARENS_BRACKETS</span></code><em class="property"> <span class="pre">=</span> <span class="pre">(re.compile('[\\]\\[\\(\\)\\{\\}\\&lt;\\&gt;]'),</span> <span class="pre">'</span> <span class="pre">\\g&lt;0&gt;</span> <span class="pre">')</span></em><a class="headerlink" href="#nltk.tokenize.treebank.TreebankWordTokenizer.PARENS_BRACKETS" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="nltk.tokenize.treebank.TreebankWordTokenizer.PUNCTUATION">
<code class="sig-name descname"><span class="pre">PUNCTUATION</span></code><em class="property"> <span class="pre">=</span> <span class="pre">[(re.compile('([:,])([^\\d])'),</span> <span class="pre">'</span> <span class="pre">\\1</span> <span class="pre">\\2'),</span> <span class="pre">(re.compile('([:,])$'),</span> <span class="pre">'</span> <span class="pre">\\1</span> <span class="pre">'),</span> <span class="pre">(re.compile('\\.\\.\\.'),</span> <span class="pre">'</span> <span class="pre">...</span> <span class="pre">'),</span> <span class="pre">(re.compile('[;&#64;#$%&amp;]'),</span> <span class="pre">'</span> <span class="pre">\\g&lt;0&gt;</span> <span class="pre">'),</span> <span class="pre">(re.compile('([^\\.])(\\.)([\\]\\)}&gt;&quot;\\\']*)\\s*$'),</span> <span class="pre">'\\1</span> <span class="pre">\\2\\3</span> <span class="pre">'),</span> <span class="pre">(re.compile('[?!]'),</span> <span class="pre">'</span> <span class="pre">\\g&lt;0&gt;</span> <span class="pre">'),</span> <span class="pre">(re.compile(&quot;([^'])'</span> <span class="pre">&quot;),</span> <span class="pre">&quot;\\1</span> <span class="pre">'</span> <span class="pre">&quot;)]</span></em><a class="headerlink" href="#nltk.tokenize.treebank.TreebankWordTokenizer.PUNCTUATION" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="nltk.tokenize.treebank.TreebankWordTokenizer.STARTING_QUOTES">
<code class="sig-name descname"><span class="pre">STARTING_QUOTES</span></code><em class="property"> <span class="pre">=</span> <span class="pre">[(re.compile('^\\&quot;'),</span> <span class="pre">'``'),</span> <span class="pre">(re.compile('(``)'),</span> <span class="pre">'</span> <span class="pre">\\1</span> <span class="pre">'),</span> <span class="pre">(re.compile('([</span> <span class="pre">\\(\\[{&lt;])(\\&quot;|\\\'{2})'),</span> <span class="pre">'\\1</span> <span class="pre">``</span> <span class="pre">')]</span></em><a class="headerlink" href="#nltk.tokenize.treebank.TreebankWordTokenizer.STARTING_QUOTES" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="nltk.tokenize.treebank.TreebankWordTokenizer.span_tokenize">
<code class="sig-name descname"><span class="pre">span_tokenize</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">text</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tokenize/treebank.html#TreebankWordTokenizer.span_tokenize"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.treebank.TreebankWordTokenizer.span_tokenize" title="Permalink to this definition">¶</a></dt>
<dd><p>Uses the post-hoc nltk.tokens.align_tokens to return the offset spans.</p>
<blockquote>
<div><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">nltk.tokenize</span> <span class="kn">import</span> <span class="n">TreebankWordTokenizer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s</span> <span class="o">=</span> <span class="s1">&#39;&#39;&#39;Good muffins cost $3.88</span><span class="se">\n</span><span class="s1">in New (York).  Please (buy) me</span><span class="se">\n</span><span class="s1">two of them.</span><span class="se">\n</span><span class="s1">(Thanks).&#39;&#39;&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">expected</span> <span class="o">=</span> <span class="p">[(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">12</span><span class="p">),</span> <span class="p">(</span><span class="mi">13</span><span class="p">,</span> <span class="mi">17</span><span class="p">),</span> <span class="p">(</span><span class="mi">18</span><span class="p">,</span> <span class="mi">19</span><span class="p">),</span> <span class="p">(</span><span class="mi">19</span><span class="p">,</span> <span class="mi">23</span><span class="p">),</span>
<span class="gp">... </span><span class="p">(</span><span class="mi">24</span><span class="p">,</span> <span class="mi">26</span><span class="p">),</span> <span class="p">(</span><span class="mi">27</span><span class="p">,</span> <span class="mi">30</span><span class="p">),</span> <span class="p">(</span><span class="mi">31</span><span class="p">,</span> <span class="mi">32</span><span class="p">),</span> <span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">36</span><span class="p">),</span> <span class="p">(</span><span class="mi">36</span><span class="p">,</span> <span class="mi">37</span><span class="p">),</span> <span class="p">(</span><span class="mi">37</span><span class="p">,</span> <span class="mi">38</span><span class="p">),</span>
<span class="gp">... </span><span class="p">(</span><span class="mi">40</span><span class="p">,</span> <span class="mi">46</span><span class="p">),</span> <span class="p">(</span><span class="mi">47</span><span class="p">,</span> <span class="mi">48</span><span class="p">),</span> <span class="p">(</span><span class="mi">48</span><span class="p">,</span> <span class="mi">51</span><span class="p">),</span> <span class="p">(</span><span class="mi">51</span><span class="p">,</span> <span class="mi">52</span><span class="p">),</span> <span class="p">(</span><span class="mi">53</span><span class="p">,</span> <span class="mi">55</span><span class="p">),</span> <span class="p">(</span><span class="mi">56</span><span class="p">,</span> <span class="mi">59</span><span class="p">),</span>
<span class="gp">... </span><span class="p">(</span><span class="mi">60</span><span class="p">,</span> <span class="mi">62</span><span class="p">),</span> <span class="p">(</span><span class="mi">63</span><span class="p">,</span> <span class="mi">68</span><span class="p">),</span> <span class="p">(</span><span class="mi">69</span><span class="p">,</span> <span class="mi">70</span><span class="p">),</span> <span class="p">(</span><span class="mi">70</span><span class="p">,</span> <span class="mi">76</span><span class="p">),</span> <span class="p">(</span><span class="mi">76</span><span class="p">,</span> <span class="mi">77</span><span class="p">),</span> <span class="p">(</span><span class="mi">77</span><span class="p">,</span> <span class="mi">78</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">list</span><span class="p">(</span><span class="n">TreebankWordTokenizer</span><span class="p">()</span><span class="o">.</span><span class="n">span_tokenize</span><span class="p">(</span><span class="n">s</span><span class="p">))</span> <span class="o">==</span> <span class="n">expected</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">expected</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Good&#39;</span><span class="p">,</span> <span class="s1">&#39;muffins&#39;</span><span class="p">,</span> <span class="s1">&#39;cost&#39;</span><span class="p">,</span> <span class="s1">&#39;$&#39;</span><span class="p">,</span> <span class="s1">&#39;3.88&#39;</span><span class="p">,</span> <span class="s1">&#39;in&#39;</span><span class="p">,</span>
<span class="gp">... </span><span class="s1">&#39;New&#39;</span><span class="p">,</span> <span class="s1">&#39;(&#39;</span><span class="p">,</span> <span class="s1">&#39;York&#39;</span><span class="p">,</span> <span class="s1">&#39;)&#39;</span><span class="p">,</span> <span class="s1">&#39;.&#39;</span><span class="p">,</span> <span class="s1">&#39;Please&#39;</span><span class="p">,</span> <span class="s1">&#39;(&#39;</span><span class="p">,</span> <span class="s1">&#39;buy&#39;</span><span class="p">,</span> <span class="s1">&#39;)&#39;</span><span class="p">,</span>
<span class="gp">... </span><span class="s1">&#39;me&#39;</span><span class="p">,</span> <span class="s1">&#39;two&#39;</span><span class="p">,</span> <span class="s1">&#39;of&#39;</span><span class="p">,</span> <span class="s1">&#39;them.&#39;</span><span class="p">,</span> <span class="s1">&#39;(&#39;</span><span class="p">,</span> <span class="s1">&#39;Thanks&#39;</span><span class="p">,</span> <span class="s1">&#39;)&#39;</span><span class="p">,</span> <span class="s1">&#39;.&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="n">s</span><span class="p">[</span><span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">]</span> <span class="k">for</span> <span class="n">start</span><span class="p">,</span> <span class="n">end</span> <span class="ow">in</span> <span class="n">TreebankWordTokenizer</span><span class="p">()</span><span class="o">.</span><span class="n">span_tokenize</span><span class="p">(</span><span class="n">s</span><span class="p">)]</span> <span class="o">==</span> <span class="n">expected</span>
<span class="go">True</span>
</pre></div>
</div>
<p>Additional example
&gt;&gt;&gt; from nltk.tokenize import TreebankWordTokenizer
&gt;&gt;&gt; s = ‘’’I said, “I’d like to buy some ‘’good muffins” which cost $3.88n each in New (York).”’’’
&gt;&gt;&gt; expected = [(0, 1), (2, 6), (6, 7), (8, 9), (9, 10), (10, 12),
… (13, 17), (18, 20), (21, 24), (25, 29), (30, 32), (32, 36),
… (37, 44), (44, 45), (46, 51), (52, 56), (57, 58), (58, 62),
… (64, 68), (69, 71), (72, 75), (76, 77), (77, 81), (81, 82),
… (82, 83), (83, 84)]
&gt;&gt;&gt; list(TreebankWordTokenizer().span_tokenize(s)) == expected
True
&gt;&gt;&gt; expected = [‘I’, ‘said’, ‘,’, ‘”’, ‘I’, “‘d”, ‘like’, ‘to’,
… ‘buy’, ‘some’, “’’”, “good”, ‘muffins’, ‘”’, ‘which’, ‘cost’,
… ‘$’, ‘3.88’, ‘each’, ‘in’, ‘New’, ‘(‘, ‘York’, ‘)’, ‘.’, ‘”’]
&gt;&gt;&gt; [s[start:end] for start, end in TreebankWordTokenizer().span_tokenize(s)] == expected
True</p>
</div></blockquote>
</dd></dl>

<dl class="py method">
<dt id="nltk.tokenize.treebank.TreebankWordTokenizer.tokenize">
<code class="sig-name descname"><span class="pre">tokenize</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">text</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">convert_parentheses</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_str</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tokenize/treebank.html#TreebankWordTokenizer.tokenize"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.treebank.TreebankWordTokenizer.tokenize" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a tokenized copy of <em>s</em>.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>list of str</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-nltk.tokenize.util">
<span id="nltk-tokenize-util-module"></span><h2>nltk.tokenize.util module<a class="headerlink" href="#module-nltk.tokenize.util" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="nltk.tokenize.util.CJKChars">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">nltk.tokenize.util.</span></code><code class="sig-name descname"><span class="pre">CJKChars</span></code><a class="reference internal" href="../_modules/nltk/tokenize/util.html#CJKChars"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.util.CJKChars" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>An object that enumerates the code points of the CJK characters as listed on
<a class="reference external" href="http://en.wikipedia.org/wiki/Basic_Multilingual_Plane#Basic_Multilingual_Plane">http://en.wikipedia.org/wiki/Basic_Multilingual_Plane#Basic_Multilingual_Plane</a></p>
<p>This is a Python port of the CJK code point enumerations of Moses tokenizer:
<a class="reference external" href="https://github.com/moses-smt/mosesdecoder/blob/master/scripts/tokenizer/detokenizer.perl#L309">https://github.com/moses-smt/mosesdecoder/blob/master/scripts/tokenizer/detokenizer.perl#L309</a></p>
<dl class="py attribute">
<dt id="nltk.tokenize.util.CJKChars.CJK_Compatibility_Forms">
<code class="sig-name descname"><span class="pre">CJK_Compatibility_Forms</span></code><em class="property"> <span class="pre">=</span> <span class="pre">(65072,</span> <span class="pre">65103)</span></em><a class="headerlink" href="#nltk.tokenize.util.CJKChars.CJK_Compatibility_Forms" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="nltk.tokenize.util.CJKChars.CJK_Compatibility_Ideographs">
<code class="sig-name descname"><span class="pre">CJK_Compatibility_Ideographs</span></code><em class="property"> <span class="pre">=</span> <span class="pre">(63744,</span> <span class="pre">64255)</span></em><a class="headerlink" href="#nltk.tokenize.util.CJKChars.CJK_Compatibility_Ideographs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="nltk.tokenize.util.CJKChars.CJK_Radicals">
<code class="sig-name descname"><span class="pre">CJK_Radicals</span></code><em class="property"> <span class="pre">=</span> <span class="pre">(11904,</span> <span class="pre">42191)</span></em><a class="headerlink" href="#nltk.tokenize.util.CJKChars.CJK_Radicals" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="nltk.tokenize.util.CJKChars.Hangul_Jamo">
<code class="sig-name descname"><span class="pre">Hangul_Jamo</span></code><em class="property"> <span class="pre">=</span> <span class="pre">(4352,</span> <span class="pre">4607)</span></em><a class="headerlink" href="#nltk.tokenize.util.CJKChars.Hangul_Jamo" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="nltk.tokenize.util.CJKChars.Hangul_Syllables">
<code class="sig-name descname"><span class="pre">Hangul_Syllables</span></code><em class="property"> <span class="pre">=</span> <span class="pre">(44032,</span> <span class="pre">55215)</span></em><a class="headerlink" href="#nltk.tokenize.util.CJKChars.Hangul_Syllables" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="nltk.tokenize.util.CJKChars.Katakana_Hangul_Halfwidth">
<code class="sig-name descname"><span class="pre">Katakana_Hangul_Halfwidth</span></code><em class="property"> <span class="pre">=</span> <span class="pre">(65381,</span> <span class="pre">65500)</span></em><a class="headerlink" href="#nltk.tokenize.util.CJKChars.Katakana_Hangul_Halfwidth" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="nltk.tokenize.util.CJKChars.Phags_Pa">
<code class="sig-name descname"><span class="pre">Phags_Pa</span></code><em class="property"> <span class="pre">=</span> <span class="pre">(43072,</span> <span class="pre">43135)</span></em><a class="headerlink" href="#nltk.tokenize.util.CJKChars.Phags_Pa" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="nltk.tokenize.util.CJKChars.Supplementary_Ideographic_Plane">
<code class="sig-name descname"><span class="pre">Supplementary_Ideographic_Plane</span></code><em class="property"> <span class="pre">=</span> <span class="pre">(131072,</span> <span class="pre">196607)</span></em><a class="headerlink" href="#nltk.tokenize.util.CJKChars.Supplementary_Ideographic_Plane" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="nltk.tokenize.util.CJKChars.ranges">
<code class="sig-name descname"><span class="pre">ranges</span></code><em class="property"> <span class="pre">=</span> <span class="pre">[(4352,</span> <span class="pre">4607),</span> <span class="pre">(11904,</span> <span class="pre">42191),</span> <span class="pre">(43072,</span> <span class="pre">43135),</span> <span class="pre">(44032,</span> <span class="pre">55215),</span> <span class="pre">(63744,</span> <span class="pre">64255),</span> <span class="pre">(65072,</span> <span class="pre">65103),</span> <span class="pre">(65381,</span> <span class="pre">65500),</span> <span class="pre">(131072,</span> <span class="pre">196607)]</span></em><a class="headerlink" href="#nltk.tokenize.util.CJKChars.ranges" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py function">
<dt id="nltk.tokenize.util.align_tokens">
<code class="sig-prename descclassname"><span class="pre">nltk.tokenize.util.</span></code><code class="sig-name descname"><span class="pre">align_tokens</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tokens</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sentence</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tokenize/util.html#align_tokens"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.util.align_tokens" title="Permalink to this definition">¶</a></dt>
<dd><p>This module attempt to find the offsets of the tokens in <em>s</em>, as a sequence
of <code class="docutils literal notranslate"><span class="pre">(start,</span> <span class="pre">end)</span></code> tuples, given the tokens and also the source string.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">nltk.tokenize</span> <span class="kn">import</span> <span class="n">TreebankWordTokenizer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">nltk.tokenize.util</span> <span class="kn">import</span> <span class="n">align_tokens</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="s2">&quot;The plane, bound for St Petersburg, crashed in Egypt&#39;s &quot;</span>
<span class="gp">... </span><span class="s2">&quot;Sinai desert just 23 minutes after take-off from Sharm el-Sheikh &quot;</span>
<span class="gp">... </span><span class="s2">&quot;on Saturday.&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokens</span> <span class="o">=</span> <span class="n">TreebankWordTokenizer</span><span class="p">()</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">expected</span> <span class="o">=</span> <span class="p">[(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">9</span><span class="p">),</span> <span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="p">(</span><span class="mi">11</span><span class="p">,</span> <span class="mi">16</span><span class="p">),</span> <span class="p">(</span><span class="mi">17</span><span class="p">,</span> <span class="mi">20</span><span class="p">),</span> <span class="p">(</span><span class="mi">21</span><span class="p">,</span> <span class="mi">23</span><span class="p">),</span>
<span class="gp">... </span><span class="p">(</span><span class="mi">24</span><span class="p">,</span> <span class="mi">34</span><span class="p">),</span> <span class="p">(</span><span class="mi">34</span><span class="p">,</span> <span class="mi">35</span><span class="p">),</span> <span class="p">(</span><span class="mi">36</span><span class="p">,</span> <span class="mi">43</span><span class="p">),</span> <span class="p">(</span><span class="mi">44</span><span class="p">,</span> <span class="mi">46</span><span class="p">),</span> <span class="p">(</span><span class="mi">47</span><span class="p">,</span> <span class="mi">52</span><span class="p">),</span> <span class="p">(</span><span class="mi">52</span><span class="p">,</span> <span class="mi">54</span><span class="p">),</span>
<span class="gp">... </span><span class="p">(</span><span class="mi">55</span><span class="p">,</span> <span class="mi">60</span><span class="p">),</span> <span class="p">(</span><span class="mi">61</span><span class="p">,</span> <span class="mi">67</span><span class="p">),</span> <span class="p">(</span><span class="mi">68</span><span class="p">,</span> <span class="mi">72</span><span class="p">),</span> <span class="p">(</span><span class="mi">73</span><span class="p">,</span> <span class="mi">75</span><span class="p">),</span> <span class="p">(</span><span class="mi">76</span><span class="p">,</span> <span class="mi">83</span><span class="p">),</span> <span class="p">(</span><span class="mi">84</span><span class="p">,</span> <span class="mi">89</span><span class="p">),</span>
<span class="gp">... </span><span class="p">(</span><span class="mi">90</span><span class="p">,</span> <span class="mi">98</span><span class="p">),</span> <span class="p">(</span><span class="mi">99</span><span class="p">,</span> <span class="mi">103</span><span class="p">),</span> <span class="p">(</span><span class="mi">104</span><span class="p">,</span> <span class="mi">109</span><span class="p">),</span> <span class="p">(</span><span class="mi">110</span><span class="p">,</span> <span class="mi">119</span><span class="p">),</span> <span class="p">(</span><span class="mi">120</span><span class="p">,</span> <span class="mi">122</span><span class="p">),</span>
<span class="gp">... </span><span class="p">(</span><span class="mi">123</span><span class="p">,</span> <span class="mi">131</span><span class="p">),</span> <span class="p">(</span><span class="mi">131</span><span class="p">,</span> <span class="mi">132</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">align_tokens</span><span class="p">(</span><span class="n">tokens</span><span class="p">,</span> <span class="n">s</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">expected</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>  <span class="c1"># Check that length of tokens and tuples are the same.</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">expected</span> <span class="o">==</span> <span class="nb">list</span><span class="p">(</span><span class="n">align_tokens</span><span class="p">(</span><span class="n">tokens</span><span class="p">,</span> <span class="n">s</span><span class="p">))</span>  <span class="c1"># Check that the output is as expected.</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokens</span> <span class="o">==</span> <span class="p">[</span><span class="n">s</span><span class="p">[</span><span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">]</span> <span class="k">for</span> <span class="n">start</span><span class="p">,</span> <span class="n">end</span> <span class="ow">in</span> <span class="n">output</span><span class="p">]</span>  <span class="c1"># Check that the slices of the string corresponds to the tokens.</span>
<span class="go">True</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tokens</strong> (<em>list</em><em>(</em><em>str</em><em>)</em>) – The list of strings that are the result of tokenization</p></li>
<li><p><strong>sentence</strong> (<em>str</em>) – The original string</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>list(tuple(int,int))</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="nltk.tokenize.util.is_cjk">
<code class="sig-prename descclassname"><span class="pre">nltk.tokenize.util.</span></code><code class="sig-name descname"><span class="pre">is_cjk</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">character</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tokenize/util.html#is_cjk"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.util.is_cjk" title="Permalink to this definition">¶</a></dt>
<dd><p>Python port of Moses’ code to check for CJK character.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">CJKChars</span><span class="p">()</span><span class="o">.</span><span class="n">ranges</span>
<span class="go">[(4352, 4607), (11904, 42191), (43072, 43135), (44032, 55215), (63744, 64255), (65072, 65103), (65381, 65500), (131072, 196607)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">is_cjk</span><span class="p">(</span><span class="sa">u</span><span class="s1">&#39;㏾&#39;</span><span class="p">)</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">is_cjk</span><span class="p">(</span><span class="sa">u</span><span class="s1">&#39;﹟&#39;</span><span class="p">)</span>
<span class="go">False</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>character</strong> (<em>char</em>) – The character that needs to be checked.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>bool</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="nltk.tokenize.util.regexp_span_tokenize">
<code class="sig-prename descclassname"><span class="pre">nltk.tokenize.util.</span></code><code class="sig-name descname"><span class="pre">regexp_span_tokenize</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">s</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">regexp</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tokenize/util.html#regexp_span_tokenize"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.util.regexp_span_tokenize" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the offsets of the tokens in <em>s</em>, as a sequence of <code class="docutils literal notranslate"><span class="pre">(start,</span> <span class="pre">end)</span></code>
tuples, by splitting the string at each successive match of <em>regexp</em>.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">nltk.tokenize.util</span> <span class="kn">import</span> <span class="n">regexp_span_tokenize</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s</span> <span class="o">=</span> <span class="s1">&#39;&#39;&#39;Good muffins cost $3.88</span><span class="se">\n</span><span class="s1">in New York.  Please buy me</span>
<span class="gp">... </span><span class="s1">two of them.</span><span class="se">\n\n</span><span class="s1">Thanks.&#39;&#39;&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">list</span><span class="p">(</span><span class="n">regexp_span_tokenize</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="sa">r</span><span class="s1">&#39;\s&#39;</span><span class="p">))</span>
<span class="go">[(0, 4), (5, 12), (13, 17), (18, 23), (24, 26), (27, 30), (31, 36),</span>
<span class="go">(38, 44), (45, 48), (49, 51), (52, 55), (56, 58), (59, 64), (66, 73)]</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>s</strong> (<em>str</em>) – the string to be tokenized</p></li>
<li><p><strong>regexp</strong> (<em>str</em>) – regular expression that matches token separators (must not be empty)</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>iter(tuple(int, int))</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="nltk.tokenize.util.spans_to_relative">
<code class="sig-prename descclassname"><span class="pre">nltk.tokenize.util.</span></code><code class="sig-name descname"><span class="pre">spans_to_relative</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">spans</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tokenize/util.html#spans_to_relative"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.util.spans_to_relative" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a sequence of relative spans, given a sequence of spans.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">nltk.tokenize</span> <span class="kn">import</span> <span class="n">WhitespaceTokenizer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">nltk.tokenize.util</span> <span class="kn">import</span> <span class="n">spans_to_relative</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s</span> <span class="o">=</span> <span class="s1">&#39;&#39;&#39;Good muffins cost $3.88</span><span class="se">\n</span><span class="s1">in New York.  Please buy me</span>
<span class="gp">... </span><span class="s1">two of them.</span><span class="se">\n\n</span><span class="s1">Thanks.&#39;&#39;&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">list</span><span class="p">(</span><span class="n">spans_to_relative</span><span class="p">(</span><span class="n">WhitespaceTokenizer</span><span class="p">()</span><span class="o">.</span><span class="n">span_tokenize</span><span class="p">(</span><span class="n">s</span><span class="p">)))</span>
<span class="go">[(0, 4), (1, 7), (1, 4), (1, 5), (1, 2), (1, 3), (1, 5), (2, 6),</span>
<span class="go">(1, 3), (1, 2), (1, 3), (1, 2), (1, 5), (2, 7)]</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>spans</strong> (<em>iter</em><em>(</em><em>tuple</em><em>(</em><em>int</em><em>, </em><em>int</em><em>)</em><em>)</em>) – a sequence of (start, end) offsets of the tokens</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>iter(tuple(int, int))</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="nltk.tokenize.util.string_span_tokenize">
<code class="sig-prename descclassname"><span class="pre">nltk.tokenize.util.</span></code><code class="sig-name descname"><span class="pre">string_span_tokenize</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">s</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sep</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tokenize/util.html#string_span_tokenize"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.util.string_span_tokenize" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the offsets of the tokens in <em>s</em>, as a sequence of <code class="docutils literal notranslate"><span class="pre">(start,</span> <span class="pre">end)</span></code>
tuples, by splitting the string at each occurrence of <em>sep</em>.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">nltk.tokenize.util</span> <span class="kn">import</span> <span class="n">string_span_tokenize</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s</span> <span class="o">=</span> <span class="s1">&#39;&#39;&#39;Good muffins cost $3.88</span><span class="se">\n</span><span class="s1">in New York.  Please buy me</span>
<span class="gp">... </span><span class="s1">two of them.</span><span class="se">\n\n</span><span class="s1">Thanks.&#39;&#39;&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">list</span><span class="p">(</span><span class="n">string_span_tokenize</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="s2">&quot; &quot;</span><span class="p">))</span>
<span class="go">[(0, 4), (5, 12), (13, 17), (18, 26), (27, 30), (31, 36), (37, 37),</span>
<span class="go">(38, 44), (45, 48), (49, 55), (56, 58), (59, 73)]</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>s</strong> (<em>str</em>) – the string to be tokenized</p></li>
<li><p><strong>sep</strong> (<em>str</em>) – the token separator</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>iter(tuple(int, int))</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="nltk.tokenize.util.xml_escape">
<code class="sig-prename descclassname"><span class="pre">nltk.tokenize.util.</span></code><code class="sig-name descname"><span class="pre">xml_escape</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">text</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tokenize/util.html#xml_escape"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.util.xml_escape" title="Permalink to this definition">¶</a></dt>
<dd><p>This function transforms the input text into an “escaped” version suitable
for well-formed XML formatting.</p>
<p>Note that the default xml.sax.saxutils.escape() function don’t escape
some characters that Moses does so we have to manually add them to the
entities dictionary.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_str</span> <span class="o">=</span> <span class="s1">&#39;&#39;&#39;)| &amp; &lt; &gt; &#39; &quot; ] [&#39;&#39;&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">expected_output</span> <span class="o">=</span>  <span class="s1">&#39;&#39;&#39;)| &amp;amp; &amp;lt; &amp;gt; &#39; &quot; ] [&#39;&#39;&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">escape</span><span class="p">(</span><span class="n">input_str</span><span class="p">)</span> <span class="o">==</span> <span class="n">expected_output</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">xml_escape</span><span class="p">(</span><span class="n">input_str</span><span class="p">)</span>
<span class="go">&#39;)&amp;#124; &amp;amp; &amp;lt; &amp;gt; &amp;apos; &amp;quot; &amp;#93; &amp;#91;&#39;</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>text</strong> (<em>str</em>) – The text that needs to be escaped.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>str</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="nltk.tokenize.util.xml_unescape">
<code class="sig-prename descclassname"><span class="pre">nltk.tokenize.util.</span></code><code class="sig-name descname"><span class="pre">xml_unescape</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">text</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tokenize/util.html#xml_unescape"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.util.xml_unescape" title="Permalink to this definition">¶</a></dt>
<dd><p>This function transforms the “escaped” version suitable
for well-formed XML formatting into humanly-readable string.</p>
<p>Note that the default xml.sax.saxutils.unescape() function don’t unescape
some characters that Moses does so we have to manually add them to the
entities dictionary.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">xml.sax.saxutils</span> <span class="kn">import</span> <span class="n">unescape</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s</span> <span class="o">=</span> <span class="s1">&#39;)&amp;#124; &amp;amp; &amp;lt; &amp;gt; &amp;apos; &amp;quot; &amp;#93; &amp;#91;&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">expected</span> <span class="o">=</span> <span class="s1">&#39;&#39;&#39;)| &amp; &lt; &gt; &#39; &quot; ] [&#39;&#39;&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">xml_unescape</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="o">==</span> <span class="n">expected</span>
<span class="go">True</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>text</strong> (<em>str</em>) – The text that needs to be unescaped.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>str</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="module-nltk.tokenize">
<span id="module-contents"></span><h2>Module contents<a class="headerlink" href="#module-nltk.tokenize" title="Permalink to this headline">¶</a></h2>
<p>NLTK Tokenizer Package</p>
<p>Tokenizers divide strings into lists of substrings.  For example,
tokenizers can be used to find the words and punctuation in a string:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">nltk.tokenize</span> <span class="kn">import</span> <span class="n">word_tokenize</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s</span> <span class="o">=</span> <span class="s1">&#39;&#39;&#39;Good muffins cost $3.88</span><span class="se">\n</span><span class="s1">in New York.  Please buy me</span>
<span class="gp">... </span><span class="s1">two of them.</span><span class="se">\n\n</span><span class="s1">Thanks.&#39;&#39;&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">word_tokenize</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
<span class="go">[&#39;Good&#39;, &#39;muffins&#39;, &#39;cost&#39;, &#39;$&#39;, &#39;3.88&#39;, &#39;in&#39;, &#39;New&#39;, &#39;York&#39;, &#39;.&#39;,</span>
<span class="go">&#39;Please&#39;, &#39;buy&#39;, &#39;me&#39;, &#39;two&#39;, &#39;of&#39;, &#39;them&#39;, &#39;.&#39;, &#39;Thanks&#39;, &#39;.&#39;]</span>
</pre></div>
</div>
<p>This particular tokenizer requires the Punkt sentence tokenization
models to be installed. NLTK also provides a simpler,
regular-expression based tokenizer, which splits text on whitespace
and punctuation:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">nltk.tokenize</span> <span class="kn">import</span> <span class="n">wordpunct_tokenize</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">wordpunct_tokenize</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
<span class="go">[&#39;Good&#39;, &#39;muffins&#39;, &#39;cost&#39;, &#39;$&#39;, &#39;3&#39;, &#39;.&#39;, &#39;88&#39;, &#39;in&#39;, &#39;New&#39;, &#39;York&#39;, &#39;.&#39;,</span>
<span class="go">&#39;Please&#39;, &#39;buy&#39;, &#39;me&#39;, &#39;two&#39;, &#39;of&#39;, &#39;them&#39;, &#39;.&#39;, &#39;Thanks&#39;, &#39;.&#39;]</span>
</pre></div>
</div>
<p>We can also operate at the level of sentences, using the sentence
tokenizer directly as follows:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">nltk.tokenize</span> <span class="kn">import</span> <span class="n">sent_tokenize</span><span class="p">,</span> <span class="n">word_tokenize</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sent_tokenize</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
<span class="go">[&#39;Good muffins cost $3.88\nin New York.&#39;, &#39;Please buy me\ntwo of them.&#39;, &#39;Thanks.&#39;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="n">word_tokenize</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">sent_tokenize</span><span class="p">(</span><span class="n">s</span><span class="p">)]</span>
<span class="go">[[&#39;Good&#39;, &#39;muffins&#39;, &#39;cost&#39;, &#39;$&#39;, &#39;3.88&#39;, &#39;in&#39;, &#39;New&#39;, &#39;York&#39;, &#39;.&#39;],</span>
<span class="go">[&#39;Please&#39;, &#39;buy&#39;, &#39;me&#39;, &#39;two&#39;, &#39;of&#39;, &#39;them&#39;, &#39;.&#39;], [&#39;Thanks&#39;, &#39;.&#39;]]</span>
</pre></div>
</div>
<p>Caution: when tokenizing a Unicode string, make sure you are not
using an encoded version of the string (it may be necessary to
decode it first, e.g. with <code class="docutils literal notranslate"><span class="pre">s.decode(&quot;utf8&quot;)</span></code>.</p>
<p>NLTK tokenizers can produce token-spans, represented as tuples of integers
having the same semantics as string slices, to support efficient comparison
of tokenizers.  (These methods are implemented as generators.)</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">nltk.tokenize</span> <span class="kn">import</span> <span class="n">WhitespaceTokenizer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">list</span><span class="p">(</span><span class="n">WhitespaceTokenizer</span><span class="p">()</span><span class="o">.</span><span class="n">span_tokenize</span><span class="p">(</span><span class="n">s</span><span class="p">))</span>
<span class="go">[(0, 4), (5, 12), (13, 17), (18, 23), (24, 26), (27, 30), (31, 36), (38, 44),</span>
<span class="go">(45, 48), (49, 51), (52, 55), (56, 58), (59, 64), (66, 73)]</span>
</pre></div>
</div>
<p>There are numerous ways to tokenize text.  If you need more control over
tokenization, see the other methods provided in this package.</p>
<p>For further information, please see Chapter 3 of the NLTK book.</p>
<dl class="py function">
<dt id="nltk.tokenize.sent_tokenize">
<code class="sig-prename descclassname"><span class="pre">nltk.tokenize.</span></code><code class="sig-name descname"><span class="pre">sent_tokenize</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">text</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">language</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'english'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tokenize.html#sent_tokenize"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.sent_tokenize" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a sentence-tokenized copy of <em>text</em>,
using NLTK’s recommended sentence tokenizer
(currently <a class="reference internal" href="#nltk.tokenize.punkt.PunktSentenceTokenizer" title="nltk.tokenize.punkt.PunktSentenceTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">PunktSentenceTokenizer</span></code></a>
for the specified language).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>text</strong> – text to split into sentences</p></li>
<li><p><strong>language</strong> – the model name in the Punkt corpus</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="nltk.tokenize.word_tokenize">
<code class="sig-prename descclassname"><span class="pre">nltk.tokenize.</span></code><code class="sig-name descname"><span class="pre">word_tokenize</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">text</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">language</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'english'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">preserve_line</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tokenize.html#word_tokenize"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tokenize.word_tokenize" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a tokenized copy of <em>text</em>,
using NLTK’s recommended word tokenizer
(currently an improved <a class="reference internal" href="#nltk.tokenize.treebank.TreebankWordTokenizer" title="nltk.tokenize.treebank.TreebankWordTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">TreebankWordTokenizer</span></code></a>
along with <a class="reference internal" href="#nltk.tokenize.punkt.PunktSentenceTokenizer" title="nltk.tokenize.punkt.PunktSentenceTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">PunktSentenceTokenizer</span></code></a>
for the specified language).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>text</strong> (<em>str</em>) – text to split into words</p></li>
<li><p><strong>language</strong> (<em>str</em>) – the model name in the Punkt corpus</p></li>
<li><p><strong>preserve_line</strong> (<em>bool</em>) – An option to keep the preserve the sentence and not sentence tokenize it.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
</div>


            <div class="clearer"></div>
          </div>
        </div>
      </div>
        </div>
        <div class="sidebar">
          
          <h3>Table of Contents</h3>
          <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../news.html">NLTK News</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install.html">Installing NLTK</a></li>
<li class="toctree-l1"><a class="reference internal" href="../data.html">Installing NLTK Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../contribute.html">Contribute to NLTK</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/nltk/nltk/wiki/FAQ">FAQ</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/nltk/nltk/wiki">Wiki</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="nltk.html">API</a></li>
<li class="toctree-l1"><a class="reference external" href="http://www.nltk.org/howto">HOWTO</a></li>
</ul>

          <div role="search">
            <h3 style="margin-top: 1.5em;">Search</h3>
            <form class="search" action="../search.html" method="get">
                <input type="text" name="q" />
                <input type="submit" value="Go" />
            </form>
          </div>

        </div>
        <div class="clearer"></div>
      </div>
    </div>

    <div class="footer-wrapper">
      <div class="footer">
        <div class="left">
          <div role="navigation" aria-label="related navigaton">
            <a href="nltk.test.unit.translate.html" title="nltk.test.unit.translate package"
              >previous</a> |
            <a href="../py-modindex.html" title="Python Module Index"
              >modules</a> |
            <a href="../genindex.html" title="General Index"
              >index</a>
          </div>
          <div role="note" aria-label="source link">
              <br/>
              <a href="../_sources/api/nltk.tokenize.rst.txt"
                rel="nofollow">Show Source</a>
          </div>
        </div>

        <div class="right">
          
    <div class="footer" role="contentinfo">
        &#169; Copyright 2020, NLTK Project.
      Last updated on Apr 07, 2021.
      Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 3.5.2.
    </div>
        </div>
        <div class="clearer"></div>
      </div>
    </div>

  </body>
</html>